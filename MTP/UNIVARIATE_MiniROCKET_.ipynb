{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set a global random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 173737,
     "status": "ok",
     "timestamp": 1724485826929,
     "user": {
      "displayName": "Bijoyashree Das",
      "userId": "11723202937662517391"
     },
     "user_tz": -330
    },
    "id": "0dmQSVsBg39H",
    "outputId": "caf40d1c-ee21-44c3-a28f-899e5801dd24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WORKSTATIONS\\anaconda3\\envs\\endbd21\\Lib\\site-packages\\aeon\\base\\__init__.py:24: FutureWarning: The aeon package will soon be releasing v1.0.0 with the removal of legacy modules and interfaces such as BaseTransformer and BaseForecaster. This will contain breaking changes. See aeon-toolkit.org for more information. Set aeon.AEON_DEPRECATION_WARNING or the AEON_DEPRECATION_WARNING environmental variable to 'False' to disable this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"MiniRocket classifier.\n",
    "\n",
    "Pipeline classifier using the MiniRocket transformer and RidgeClassifierCV classifier.\n",
    "\"\"\"\n",
    "\n",
    "__maintainer__ = [\"MatthewMiddlehurst\"]\n",
    "__all__ = [\"MiniRocketClassifier\"]\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from aeon.base._base import _clone_estimator\n",
    "from aeon.classification import BaseClassifier\n",
    "from aeon.transformations.collection.convolution_based import MiniRocket\n",
    "\n",
    "\n",
    "class MiniRocketClassifier(BaseClassifier):\n",
    "    \"\"\"\n",
    "    MiniRocket transformer using RidgeClassifierCV.\n",
    "\n",
    "    This classifier transforms the input data using the MiniRocket [1]_ transformer\n",
    "    extracting features from randomly generated kernels, performs a Standard scaling\n",
    "    and fits a sklearn classifier using the transformed data (default classifier is\n",
    "    RidgeClassifierCV).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_kernels : int, default=10,000\n",
    "        The number of kernels for the Rocket transform.\n",
    "    max_dilations_per_kernel : int, default=32\n",
    "        The maximum number of dilations per kernel.\n",
    "    estimator : sklearn compatible classifier or None, default=None\n",
    "        The estimator used. If None, a RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        is used.\n",
    "    class_weight{“balanced”, “balanced_subsample”}, dict or list of dicts, default=None\n",
    "        Only applies if estimator is None and the default is used.\n",
    "        From sklearn documentation:\n",
    "        If not given, all classes are supposed to have weight one.\n",
    "        The “balanced” mode uses the values of y to automatically adjust weights\n",
    "        inversely proportional to class frequencies in the input data as\n",
    "        n_samples / (n_classes * np.bincount(y))\n",
    "        The “balanced_subsample” mode is the same as “balanced” except that weights\n",
    "        are computed based on the bootstrap sample for every tree grown.\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "        Note that these weights will be multiplied with sample_weight (passed through\n",
    "        the fit method) if sample_weight is specified.\n",
    "    n_jobs : int, default=1\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``-1`` means using all processors.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        If `int`, random_state is the seed used by the random number generator;\n",
    "        If `RandomState` instance, random_state is the random number generator;\n",
    "        If `None`, the random number generator is the `RandomState` instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_classes_ : int\n",
    "        The number of classes.\n",
    "    classes_ : list\n",
    "        The classes labels.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Dempster, A., Schmidt, D.F. and Webb, G.I., 2021, August. Minirocket: A very\n",
    "        fast (almost) deterministic transform for time series classification. In\n",
    "        Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data\n",
    "        mining (pp. 248-257).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from aeon.classification.convolution_based import MiniRocketClassifier\n",
    "    >>> from aeon.datasets import load_unit_test\n",
    "    >>> X_train, y_train = load_unit_test(split=\"train\")\n",
    "    >>> X_test, y_test = load_unit_test(split=\"test\")\n",
    "    >>> clf = MiniRocketClassifier(n_kernels=500)\n",
    "    >>> clf.fit(X_train, y_train)\n",
    "    MiniRocketClassifier(...)\n",
    "    >>> y_pred = clf.predict(X_test)\n",
    "    \"\"\"\n",
    "\n",
    "    _tags = {\n",
    "        \"capability:multithreading\": True,\n",
    "        \"capability:multivariate\": True,\n",
    "        \"algorithm_type\": \"convolution\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_kernels: int = 10000,\n",
    "        max_dilations_per_kernel: int = 32,\n",
    "        estimator=None,\n",
    "        class_weight=None,\n",
    "        n_jobs: int = 1,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.n_kernels = n_kernels\n",
    "        self.max_dilations_per_kernel = max_dilations_per_kernel\n",
    "        self.estimator = estimator\n",
    "\n",
    "        self.class_weight = class_weight\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        \"\"\"Fit Rocket variant to training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray\n",
    "            The training data of shape = (n_cases, n_channels, n_timepoints).\n",
    "        y : 3D np.ndarray\n",
    "            The class labels, shape = (n_cases,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self :\n",
    "            Reference to self.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Changes state by creating a fitted model that updates attributes\n",
    "        ending in \"_\" and sets is_fitted flag to True.\n",
    "        \"\"\"\n",
    "        self.n_cases_, self.n_channels_, self.n_timepoints_ = X.shape\n",
    "\n",
    "        self._transformer = MiniRocket(\n",
    "            n_kernels=self.n_kernels,\n",
    "            max_dilations_per_kernel=self.max_dilations_per_kernel,\n",
    "            n_jobs=self.n_jobs,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "        self._scaler = StandardScaler(with_mean=False)\n",
    "        self._estimator = _clone_estimator(\n",
    "            (\n",
    "                RidgeClassifierCV(\n",
    "                    alphas=np.logspace(-3, 3, 10), class_weight=self.class_weight\n",
    "                )\n",
    "                if self.estimator is None\n",
    "                else self.estimator\n",
    "            ),\n",
    "            self.random_state,\n",
    "        )\n",
    "\n",
    "        self.pipeline_ = make_pipeline(\n",
    "            self._transformer,\n",
    "            self._scaler,\n",
    "            self._estimator,\n",
    "        )\n",
    "        self.pipeline_.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict(self, X) -> np.ndarray:\n",
    "        \"\"\"Predicts labels for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape = (n_cases, n_channels, n_timepoints)\n",
    "            The data to make predictions for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like, shape = (n_cases,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        return self.pipeline_.predict(X)\n",
    "\n",
    "    def _predict_proba(self, X) -> np.ndarray:\n",
    "        \"\"\"Predicts labels probabilities for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape = (n_cases, n_channels, n_timepoints)\n",
    "            The data to make predict probabilities for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like, shape = (n_cases, n_classes_)\n",
    "            Predicted probabilities using the ordering in classes_.\n",
    "        \"\"\"\n",
    "        m = getattr(self._estimator, \"predict_proba\", None)\n",
    "        if callable(m):\n",
    "            return self.pipeline_.predict_proba(X)\n",
    "        else:\n",
    "            dists = np.zeros((X.shape[0], self.n_classes_))\n",
    "            preds = self.pipeline_.predict(X)\n",
    "            for i in range(0, X.shape[0]):\n",
    "                dists[i, np.where(self.classes_ == preds[i])] = 1\n",
    "            return dists\n",
    "\n",
    "    @classmethod\n",
    "    def _get_test_params(cls, parameter_set=\"default\"):\n",
    "        \"\"\"Return testing parameter settings for the estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameter_set : str, default=\"default\"\n",
    "            Name of the set of test parameters to return, for use in tests. If no\n",
    "            special parameters are defined for a value, will return `\"default\"` set.\n",
    "            RocketClassifier provides the following special sets:\n",
    "                 \"results_comparison\" - used in some classifiers to compare against\n",
    "                    previously generated results where the default set of parameters\n",
    "                    cannot produce suitable probability estimates\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        params : dict or list of dict, default={}\n",
    "            Parameters to create testing instances of the class.\n",
    "            Each dict are parameters to construct an \"interesting\" test instance, i.e.,\n",
    "            `MyClass(**params)` or `MyClass(**params[i])` creates a valid test instance.\n",
    "        \"\"\"\n",
    "        if parameter_set == \"results_comparison\":\n",
    "            return {\"n_kernels\": 100}\n",
    "        else:\n",
    "            return {\"n_kernels\": 20, \"max_dilations_per_kernel\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqleLI-_g39L",
    "outputId": "0a9ce574-c0ec-44a8-895c-a0ffaf89dc15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in folder 'bed': 1713\n",
      "Total number of files in folder 'bird': 1731\n",
      "Total number of files in folder 'cat': 1733\n",
      "Total number of files in folder 'dog': 1746\n",
      "Total number of files in folder 'down': 2359\n",
      "Total number of files in folder 'eight': 2352\n",
      "Total number of files in folder 'five': 2357\n",
      "Total number of files in folder 'four': 2372\n",
      "Total number of files in folder 'go': 2372\n",
      "Total number of files in folder 'happy': 1742\n",
      "(20477, 1, 16000)\n",
      "(20477,)\n",
      "[0 0 0 ... 9 9 9]\n",
      "Overall total number of files in the dataset: 20477\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def load_data_from_directory(directory, sample_length=16000, n_channels=1):\n",
    "    X = []\n",
    "    y = []\n",
    "    labels = sorted(os.listdir(directory))\n",
    "    label_map = {label: idx for idx, label in enumerate(labels)}  # Create a label to index mapping\n",
    "\n",
    "    total_files = 0  # Initialize a counter for total files\n",
    "\n",
    "    for label in labels:\n",
    "        class_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(class_dir):\n",
    "            file_count = 0  # Counter for files in the current class directory\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.endswith('.wav'):\n",
    "                    file_count += 1  # Increment file count for the current class\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    # Load audio\n",
    "                    signal, sr = librosa.load(file_path, sr=16000)\n",
    "                    # Ensure length is 1 second (16000 samples)\n",
    "                    if len(signal) != sample_length:\n",
    "                        # Pad or truncate to sample_length\n",
    "                        if len(signal) < sample_length:\n",
    "                            signal = np.pad(signal, (0, sample_length - len(signal)))\n",
    "                        else:\n",
    "                            signal = signal[:sample_length]\n",
    "                    # Reshape signal to match (n_channels, n_timepoints)\n",
    "                    X.append(signal.reshape(n_channels, -1))  # Reshape to (1, sample_length)\n",
    "                    y.append(label_map[label])  # Use the index of the label\n",
    "\n",
    "            total_files += file_count  # Add the current class file count to the total\n",
    "            print(f\"Total number of files in folder '{label}': {file_count}\")\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    print(f\"Overall total number of files in the dataset: {total_files}\")\n",
    "\n",
    "    return X, y, labels\n",
    "\n",
    "# Load and preprocess data\n",
    "directory = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/GoogleSpeechNew\"\n",
    "X, y, labels = load_data_from_directory(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Lc6AZsymg39M"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Q5btkexOg39M"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Split data into training and testing sets with a fixed random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Classifier\n",
    "\n",
    "clf = MiniRocketClassifier(\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier on the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11pB32yMg39N"
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test data:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision, recall, and f1 score for each class (macro average)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Precision (macro): {precision}\")\n",
    "print(f\"Recall (macro): {recall}\")\n",
    "print(f\"F1 Score (macro): {f1}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
