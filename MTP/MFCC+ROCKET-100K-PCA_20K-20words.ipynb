{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e07b87-8f73-4cc4-8a84-928964fc2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RandOm Convolutional KErnel Transform (Rocket).\n",
    "\n",
    "Pipeline classifier using the ROCKET transformer and an sklearn classifier.\n",
    "\"\"\"\n",
    "\n",
    "__maintainer__ = []\n",
    "__all__ = [\"RocketClassifier\"]\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from aeon.base._base import _clone_estimator\n",
    "from aeon.classification import BaseClassifier\n",
    "from aeon.transformations.collection.convolution_based import (\n",
    "    MiniRocket,\n",
    "    MultiRocket,\n",
    "    Rocket,\n",
    ")\n",
    "\n",
    "\n",
    "class RocketClassifier(BaseClassifier):\n",
    "    \"\"\"\n",
    "    Classifier wrapped for the Rocket transformer using RidgeClassifierCV.\n",
    "\n",
    "    This classifier simply transforms the input data using a Rocket [1,2,3]_\n",
    "    transformer, performs a Standard scaling and fits a sklearn classifier,\n",
    "    using the transformed data (default classifier is RidgeClassifierCV).\n",
    "\n",
    "    The classifier can be configured to use Rocket [1]_, MiniRocket [2]_ or\n",
    "    MultiRocket [3]_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_kernels : int, default=10,000\n",
    "        The number of kernels for the Rocket transform.\n",
    "    rocket_transform : str, default=\"rocket\"\n",
    "        The type of Rocket transformer to use.\n",
    "        Valid inputs = [\"rocket\", \"minirocket\", \"multirocket\"].\n",
    "    max_dilations_per_kernel : int, default=32\n",
    "        MiniRocket and MultiRocket only. The maximum number of dilations per kernel.\n",
    "    n_features_per_kernel : int, default=4\n",
    "        MultiRocket only. The number of features per kernel.\n",
    "    estimator : sklearn compatible classifier or None, default=None\n",
    "        The estimator used. If None, a RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        is used.\n",
    "    class_weight{“balanced”, “balanced_subsample”}, dict or list of dicts, default=None\n",
    "        Only applies if estimator is None and the default is used.\n",
    "        From sklearn documentation:\n",
    "        If not given, all classes are supposed to have weight one.\n",
    "        The “balanced” mode uses the values of y to automatically adjust weights\n",
    "        inversely proportional to class frequencies in the input data as\n",
    "        n_samples / (n_classes * np.bincount(y))\n",
    "        The “balanced_subsample” mode is the same as “balanced” except that weights\n",
    "        are computed based on the bootstrap sample for every tree grown.\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "        Note that these weights will be multiplied with sample_weight (passed through\n",
    "        the fit method) if sample_weight is specified.\n",
    "    n_jobs : int, default=1\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``-1`` means using all processors.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        If `int`, random_state is the seed used by the random number generator;\n",
    "        If `RandomState` instance, random_state is the random number generator;\n",
    "        If `None`, the random number generator is the `RandomState` instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_classes_ : int\n",
    "        The number of classes.\n",
    "    classes_ : list\n",
    "        The classes labels.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    Rocket\n",
    "        Rocket transformers are in transformations/collection.\n",
    "    RocketRegressor\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Dempster, A., Petitjean, F. and Webb, G.I., 2020. ROCKET: exceptionally fast\n",
    "        and accurate time series classification using random convolutional kernels.\n",
    "        Data Mining and Knowledge Discovery, 34(5), pp.1454-1495.\n",
    "    .. [2] Dempster, A., Schmidt, D.F. and Webb, G.I., 2021, August. Minirocket: A very\n",
    "        fast (almost) deterministic transform for time series classification. In\n",
    "        Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data\n",
    "        mining (pp. 248-257).\n",
    "    .. [3] Tan, C.W., Dempster, A., Bergmeir, C. and Webb, G.I., 2022. MultiRocket:\n",
    "        multiple pooling operators and transformations for fast and effective time\n",
    "        series classification. Data Mining and Knowledge Discovery, 36(5), pp.1623-1646.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from aeon.classification.convolution_based import RocketClassifier\n",
    "    >>> from aeon.datasets import load_unit_test\n",
    "    >>> X_train, y_train = load_unit_test(split=\"train\")\n",
    "    >>> X_test, y_test = load_unit_test(split=\"test\")\n",
    "    >>> clf = RocketClassifier(num_kernels=500)\n",
    "    >>> clf.fit(X_train, y_train)\n",
    "    RocketClassifier(...)\n",
    "    >>> y_pred = clf.predict(X_test)\n",
    "    \"\"\"\n",
    "\n",
    "    _tags = {\n",
    "        \"capability:multithreading\": True,\n",
    "        \"capability:multivariate\": True,\n",
    "        \"algorithm_type\": \"convolution\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_kernels=20000,\n",
    "        rocket_transform=\"rocket\",\n",
    "        max_dilations_per_kernel=32,\n",
    "        n_features_per_kernel=4,\n",
    "        estimator=None,\n",
    "        class_weight=None,\n",
    "        n_jobs=1,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.num_kernels = num_kernels\n",
    "        self.rocket_transform = rocket_transform\n",
    "        self.max_dilations_per_kernel = max_dilations_per_kernel\n",
    "        self.n_features_per_kernel = n_features_per_kernel\n",
    "        self.estimator = estimator\n",
    "\n",
    "        self.class_weight = class_weight\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        \"\"\"Fit Rocket variant to training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray\n",
    "            The training data of shape = (n_cases, n_channels, n_timepoints).\n",
    "        y : 3D np.ndarray\n",
    "            The class labels, shape = (n_cases,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self :\n",
    "            Reference to self.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Changes state by creating a fitted model that updates attributes\n",
    "        ending in \"_\" and sets is_fitted flag to True.\n",
    "        \"\"\"\n",
    "        self.n_cases_, self.n_channels_, self.n_timepoints_ = X.shape\n",
    "\n",
    "        rocket_transform = self.rocket_transform.lower()\n",
    "        if rocket_transform == \"rocket\":\n",
    "            self._transformer = Rocket(\n",
    "                num_kernels=self.num_kernels,\n",
    "                n_jobs=self.n_jobs,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "        elif rocket_transform == \"minirocket\":\n",
    "            self._transformer = MiniRocket(\n",
    "                num_kernels=self.num_kernels,\n",
    "                max_dilations_per_kernel=self.max_dilations_per_kernel,\n",
    "                n_jobs=self.n_jobs,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "        elif rocket_transform == \"multirocket\":\n",
    "            self._transformer = MultiRocket(\n",
    "                num_kernels=self.num_kernels,\n",
    "                max_dilations_per_kernel=self.max_dilations_per_kernel,\n",
    "                n_features_per_kernel=self.n_features_per_kernel,\n",
    "                n_jobs=self.n_jobs,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid Rocket transformer: {self.rocket_transform}\")\n",
    "\n",
    "        self._scaler = StandardScaler(with_mean=False)\n",
    "        self._estimator = _clone_estimator(\n",
    "            (\n",
    "                RidgeClassifierCV(\n",
    "                    alphas=np.logspace(-3, 3, 10), class_weight=self.class_weight\n",
    "                )\n",
    "                if self.estimator is None\n",
    "                else self.estimator\n",
    "            ),\n",
    "            self.random_state,\n",
    "        )\n",
    "\n",
    "        self.pipeline_ = make_pipeline(\n",
    "            self._transformer,\n",
    "            self._scaler,\n",
    "            self._estimator,\n",
    "        )\n",
    "        self.pipeline_.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict(self, X) -> np.ndarray:\n",
    "        \"\"\"Predicts labels for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape = (n_cases, n_channels, n_timepoints)\n",
    "            The data to make predictions for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like, shape = (n_cases,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        return self.pipeline_.predict(X)\n",
    "\n",
    "    def _predict_proba(self, X) -> np.ndarray:\n",
    "        \"\"\"Predicts labels probabilities for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape = (n_cases, n_channels, n_timepoints)\n",
    "            The data to make predict probabilities for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like, shape = (n_cases, n_classes_)\n",
    "            Predicted probabilities using the ordering in classes_.\n",
    "        \"\"\"\n",
    "        m = getattr(self._estimator, \"predict_proba\", None)\n",
    "        if callable(m):\n",
    "            return self.pipeline_.predict_proba(X)\n",
    "        else:\n",
    "            dists = np.zeros((X.shape[0], self.n_classes_))\n",
    "            preds = self.pipeline_.predict(X)\n",
    "            for i in range(0, X.shape[0]):\n",
    "                dists[i, np.where(self.classes_ == preds[i])] = 1\n",
    "            return dists\n",
    "\n",
    "    @classmethod\n",
    "    def get_test_params(cls, parameter_set=\"default\"):\n",
    "        \"\"\"Return testing parameter settings for the estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameter_set : str, default=\"default\"\n",
    "            Name of the set of test parameters to return, for use in tests. If no\n",
    "            special parameters are defined for a value, will return `\"default\"` set.\n",
    "            RocketClassifier provides the following special sets:\n",
    "                 \"results_comparison\" - used in some classifiers to compare against\n",
    "                    previously generated results where the default set of parameters\n",
    "                    cannot produce suitable probability estimates\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        params : dict or list of dict, default={}\n",
    "            Parameters to create testing instances of the class.\n",
    "            Each dict are parameters to construct an \"interesting\" test instance, i.e.,\n",
    "            `MyClass(**params)` or `MyClass(**params[i])` creates a valid test instance.\n",
    "            `create_test_instance` uses the first (or only) dictionary in `params`.\n",
    "        \"\"\"\n",
    "        if parameter_set == \"results_comparison\":\n",
    "            return {\"num_kernels\": 100}\n",
    "        else:\n",
    "            return {\"num_kernels\": 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51a7cb3-c7a1-4960-ae1a-b4793104bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from aeon.datasets import load_unit_test\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class RocketClassifierWithPCS(RocketClassifier):\n",
    "    \"\"\"\n",
    "    RocketClassifier extended to include Principal Component Selection (PCS)\n",
    "    using PCA on the feature matrix before classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_kernels=100000,\n",
    "        rocket_transform=\"rocket\",\n",
    "        max_dilations_per_kernel=32,\n",
    "        n_features_per_kernel=4,\n",
    "        n_pcs=20000,  # Number of principal components to keep\n",
    "        estimator=None,\n",
    "        class_weight=None,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "    ):\n",
    "        self.n_pcs = n_pcs\n",
    "        super().__init__(\n",
    "            num_kernels=num_kernels,\n",
    "            rocket_transform=rocket_transform,\n",
    "            max_dilations_per_kernel=max_dilations_per_kernel,\n",
    "            n_features_per_kernel=n_features_per_kernel,\n",
    "            estimator=estimator,\n",
    "            class_weight=class_weight,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    \n",
    "    def _fit(self, X, y):\n",
    "        \"\"\"Fit Rocket variant and PCA to the training data.\"\"\"\n",
    "        super()._fit(X, y)  # Call the original RocketClassifier's fit method\n",
    "\n",
    "        # Apply PCA if n_pcs is specified\n",
    "        if self.n_pcs is not None:\n",
    "            self._pca = PCA(n_components=self.n_pcs, random_state=self.random_state)\n",
    "        else:\n",
    "            self._pca = PCA(random_state=self.random_state)\n",
    "\n",
    "        # Transform the Rocket features with PCA\n",
    "\n",
    "        # [changed] Not require to perform this as this step is involed in self.pipeline_.fit(X, y)\n",
    "        # rocket_features = self._transformer.transform(X)\n",
    "        # self._pca.fit(rocket_features)\n",
    "        # pca_features = self._pca.transform(rocket_features)\n",
    "\n",
    "        # Update the pipeline to include PCA\n",
    "        self.pipeline_ = make_pipeline(\n",
    "            self._transformer,\n",
    "            self._scaler,\n",
    "            self._pca,\n",
    "            self._estimator,\n",
    "        )\n",
    "\n",
    "        # [changed] pca_features to X\n",
    "        self.pipeline_.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        \"\"\"Predict labels for sequences in X after PCA transformation.\"\"\"\n",
    "        # [changed] no need apply them explicitly\n",
    "        # rocket_features = self._transformer.transform(X)\n",
    "        # pca_features = self._pca.transform(rocket_features)\n",
    "        return self.pipeline_.predict(X)\n",
    "\n",
    "    def _predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities for sequences in X after PCA transformation.\"\"\"\n",
    "        # [changed]\n",
    "        # rocket_features = self._transformer.transform(X)\n",
    "        # pca_features = self._pca.transform(rocket_features)\n",
    "        return self.pipeline_.predict_proba(X)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785cca51-6597-46df-b258-368030718e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.backends.cudnn.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fd77d-0f7d-4453-b61d-f131ce7207a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "029f5e27-41d9-436d-aed0-8c6189a23d76",
   "metadata": {},
   "source": [
    "### Use this mfcc extraction(from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8296dc-0e4a-47d0-9872-2ebfdecf31bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (47390, 99, 13)\n",
      "Labels array shape: (47390,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.fftpack import dct  # Import DCT from scipy\n",
    "import librosa  # Ensure librosa is imported for loading audio files\n",
    "\n",
    "# Custom Dataset Class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "def pre_emphasis(signal, alpha=0.97):\n",
    "    \"\"\"Apply pre-emphasis filter.\"\"\"\n",
    "    return np.append(signal[0], signal[1:] - alpha * signal[:-1])\n",
    "\n",
    "def framing(signal, frame_size, hop_size):\n",
    "    \"\"\"Split signal into overlapping frames.\"\"\"\n",
    "    num_frames = int(np.ceil(float(np.abs(len(signal) - frame_size)) / hop_size)) + 1\n",
    "    pad_signal_length = num_frames * hop_size + frame_size\n",
    "    z = np.zeros(pad_signal_length)\n",
    "    z[:len(signal)] = signal\n",
    "    \n",
    "    frames = np.lib.stride_tricks.as_strided(z,\n",
    "        shape=(num_frames, frame_size),\n",
    "        strides=(z.strides[0] * hop_size, z.strides[0])).copy()\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def hamming_window(frame):\n",
    "    \"\"\"Apply Hamming window to a frame.\"\"\"\n",
    "    return np.hamming(len(frame)) * frame\n",
    "\n",
    "def mel_filter_bank(num_filters, fft_size, sample_rate, low_freq=0, high_freq=None):\n",
    "    \"\"\"Create a Mel filter bank.\"\"\"\n",
    "    if high_freq is None:\n",
    "        high_freq = sample_rate / 2\n",
    "    \n",
    "    # Convert frequency to Mel scale\n",
    "    low_mel = 2595 * np.log10(1 + low_freq / 700)\n",
    "    high_mel = 2595 * np.log10(1 + high_freq / 700)\n",
    "    \n",
    "    mel_points = np.linspace(low_mel, high_mel, num_filters + 2)\n",
    "    hz_points = 700 * (10**(mel_points / 2595) - 1)\n",
    "    \n",
    "    bin_points = np.floor((fft_size + 1) * hz_points / sample_rate).astype(int)\n",
    "    \n",
    "    filters = np.zeros((num_filters, int(np.floor(fft_size / 2 + 1))))\n",
    "    \n",
    "    for n in range(1, num_filters + 1):\n",
    "        filters[n - 1, bin_points[n - 1]:bin_points[n]] = \\\n",
    "            (np.arange(bin_points[n - 1], bin_points[n]) - bin_points[n - 1]) / (bin_points[n] - bin_points[n - 1])\n",
    "        filters[n - 1, bin_points[n]:bin_points[n + 1]] = \\\n",
    "            (bin_points[n + 1] - np.arange(bin_points[n], bin_points[n + 1])) / (bin_points[n + 1] - bin_points[n])\n",
    "    \n",
    "    return filters\n",
    "\n",
    "def compute_mfcc(signal, sample_rate=16000, n_mfcc=13, n_fft=400, hop_length=160):\n",
    "    \"\"\"Compute MFCC from scratch.\"\"\"\n",
    "    # Step 1: Pre-emphasis\n",
    "    emphasized_signal = pre_emphasis(signal)\n",
    "\n",
    "    # Step 2: Framing\n",
    "    frames = framing(emphasized_signal, n_fft, hop_length)\n",
    "\n",
    "    # Step 3: Apply Hamming window\n",
    "    windowed_frames = np.array([hamming_window(frame) for frame in frames])\n",
    "\n",
    "    # Step 4: FFT and Power Spectrum\n",
    "    mag_frames = np.abs(np.fft.rfft(windowed_frames, n=n_fft)) ** 2\n",
    "\n",
    "    # Step 5: Mel Filter Bank\n",
    "    mel_filters = mel_filter_bank(n_mfcc, n_fft, sample_rate)\n",
    "    \n",
    "    # Step 6: Apply Mel filter bank to power spectrum\n",
    "    mel_energies = np.dot(mag_frames, mel_filters.T)\n",
    "\n",
    "    # Step 7: Logarithm of Mel energies\n",
    "    log_mel_energies = np.log(mel_energies + np.finfo(float).eps)\n",
    "\n",
    "    # Step 8: Discrete Cosine Transform (DCT)\n",
    "    mfccs = dct(log_mel_energies, type=2, axis=1, norm='ortho')[:, :n_mfcc]\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "def load_data_with_mfcc(directory, n_mfcc=13, n_fft=400, hop_size=160, target_length=16000):\n",
    "    \"\"\"Load data from a directory and extract MFCC features.\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    labels = sorted(os.listdir(directory))\n",
    "    label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    for label in labels:\n",
    "        class_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.endswith('.wav'):\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    signal, rate = librosa.load(file_path, sr=None)  # Load audio to get its length\n",
    "                    \n",
    "                    # Check if the audio signal length is less than the target length (16000 samples)\n",
    "                    if len(signal) < target_length:\n",
    "                        # Pad the signal to 16000 samples if it's too short\n",
    "                        padding = target_length - len(signal)\n",
    "                        signal = np.pad(signal, (0, padding), 'constant')\n",
    "\n",
    "                    # Check if the audio length is greater than the target length (16000 samples)\n",
    "                    if len(signal) > target_length:\n",
    "                        # Truncate the signal to 16000 samples if it's too long\n",
    "                        signal = signal[:target_length]\n",
    "\n",
    "                    audio_length = len(signal)  # Length in samples\n",
    "                    mfcc = compute_mfcc(signal, sample_rate=rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_size)\n",
    "                    num_frames = mfcc.shape[1]\n",
    "\n",
    "                    # Check if the first window is less than 25 ms (400 samples)\n",
    "                    if num_frames > 0 and (num_frames * hop_size < 400):  \n",
    "                        print(f\"Stopping processing for {file_name}: first window is less than 30 ms.\")\n",
    "                        break\n",
    "                    \n",
    "                    X.append(mfcc)\n",
    "                    y.append(label_map[label])\n",
    "                    \n",
    "                    # Display number of frames and audio length for each sample\n",
    "                    #print(f\"File: {file_name}, Label: {label}, Audio Length: {audio_length} samples, Number of frames: {num_frames}\")\n",
    "\n",
    "                    # Print total number of windows for each file\n",
    "                    #print(f\"Total number of windows for {file_name}: {num_frames}\")\n",
    "\n",
    "                    # Print shape of the feature vector (MFCC matrix)\n",
    "                    #print(f\"MFCC feature vector shape for {file_name}: {mfcc.shape}\")\n",
    "\n",
    "            else:\n",
    "                continue  \n",
    "            break  \n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(\"Feature array shape:\", X.shape)  \n",
    "    print(\"Labels array shape:\", y.shape)\n",
    "\n",
    "    return X, y, labels\n",
    "\n",
    "\n",
    "# Section 3: Data Loading and Preprocessing\n",
    "directory = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/20keywordsGSC\"\n",
    "X, y, labels = load_data_with_mfcc(directory)\n",
    "\n",
    "# Reshape X for CNN input (add channel dimension if needed)\n",
    "if X.size > 0:\n",
    "   X = X[:, :, :]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7208e75-8254-4cec-a327-dac61954bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X for CNN input (swap dimensions 1 and 2)\n",
    "if X.size > 0:\n",
    "   X = X.transpose(0, 2, 1)  # Change shape from (23682, 50, 13) to (23682, 13, 50)\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# You can add your model training and evaluation code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ae1395-55d6-4729-8226-33b8b9b64516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47390, 13, 99)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e6a176f-d37a-42ea-92ba-eb827786cead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37912, 13, 99)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0f55a3-b114-48a0-9e1b-0024076d3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RocketClassifierWithPCS(num_kernels=120000, n_pcs=20000, random_state=42)\n",
    "# clf = RocketClassifierWithPCS(num_kernels=1000, n_pcs=200, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49cba261-582a-4b4f-b4bd-004c08d24b02",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 67.8 GiB for an array with shape (37912, 240000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the classifier on the training data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\classification\\base.py:129\u001b[0m, in \u001b[0;36mBaseClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    126\u001b[0m X, y, single_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_setup(X, y)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m single_class:\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_time_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)) \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# this should happen last\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 39\u001b[0m, in \u001b[0;36mRocketClassifierWithPCS._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit Rocket variant and PCA to the training data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call the original RocketClassifier's fit method\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Apply PCA if n_pcs is specified\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_pcs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[1], line 200\u001b[0m, in \u001b[0;36mRocketClassifier._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator \u001b[38;5;241m=\u001b[39m _clone_estimator(\n\u001b[0;32m    185\u001b[0m     (\n\u001b[0;32m    186\u001b[0m         RidgeClassifierCV(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m    193\u001b[0m )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_ \u001b[38;5;241m=\u001b[39m make_pipeline(\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer,\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scaler,\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator,\n\u001b[0;32m    199\u001b[0m )\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:2562\u001b[0m, in \u001b[0;36mRidgeClassifierCV.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2556\u001b[0m \u001b[38;5;66;03m# If cv is None, gcv mode will be used and we used the binarized Y\u001b[39;00m\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;66;03m# since y will not be binarized in _RidgeGCV estimator.\u001b[39;00m\n\u001b[0;32m   2558\u001b[0m \u001b[38;5;66;03m# If cv is not None, a GridSearchCV with some RidgeClassifier\u001b[39;00m\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;66;03m# estimators are used where y will be binarized. Thus, we pass y\u001b[39;00m\n\u001b[0;32m   2560\u001b[0m \u001b[38;5;66;03m# instead of the binarized Y.\u001b[39;00m\n\u001b[0;32m   2561\u001b[0m target \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[1;32m-> 2562\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:2182\u001b[0m, in \u001b[0;36m_BaseRidgeCV.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2173\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m _RidgeGCV(\n\u001b[0;32m   2174\u001b[0m         alphas,\n\u001b[0;32m   2175\u001b[0m         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2180\u001b[0m         alpha_per_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_per_target,\n\u001b[0;32m   2181\u001b[0m     )\n\u001b[1;32m-> 2182\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_ \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39malpha_\n\u001b[0;32m   2184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score_ \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:1978\u001b[0m, in \u001b[0;36m_RidgeGCV.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1974\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas)\n\u001b[1;32m-> 1978\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m \u001b[43m_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1980\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1981\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1983\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1986\u001b[0m gcv_mode \u001b[38;5;241m=\u001b[39m _check_gcv_mode(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcv_mode)\n\u001b[0;32m   1988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gcv_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meigen\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:261\u001b[0m, in \u001b[0;36m_preprocess_data\u001b[1;34m(X, y, fit_intercept, normalize, copy, copy_y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    253\u001b[0m     X_offset, X_var, _ \u001b[38;5;241m=\u001b[39m _incremental_mean_and_var(\n\u001b[0;32m    254\u001b[0m         X,\n\u001b[0;32m    255\u001b[0m         last_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 261\u001b[0m     X_offset \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m X_offset \u001b[38;5;241m=\u001b[39m X_offset\u001b[38;5;241m.\u001b[39mastype(X\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    264\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m X_offset\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:551\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    549\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 551\u001b[0m     avg \u001b[38;5;241m=\u001b[39m avg_as_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(axis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw) \u001b[38;5;241m/\u001b[39m scl\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scl\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m avg_as_array\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 67.8 GiB for an array with shape (37912, 240000) and data type float64"
     ]
    }
   ],
   "source": [
    "# Fit the classifier on the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79050deb-a191-471d-a83e-8b1760179385",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = clf.pipeline_[-1]  \n",
    "\n",
    "# Counting the number of parameters (coef_ and intercept_)\n",
    "n_params = param.coef_.size + param.intercept_.size\n",
    "print(f\"Number of parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22246a57-a8bd-4b29-82ea-c23ef748f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the intermediate data from the pipeline after feature extraction and PCA transformation\n",
    "# Get the transformer part of the pipeline\n",
    "transformer = clf.pipeline_[0]  # This is the Rocket transformer\n",
    "scaler = clf.pipeline_[1]       # StandardScaler\n",
    "pca = clf.pipeline_[2]          # PCA\n",
    "\n",
    "# Transform the X_test data using the same sequence of operations\n",
    "X_test_transformed = transformer.transform(X_test)\n",
    "X_test_scaled = scaler.transform(X_test_transformed)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Save the transformed X_test to a file\n",
    "np.save('X_test_PCA_ROCKET_20K.npy', X_test_pca)\n",
    "\n",
    "print(\"Transformed X_test has been saved as 'X_test_PCA_ROCKET_20K.npy'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2345bc0-8f42-4b75-9d74-d2d9d354159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabc1de-8327-4d3a-976a-b0179dab083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the intermediate data from the pipeline after feature extraction and PCA transformation\n",
    "# Get the transformer part of the pipeline\n",
    "transformer = clf.pipeline_[0]  # This is the Rocket transformer\n",
    "scaler = clf.pipeline_[1]       # StandardScaler\n",
    "pca = clf.pipeline_[2]          # PCA\n",
    "\n",
    "# Transform the X_test data using the same sequence of operations\n",
    "X_train_transformed = transformer.transform(X_train)\n",
    "X_train_scaled = scaler.transform(X_train_transformed)\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "# Save the transformed X_test to a file\n",
    "np.save('X_train_PCA_ROCKET_20K.npy', X_train_pca)\n",
    "\n",
    "print(\"Transformed X_train has been saved as 'X_train_PCA_ROCKET_20K.npy'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59d463-9f68-4cae-ae90-073d2287e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17baa061-6b69-4bc0-ab71-34f79c2b0c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c885bb-8347-4f9e-97bf-2db2541722f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d8564-62df-4634-b9cc-f07f4008a451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d78a81-b31c-4818-9945-32b240a76cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967ce4f-b913-4b01-b655-a8df965f7dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492b79d-65b2-4823-bdb4-f76d622d97bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c5e6f-96e1-44f6-9b83-2ee40e4f0dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd12230-31f0-48c5-a927-8df55447d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test data:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dfc69-1415-4ec6-8fea-e11a20167413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision, recall, and f1 score for each class (macro average)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Precision (macro): {precision}\")\n",
    "print(f\"Recall (macro): {recall}\")\n",
    "print(f\"F1 Score (macro): {f1}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29de87-10ed-4bc8-b7d7-33bb02feaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Class label mapping\n",
    "class_labels = ['down', 'go', 'happy','house', 'left', 'marvin','no', 'off', 'on', 'right', 'seven',\n",
    "                'silence', 'stop', 'three','unknown', 'up', 'yes']\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define an orange colormap\n",
    "cmap = plt.cm.Oranges  # Matches your provided image\n",
    "\n",
    "# Normalize the matrix to scale colors appropriately\n",
    "norm = mcolors.Normalize(vmin=0, vmax=np.max(cm))  # Normalizes based on max value\n",
    "\n",
    "# Create and plot confusion matrix with labels\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap=cmap, ax=ax, values_format='d')\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Update title\n",
    "#plt.title(\"Confusion Matrix with Orange Color Coding\")\n",
    "\n",
    "# Save and show plot\n",
    "plt.savefig('cnn_confusion_matrix_128,256.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9425f-d946-4c4c-904c-36dc9358a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,classification_report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=labels))\n",
    "# Map for labels\n",
    "print(\"Label mapping:\", {idx: label for label, idx in enumerate(labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d3aa6-9887-495b-8b8f-dd893995e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: Define y_test and y_pred\n",
    "# Replace these with your actual labels\n",
    "# y_test = [0, 1, 2, 0, 1, 2]  # True labels\n",
    "# y_pred = [0, 2, 1, 0, 0, 1]  # Predicted labels\n",
    "\n",
    "# Get the unique classes and binarize labels\n",
    "classes = np.unique(y_test)\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "y_pred_bin = label_binarize(y_pred, classes=classes)\n",
    "\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(n_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_pred_bin[:, i])\n",
    "    plt.plot(recall, precision, lw=2, label=f'Class {classes[i]}')\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb42686-7879-4a80-a1df-1dbd7d3c32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed58c6-d73a-420b-b07f-e66be10749a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "n_classes = 12\n",
    "\n",
    "# Binarize the labels for multiclass precision-recall\n",
    "y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "\n",
    "\n",
    "\n",
    "# Predict probabilities\n",
    "y_score = clf.predict_proba(X_test)\n",
    "\n",
    "# Compute Precision-Recall curves for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(recall[i], precision[i], lw=2, label=f'Class {i}')\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision', fontsize=14)\n",
    "plt.title('Precision-Recall Curve (Multiclass)', fontsize=16)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8ffd6-c0fb-4a41-a9ed-df520d171802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Class label mapping\n",
    "class_labels = ['down', 'go', 'left', 'no', 'off', 'on', 'right', \n",
    "                'silence', 'stop', 'unknown', 'up', 'yes']\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define an orange colormap\n",
    "cmap = plt.cm.Oranges  # Matches your provided image\n",
    "\n",
    "# Normalize the matrix to scale colors appropriately\n",
    "norm = mcolors.Normalize(vmin=0, vmax=np.max(cm))  # Normalizes based on max value\n",
    "\n",
    "# Create and plot confusion matrix with labels\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap=cmap, ax=ax, values_format='d')\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Update title\n",
    "#plt.title(\"Confusion Matrix with Orange Color Coding\")\n",
    "\n",
    "# Save and show plot\n",
    "plt.savefig('confusion_matrix_combined.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1408ee-bedc-4ad0-9c18-983490866e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
