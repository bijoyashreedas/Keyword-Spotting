{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80986c08-e5a2-49db-8342-9a590909f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A base class for interval extracting forest estimators.\"\"\"\n",
    "\n",
    "__maintainer__ = []\n",
    "__all__ = [\"BaseIntervalForest\"]\n",
    "\n",
    "import inspect\n",
    "import time\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import BaseEstimator, is_classifier, is_regressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.tree import BaseDecisionTree, DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from aeon.base._base import _clone_estimator\n",
    "from aeon.classification.sklearn import ContinuousIntervalTree\n",
    "from aeon.transformations.base import BaseTransformer\n",
    "from aeon.transformations.collection.interval_based import (\n",
    "    RandomIntervals,\n",
    "    SupervisedIntervals,\n",
    ")\n",
    "from aeon.utils.numba.stats import row_mean, row_slope, row_std\n",
    "from aeon.utils.validation import check_n_jobs\n",
    "\n",
    "\n",
    "class BaseIntervalForest(ABC):\n",
    "    \"\"\"A base class for interval extracting forest estimators.\n",
    "\n",
    "    Allows the implementation of classifiers and regressors along the lines of [1][2][3]\n",
    "    which extract intervals and create an ensemble from the subsequent features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : BaseEstimator or None, default=None\n",
    "        scikit-learn BaseEstimator used to build the interval ensemble. If None, use a\n",
    "        simple decision tree.\n",
    "    n_estimators : int, default=200\n",
    "        Number of estimators to build for the ensemble.\n",
    "    interval_selection_method : \"random\", \"supervised\" or \"random-supervised\",\n",
    "            default=\"random\"\n",
    "        The interval selection transformer to use.\n",
    "            - \"random\" uses a RandomIntervalTransformer.\n",
    "            - \"supervised\" uses a SupervisedIntervalTransformer.\n",
    "            - \"random-supervised\" uses a SupervisedIntervalTransformer with\n",
    "                randomised elements.\n",
    "\n",
    "        Supervised methods can only be used for classification tasks, and require\n",
    "        function inputs for interval_features rather than transformers.\n",
    "    n_intervals : int, str, list or tuple, default=\"sqrt\"\n",
    "        Number of intervals to extract per tree for each series_transformers series.\n",
    "\n",
    "        An int input will extract that number of intervals from the series, while a str\n",
    "        input will return a function of the series length (may differ per\n",
    "        series_transformers output) to extract that number of intervals.\n",
    "        Valid str inputs are:\n",
    "            - \"sqrt\": square root of the series length.\n",
    "            - \"sqrt-div\": sqrt of series length divided by the number\n",
    "                of series_transformers.\n",
    "\n",
    "        A list or tuple of ints and/or strs will extract the number of intervals using\n",
    "        the above rules and sum the results for the final n_intervals. i.e. [4, \"sqrt\"]\n",
    "        will extract sqrt(n_timepoints) + 4 intervals.\n",
    "\n",
    "        Different number of intervals for each series_transformers series can be\n",
    "        specified using a nested list or tuple. Any list or tuple input containing\n",
    "        another list or tuple must be the same length as the number of\n",
    "        series_transformers.\n",
    "\n",
    "        While random interval extraction will extract the n_intervals intervals total\n",
    "        (removing duplicates), supervised intervals will run the supervised extraction\n",
    "        process n_intervals times, returning more intervals than specified.\n",
    "    min_interval_length : int, float, list, or tuple, default=3\n",
    "        Minimum length of intervals to extract from series. float inputs take a\n",
    "        proportion of the series length to use as the minimum interval length.\n",
    "\n",
    "        Different minimum interval lengths for each series_transformers series can be\n",
    "        specified using a list or tuple. Any list or tuple input must be the same length\n",
    "        as the number of series_transformers.\n",
    "    max_interval_length : int, float, list, or tuple, default=np.inf\n",
    "        Maximum length of intervals to extract from series. float inputs take a\n",
    "        proportion of the series length to use as the maximum interval length.\n",
    "\n",
    "        Different maximum interval lengths for each series_transformers series can be\n",
    "        specified using a list or tuple. Any list or tuple input must be the same length\n",
    "        as the number of series_transformers.\n",
    "\n",
    "        Ignored for supervised interval_selection_method inputs.\n",
    "    interval_features : BaseTransformer, callable, list, tuple, or None, default=None\n",
    "        The features to extract from the intervals using transformers or callable\n",
    "        functions. If None, use the mean, standard deviation, and slope of the series.\n",
    "\n",
    "        Both transformers and functions should be able to take a 2D np.ndarray input.\n",
    "        Functions should output a 1d array (the feature for each series), and\n",
    "        transformers should output a 2d array where rows are the features for each\n",
    "        series. A list or tuple of transformers and/or functions will extract all\n",
    "        features and concatenate the output.\n",
    "\n",
    "        Different features for each series_transformers series can be specified using a\n",
    "        nested list or tuple. Any list or tuple input containing another list or tuple\n",
    "        must be the same length as the number of series_transformers.\n",
    "    series_transformers : BaseTransformer, list, tuple, or None, default=None\n",
    "        The transformers to apply to the series before extracting intervals. If None,\n",
    "        use the series as is.\n",
    "\n",
    "        A list or tuple of transformers will extract intervals from\n",
    "        all transformations concatenate the output. Including None in the list or tuple\n",
    "        will use the series as is for interval extraction.\n",
    "    att_subsample_size : int, float, list, tuple or None, default=None\n",
    "        The number of attributes to subsample for each estimator. If None, use all\n",
    "\n",
    "        If int, use that number of attributes for all estimators. If float, use that\n",
    "        proportion of attributes for all estimators.\n",
    "\n",
    "        Different subsample sizes for each series_transformers series can be specified\n",
    "        using a list or tuple. Any list or tuple input must be the same length as the\n",
    "        number of series_transformers.\n",
    "    replace_nan : \"nan\", int, float or None, default=None\n",
    "        The value to replace NaNs and infinite values with before fitting the base\n",
    "        estimator. int or float input will replace with the specified value, while\n",
    "        \"nan\" will replace infinite values with NaNs. If None, do not replace NaNs.\n",
    "    time_limit_in_minutes : int, default=0\n",
    "        Time contract to limit build time in minutes, overriding n_estimators.\n",
    "        Default of 0 means n_estimators are used.\n",
    "    contract_max_n_estimators : int, default=500\n",
    "        Max number of estimators when time_limit_in_minutes is set.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        If `int`, random_state is the seed used by the random number generator;\n",
    "        If `RandomState` instance, random_state is the random number generator;\n",
    "        If `None`, the random number generator is the `RandomState` instance used\n",
    "        by `np.random`.\n",
    "    n_jobs : int, default=1\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``-1`` means using all processors.\n",
    "    parallel_backend : str, ParallelBackendBase instance or None, default=None\n",
    "        Specify the parallelisation backend implementation in joblib, if None a 'prefer'\n",
    "        value of \"threads\" is used by default.\n",
    "        Valid options are \"loky\", \"multiprocessing\", \"threading\" or a custom backend.\n",
    "        See the joblib Parallel documentation for more details.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_cases_ : int\n",
    "        The number of train cases.\n",
    "    n_channels_ : int\n",
    "        The number of channels per case.\n",
    "    n_timepoints_ : int\n",
    "        The length of each series.\n",
    "    total_intervals_ : int\n",
    "        Total number of intervals per tree from all representations.\n",
    "    estimators_ : list of shape (n_estimators) of BaseEstimator\n",
    "        The collections of estimators trained in fit.\n",
    "    intervals_ : list of shape (n_estimators) of BaseTransformer\n",
    "        Stores the interval extraction transformer for all estimators.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] H.Deng, G.Runger, E.Tuv and M.Vladimir, \"A time series forest for\n",
    "       classification and feature extraction\", Information Sciences, 239, 2013\n",
    "    .. [2] Matthew Middlehurst and James Large and Anthony Bagnall. \"The Canonical\n",
    "       Interval Forest (CIF) Classifier for Time Series Classification.\"\n",
    "       IEEE International Conference on Big Data 2020\n",
    "    .. [3] Cabello, Nestor, et al. \"Fast and Accurate Time Series Classification\n",
    "       Through Supervised Interval Search.\" IEEE ICDM 2020\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator=None,\n",
    "        n_estimators=200,\n",
    "        interval_selection_method=\"random\",\n",
    "        n_intervals=\"sqrt\",\n",
    "        min_interval_length=3,\n",
    "        max_interval_length=np.inf,\n",
    "        interval_features=None,\n",
    "        series_transformers=None,\n",
    "        att_subsample_size=None,\n",
    "        replace_nan=None,\n",
    "        time_limit_in_minutes=None,\n",
    "        contract_max_n_estimators=500,\n",
    "        random_state=None,\n",
    "        n_jobs=1,\n",
    "        parallel_backend=None,\n",
    "    ):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.interval_selection_method = interval_selection_method\n",
    "        self.n_intervals = n_intervals\n",
    "        self.min_interval_length = min_interval_length\n",
    "        self.max_interval_length = max_interval_length\n",
    "        self.interval_features = interval_features\n",
    "        self.series_transformers = series_transformers\n",
    "        self.att_subsample_size = att_subsample_size\n",
    "        self.replace_nan = replace_nan\n",
    "        self.time_limit_in_minutes = time_limit_in_minutes\n",
    "        self.contract_max_n_estimators = contract_max_n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.parallel_backend = parallel_backend\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    # if subsampling attributes, an interval_features transformer must contain a\n",
    "    # parameter name from transformer_feature_selection and an attribute name\n",
    "    # (or property) from transformer_feature_names to allow features to be subsampled\n",
    "    transformer_feature_selection = [\"features\"]\n",
    "    transformer_feature_names = [\n",
    "        \"features_arguments_\",\n",
    "        \"_features_arguments\",\n",
    "        \"get_features_arguments\",\n",
    "        \"_get_features_arguments\",\n",
    "    ]\n",
    "    # an interval_features transformer must contain one of these attribute names to\n",
    "    # be able to skip transforming features in predict\n",
    "    transformer_feature_skip = [\"transform_features_\", \"_transform_features\"]\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        if getattr(self, \"__unit_test_flag\", False):\n",
    "            self._transformed_data = self._fit_forest(X, y, save_transformed_data=True)\n",
    "        else:\n",
    "            self._fit_forest(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict(self, X):\n",
    "        if is_regressor(self):\n",
    "            Xt = self._predict_setup(X)\n",
    "\n",
    "            y_preds = Parallel(\n",
    "                n_jobs=self._n_jobs,\n",
    "                backend=self.parallel_backend,\n",
    "                prefer=\"threads\",\n",
    "            )(\n",
    "                delayed(self._predict_for_estimator)(\n",
    "                    Xt,\n",
    "                    self.estimators_[i],\n",
    "                    self.intervals_[i],\n",
    "                    predict_proba=False,\n",
    "                )\n",
    "                for i in range(self._n_estimators)\n",
    "            )\n",
    "\n",
    "            return np.mean(y_preds, axis=0)\n",
    "        else:\n",
    "            return np.array(\n",
    "                [self.classes_[int(np.argmax(prob))] for prob in self._predict_proba(X)]\n",
    "            )\n",
    "\n",
    "    def _predict_proba(self, X):\n",
    "        Xt = self._predict_setup(X)\n",
    "\n",
    "        y_probas = Parallel(\n",
    "            n_jobs=self._n_jobs, backend=self.parallel_backend, prefer=\"threads\"\n",
    "        )(\n",
    "            delayed(self._predict_for_estimator)(\n",
    "                Xt,\n",
    "                self.estimators_[i],\n",
    "                self.intervals_[i],\n",
    "                predict_proba=True,\n",
    "            )\n",
    "            for i in range(self._n_estimators)\n",
    "        )\n",
    "\n",
    "        output = np.sum(y_probas, axis=0) / (\n",
    "            np.ones(self.n_classes_) * self._n_estimators\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def _fit_predict(self, X, y) -> np.ndarray:\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        if is_regressor(self):\n",
    "            Xt = self._fit_forest(X, y, save_transformed_data=True)\n",
    "\n",
    "            p = Parallel(\n",
    "                n_jobs=self._n_jobs, backend=self.parallel_backend, prefer=\"threads\"\n",
    "            )(\n",
    "                delayed(self._train_estimate_for_estimator)(\n",
    "                    Xt,\n",
    "                    y,\n",
    "                    i,\n",
    "                    check_random_state(rng.randint(np.iinfo(np.int32).max)),\n",
    "                )\n",
    "                for i in range(self._n_estimators)\n",
    "            )\n",
    "            y_preds, oobs = zip(*p)\n",
    "\n",
    "            results = np.sum(y_preds, axis=0)\n",
    "            divisors = np.zeros(self.n_cases_)\n",
    "            for oob in oobs:\n",
    "                for inst in oob:\n",
    "                    divisors[inst] += 1\n",
    "\n",
    "            label_average = np.mean(y)\n",
    "            for i in range(self.n_cases_):\n",
    "                results[i] = (\n",
    "                    label_average if divisors[i] == 0 else results[i] / divisors[i]\n",
    "                )\n",
    "        else:\n",
    "            return np.array(\n",
    "                [\n",
    "                    self.classes_[int(rng.choice(np.flatnonzero(prob == prob.max())))]\n",
    "                    for prob in self._fit_predict_proba(X, y)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _fit_predict_proba(self, X, y) -> np.ndarray:\n",
    "        if is_regressor(self):\n",
    "            raise ValueError(\n",
    "                \"Train probability estimates are only available for classification\"\n",
    "            )\n",
    "\n",
    "        Xt = self._fit_forest(X, y, save_transformed_data=True)\n",
    "\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        p = Parallel(\n",
    "            n_jobs=self._n_jobs, backend=self.parallel_backend, prefer=\"threads\"\n",
    "        )(\n",
    "            delayed(self._train_estimate_for_estimator)(\n",
    "                Xt,\n",
    "                y,\n",
    "                i,\n",
    "                check_random_state(rng.randint(np.iinfo(np.int32).max)),\n",
    "                probas=True,\n",
    "            )\n",
    "            for i in range(self._n_estimators)\n",
    "        )\n",
    "        y_probas, oobs = zip(*p)\n",
    "\n",
    "        results = np.sum(y_probas, axis=0)\n",
    "        divisors = np.zeros(self.n_cases_)\n",
    "        for oob in oobs:\n",
    "            for inst in oob:\n",
    "                divisors[inst] += 1\n",
    "\n",
    "        for i in range(self.n_cases_):\n",
    "            results[i] = (\n",
    "                np.ones(self.n_classes_) * (1 / self.n_classes_)\n",
    "                if divisors[i] == 0\n",
    "                else results[i] / (np.ones(self.n_classes_) * divisors[i])\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _fit_forest(self, X, y, save_transformed_data=False):\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        self.n_cases_, self.n_channels_, self.n_timepoints_ = X.shape\n",
    "\n",
    "        self._base_estimator = self.base_estimator\n",
    "        if self.base_estimator is None:\n",
    "            if is_classifier(self):\n",
    "                self._base_estimator = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "            elif is_regressor(self):\n",
    "                self._base_estimator = DecisionTreeRegressor(criterion=\"absolute_error\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"{self} must be a scikit-learn compatible classifier or \"\n",
    "                    \"regressor.\"\n",
    "                )\n",
    "        # base_estimator must be an sklearn estimator\n",
    "        elif not isinstance(self.base_estimator, BaseEstimator):\n",
    "            raise ValueError(\n",
    "                \"base_estimator must be a scikit-learn BaseEstimator or None. \"\n",
    "                f\"Found: {self.base_estimator}\"\n",
    "            )\n",
    "\n",
    "        # use the base series if series_transformers is None\n",
    "        if self.series_transformers is None or self.series_transformers == []:\n",
    "            Xt = [X]\n",
    "            self._series_transformers = [None]\n",
    "        # clone series_transformers if it is a transformer and transform the input data\n",
    "        elif _is_transformer(self.series_transformers):\n",
    "            t = _clone_estimator(self.series_transformers, random_state=rng)\n",
    "            Xt = [t.fit_transform(X, y)]\n",
    "            self._series_transformers = [t]\n",
    "        # clone each series_transformers transformer and include the base series if None\n",
    "        # is in the list\n",
    "        elif isinstance(self.series_transformers, (list, tuple)):\n",
    "            Xt = []\n",
    "            self._series_transformers = []\n",
    "\n",
    "            for transformer in self.series_transformers:\n",
    "                if transformer is None:\n",
    "                    Xt.append(X)\n",
    "                    self._series_transformers.append(None)\n",
    "                elif _is_transformer(transformer):\n",
    "                    t = _clone_estimator(transformer, random_state=rng)\n",
    "                    Xt.append(t.fit_transform(X, y))\n",
    "                    self._series_transformers.append(t)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid series_transformers list input. Found {transformer}\"\n",
    "                    )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid series_transformers input. Found {self.series_transformers}\"\n",
    "            )\n",
    "\n",
    "        # if only a single n_intervals value is passed it must be an int or str\n",
    "        if isinstance(self.n_intervals, (int, str)):\n",
    "            n_intervals = [[self.n_intervals]] * len(Xt)\n",
    "        elif isinstance(self.n_intervals, (list, tuple)):\n",
    "            # if input is a list and only contains ints or strs, use the list for all\n",
    "            # series in Xt\n",
    "            if all(isinstance(item, (int, str)) for item in self.n_intervals):\n",
    "                n_intervals = [self.n_intervals] * len(Xt)\n",
    "            # other lists must be the same length as Xt\n",
    "            elif len(self.n_intervals) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"n_intervals as a list or tuple containing other lists or tuples \"\n",
    "                    \"must be the same length as series_transformers.\"\n",
    "                )\n",
    "            # list items can be a list of items or a single item for each\n",
    "            # series_transformer, but each individual item must be an int or str\n",
    "            else:\n",
    "                n_intervals = []\n",
    "                for items in self.n_intervals:\n",
    "                    if isinstance(items, (list, tuple)):\n",
    "                        if not all(isinstance(item, (int, str)) for item in items):\n",
    "                            raise ValueError(\n",
    "                                \"Individual items in a n_intervals list or tuple must \"\n",
    "                                f\"be an int or str. Input {items} does not contain \"\n",
    "                                \"only ints or strs\"\n",
    "                            )\n",
    "                        n_intervals.append(items)\n",
    "                    elif isinstance(items, (int, str)):\n",
    "                        n_intervals.append([items])\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Individual items in a n_intervals list or tuple must be \"\n",
    "                            f\"an int or str. Found: {items}\"\n",
    "                        )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid n_intervals input. Found {self.n_intervals}\")\n",
    "\n",
    "        # add together the number of intervals for each series_transformer\n",
    "        # str input must be one of a set valid options\n",
    "        self._n_intervals = [0] * len(Xt)\n",
    "        for i, series in enumerate(Xt):\n",
    "            for method in n_intervals[i]:\n",
    "                if isinstance(method, int):\n",
    "                    self._n_intervals[i] += method\n",
    "                elif isinstance(method, str):\n",
    "                    # sqrt of series length\n",
    "                    if method.lower() == \"sqrt\":\n",
    "                        self._n_intervals[i] += int(\n",
    "                            np.sqrt(series.shape[2]) * np.sqrt(series.shape[1])\n",
    "                        )\n",
    "                    # sqrt of series length divided by the number of series_transformers\n",
    "                    elif method.lower() == \"sqrt-div\":\n",
    "                        self._n_intervals[i] += int(\n",
    "                            (np.sqrt(series.shape[2]) * np.sqrt(series.shape[1]))\n",
    "                            / len(Xt)\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Invalid str input for n_intervals. Must be \"\n",
    "                            f'(\"sqrt\",\"sqrt-div\"). Found {method}'\n",
    "                        )\n",
    "\n",
    "        # each series_transformer must have at least 1 interval extracted\n",
    "        for i, n in enumerate(self._n_intervals):\n",
    "            if n <= 0:\n",
    "                self._n_intervals[i] = 1\n",
    "\n",
    "        self.total_intervals_ = sum(self._n_intervals)\n",
    "\n",
    "        # minimum interval length\n",
    "        if isinstance(self.min_interval_length, int):\n",
    "            self._min_interval_length = [self.min_interval_length] * len(Xt)\n",
    "        # min_interval_length must be less than one if it is a float (proportion of\n",
    "        # of the series length)\n",
    "        elif (\n",
    "            isinstance(self.min_interval_length, float)\n",
    "            and self.min_interval_length <= 1\n",
    "        ):\n",
    "            self._min_interval_length = [\n",
    "                int(self.min_interval_length * t.shape[2]) for t in Xt\n",
    "            ]\n",
    "        # if the input is a list, it must be the same length as the number of\n",
    "        # series_transformers\n",
    "        # list values must be ints or floats. The same checks as above are performed\n",
    "        elif isinstance(self.min_interval_length, (list, tuple)):\n",
    "            if len(self.min_interval_length) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"min_interval_length as a list or tuple must be the same length \"\n",
    "                    \"as series_transformers.\"\n",
    "                )\n",
    "\n",
    "            self._min_interval_length = []\n",
    "            for i, length in enumerate(self.min_interval_length):\n",
    "                if isinstance(length, float) and length <= 1:\n",
    "                    self._min_interval_length.append(int(length * Xt[i].shape[2]))\n",
    "                elif isinstance(length, int):\n",
    "                    self._min_interval_length.append(length)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"min_interval_length list items must be int or floats. \"\n",
    "                        f\"Found {length}\"\n",
    "                    )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid min_interval_length input. Found {self.min_interval_length}\"\n",
    "            )\n",
    "\n",
    "        # min_interval_length cannot be less than 3 or greater than the series length\n",
    "        for i, n in enumerate(self._min_interval_length):\n",
    "            if n > Xt[i].shape[2]:\n",
    "                self._min_interval_length[i] = Xt[i].shape[2]\n",
    "            elif n < 3:\n",
    "                self._min_interval_length[i] = 3\n",
    "\n",
    "        # maximum interval length\n",
    "        if (\n",
    "            isinstance(self.max_interval_length, int)\n",
    "            or self.max_interval_length == np.inf\n",
    "        ):\n",
    "            self._max_interval_length = [self.max_interval_length] * len(Xt)\n",
    "        # max_interval_length must be at less than one if it is a float  (proportion of\n",
    "        # of the series length)\n",
    "        elif (\n",
    "            isinstance(self.max_interval_length, float)\n",
    "            and self.max_interval_length <= 1\n",
    "        ):\n",
    "            self._max_interval_length = [\n",
    "                int(self.max_interval_length * t.shape[2]) for t in Xt\n",
    "            ]\n",
    "        # if the input is a list, it must be the same length as the number of\n",
    "        # series_transformers\n",
    "        # list values must be ints or floats. The same checks as above are performed\n",
    "        elif isinstance(self.max_interval_length, (list, tuple)):\n",
    "            if len(self.max_interval_length) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"max_interval_length as a list or tuple must be the same length \"\n",
    "                    \"as series_transformers.\"\n",
    "                )\n",
    "\n",
    "            self._max_interval_length = []\n",
    "            for i, length in enumerate(self.max_interval_length):\n",
    "                if isinstance(length, float) and length <= 1:\n",
    "                    self._max_interval_length.append(int(length * Xt[i].shape[2]))\n",
    "                elif isinstance(length, int):\n",
    "                    self._max_interval_length.append(length)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"max_interval_length list items must be int or floats. \"\n",
    "                        f\"Found {length}\"\n",
    "                    )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid max_interval_length input. Found {self.max_interval_length}\"\n",
    "            )\n",
    "\n",
    "        # max_interval_length cannot be less than min_interval_length or greater than\n",
    "        # the series length\n",
    "        for i, n in enumerate(self._max_interval_length):\n",
    "            if n < self._min_interval_length[i]:\n",
    "                self._max_interval_length[i] = self._min_interval_length[i]\n",
    "            elif n > Xt[i].shape[2]:\n",
    "                self._max_interval_length[i] = Xt[i].shape[2]\n",
    "\n",
    "        # we store whether each series_transformer contains a transformer and/or\n",
    "        # function in its interval_features\n",
    "        self._interval_transformer = [False] * len(Xt)\n",
    "        self._interval_function = [False] * len(Xt)\n",
    "        # single transformer or function for all series_transformers\n",
    "        if isinstance(self.interval_features, BaseTransformer):\n",
    "            self._interval_transformer = [True] * len(Xt)\n",
    "            transformer = _clone_estimator(self.interval_features, random_state=rng)\n",
    "            self._interval_features = [[transformer]] * len(Xt)\n",
    "        elif callable(self.interval_features):\n",
    "            self._interval_function = [True] * len(Xt)\n",
    "            self._interval_features = [[self.interval_features]] * len(Xt)\n",
    "        elif isinstance(self.interval_features, (list, tuple)):\n",
    "            # if input is a list and only contains transformers or functions, use the\n",
    "            # list for all series in Xt\n",
    "            if all(\n",
    "                isinstance(item, BaseTransformer) or callable(item)\n",
    "                for item in self.interval_features\n",
    "            ):\n",
    "                for feature in self.interval_features:\n",
    "                    if isinstance(feature, BaseTransformer):\n",
    "                        self._interval_transformer[0] = True\n",
    "                    elif callable(feature):\n",
    "                        self._interval_function[0] = True\n",
    "                self._interval_features = [self.interval_features] * len(Xt)\n",
    "            # other lists must be the same length as Xt\n",
    "            elif len(self.interval_features) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"interval_features as a list or tuple containing other lists or \"\n",
    "                    \"tuples must be the same length as series_transformers.\"\n",
    "                )\n",
    "            # list items can be a list of items or a single item for each\n",
    "            # series_transformer, but each individual item must be a transformer\n",
    "            # or function\n",
    "            else:\n",
    "                self._interval_features = []\n",
    "                for i, feature in enumerate(self.interval_features):\n",
    "                    if isinstance(feature, (list, tuple)):\n",
    "                        for method in feature:\n",
    "                            if isinstance(method, BaseTransformer):\n",
    "                                self._interval_transformer[i] = True\n",
    "                                feature = _clone_estimator(feature, random_state=rng)\n",
    "                            elif callable(method):\n",
    "                                self._interval_function[i] = True\n",
    "                            else:\n",
    "                                raise ValueError(\n",
    "                                    \"Individual items in a interval_features list or \"\n",
    "                                    \"tuple must be a transformer or function. Input \"\n",
    "                                    f\"{feature} does not contain only transformers and \"\n",
    "                                    f\"functions.\"\n",
    "                                )\n",
    "                        self._interval_features.append(feature)\n",
    "                    elif isinstance(feature, BaseTransformer):\n",
    "                        self._interval_transformer[i] = True\n",
    "                        feature = _clone_estimator(feature, random_state=rng)\n",
    "                        self._interval_features.append([feature])\n",
    "                    elif callable(feature):\n",
    "                        self._interval_function[i] = True\n",
    "                        self._interval_features.append([feature])\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Individual items in a interval_features list or tuple \"\n",
    "                            f\"must be a transformer or function. Found {feature}\"\n",
    "                        )\n",
    "        # use basic summary stats by default if None\n",
    "        elif self.interval_features is None:\n",
    "            self._interval_function = [True] * len(Xt)\n",
    "            self._interval_features = [[row_mean, row_std, row_slope]] * len(Xt)\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid interval_features input. Found {self.interval_features}\"\n",
    "            )\n",
    "\n",
    "        # att_subsample_size must be at least one if it is an int\n",
    "        if isinstance(self.att_subsample_size, int):\n",
    "            if self.att_subsample_size < 1:\n",
    "                raise ValueError(\n",
    "                    \"att_subsample_size must be at least one if it is an int.\"\n",
    "                )\n",
    "\n",
    "            self._att_subsample_size = [self.att_subsample_size] * len(Xt)\n",
    "        # att_subsample_size must be at less than one if it is a float (proportion of\n",
    "        # total attributed to subsample)\n",
    "        elif isinstance(self.att_subsample_size, float):\n",
    "            if self.att_subsample_size > 1 or self.att_subsample_size <= 0:\n",
    "                raise ValueError(\n",
    "                    \"att_subsample_size must be between 0 and 1 if it is a float.\"\n",
    "                )\n",
    "\n",
    "            self._att_subsample_size = [self.att_subsample_size] * len(Xt)\n",
    "        # default is no attribute subsampling with None\n",
    "        elif self.att_subsample_size is None:\n",
    "            self._att_subsample_size = [self.att_subsample_size] * len(Xt)\n",
    "        # if the input is a list, it must be the same length as the number of\n",
    "        # series_transformers\n",
    "        # list values must be ints, floats or None. The same checks as above are\n",
    "        # performed\n",
    "        elif isinstance(self.att_subsample_size, (list, tuple)):\n",
    "            if len(self.att_subsample_size) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"att_subsample_size as a list or tuple must be the same length as \"\n",
    "                    \"series_transformers.\"\n",
    "                )\n",
    "\n",
    "            self._att_subsample_size = []\n",
    "            for ssize in self.att_subsample_size:\n",
    "                if isinstance(ssize, int):\n",
    "                    if ssize < 1:\n",
    "                        raise ValueError(\n",
    "                            \"att_subsample_size in list must be at least one if it is \"\n",
    "                            \"an int.\"\n",
    "                        )\n",
    "\n",
    "                    self._att_subsample_size.append(ssize)\n",
    "                elif isinstance(ssize, float):\n",
    "                    if ssize > 1:\n",
    "                        raise ValueError(\n",
    "                            \"att_subsample_size in list must be between 0 and 1 if it \"\n",
    "                            \"is a \"\n",
    "                            \"float.\"\n",
    "                        )\n",
    "\n",
    "                    self._att_subsample_size.append(ssize)\n",
    "                elif ssize is None:\n",
    "                    self._att_subsample_size.append(ssize)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Invalid interval_features input in list. Found \"\n",
    "                        f\"{self.att_subsample_size}\"\n",
    "                    )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid interval_features input. Found {self.att_subsample_size}\"\n",
    "            )\n",
    "\n",
    "        # if we are subsampling attributes for a series_transformer and it uses a\n",
    "        # BaseTransformer, we must ensure it has the required parameters and\n",
    "        # attributes to do so\n",
    "        self._transformer_feature_selection = [[]] * len(Xt)\n",
    "        self._transformer_feature_names = [[]] * len(Xt)\n",
    "        for r, att_subsample in enumerate(self._att_subsample_size):\n",
    "            if att_subsample is not None:\n",
    "                for transformer in self._interval_features[r]:\n",
    "                    if isinstance(transformer, BaseTransformer):\n",
    "                        params = inspect.signature(transformer.__init__).parameters\n",
    "\n",
    "                        # the transformer must have a parameter with one of the\n",
    "                        # names listed in transformer_feature_selection as a way to\n",
    "                        # select which features the transformer should transform\n",
    "                        has_params = False\n",
    "                        for n in self.transformer_feature_selection:\n",
    "                            if params.get(n, None) is not None:\n",
    "                                has_params = True\n",
    "                                self._transformer_feature_selection[r].append(n)\n",
    "                                break\n",
    "\n",
    "                        if not has_params:\n",
    "                            raise ValueError(\n",
    "                                \"All transformers in interval_features must have a \"\n",
    "                                \"parameter named in transformer_feature_selection to \"\n",
    "                                \"be used in attribute subsampling.\"\n",
    "                            )\n",
    "\n",
    "                        # the transformer must have an attribute with one of the\n",
    "                        # names listed in transformer_feature_names as a list or tuple\n",
    "                        # of valid options for the previous parameter\n",
    "                        has_feature_names = False\n",
    "                        for n in self.transformer_feature_names:\n",
    "                            if hasattr(transformer, n) and isinstance(\n",
    "                                getattr(transformer, n), (list, tuple)\n",
    "                            ):\n",
    "                                has_feature_names = True\n",
    "                                self._transformer_feature_names[r].append(n)\n",
    "                                break\n",
    "\n",
    "                        if not has_feature_names:\n",
    "                            raise ValueError(\n",
    "                                \"All transformers in interval_features must have an \"\n",
    "                                \"attribute or property named in \"\n",
    "                                \"transformer_feature_names to be used in attribute \"\n",
    "                                \"subsampling.\"\n",
    "                            )\n",
    "\n",
    "        # verify the interval_selection_method is a valid string\n",
    "        if isinstance(self.interval_selection_method, str):\n",
    "            # SupervisedIntervals cannot currently handle transformers or regression\n",
    "            if (\n",
    "                self.interval_selection_method.lower() == \"supervised\"\n",
    "                or self.interval_selection_method.lower() == \"random-supervised\"\n",
    "            ):\n",
    "                if any(self._interval_transformer):\n",
    "                    raise ValueError(\n",
    "                        \"Supervised interval_selection_method must only have function \"\n",
    "                        \"inputs for interval_features.\"\n",
    "                    )\n",
    "\n",
    "                if is_regressor(self):\n",
    "                    raise ValueError(\n",
    "                        \"Supervised interval_selection_method cannot be used for \"\n",
    "                        \"regression.\"\n",
    "                    )\n",
    "            # RandomIntervals\n",
    "            elif not self.interval_selection_method.lower() == \"random\":\n",
    "                raise ValueError(\n",
    "                    'Unknown interval_selection_method, must be one of (\"random\",'\n",
    "                    '\"supervised\",\"random-supervised\"). '\n",
    "                    f\"Found: {self.interval_selection_method}\"\n",
    "                )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Unknown interval_selection_method, must be one of (\"random\",'\n",
    "                '\"supervised\",\"random-supervised\"). '\n",
    "                f\"Found: {self.interval_selection_method}\"\n",
    "            )\n",
    "\n",
    "        # verify replace_nan is a valid string, number or None\n",
    "        if (\n",
    "            (not isinstance(self.replace_nan, str) or self.replace_nan.lower() != \"nan\")\n",
    "            and not isinstance(self.replace_nan, (int, float))\n",
    "            and self.replace_nan is not None\n",
    "        ):\n",
    "            raise ValueError(f\"Invalid replace_nan input. Found {self.replace_nan}\")\n",
    "\n",
    "        self._n_jobs = check_n_jobs(self.n_jobs)\n",
    "\n",
    "        if self.time_limit_in_minutes is not None and self.time_limit_in_minutes > 0:\n",
    "            time_limit = self.time_limit_in_minutes * 60\n",
    "            start_time = time.time()\n",
    "            train_time = 0\n",
    "\n",
    "            self._n_estimators = 0\n",
    "            self.estimators_ = []\n",
    "            self.intervals_ = []\n",
    "            transformed_intervals = []\n",
    "\n",
    "            while (\n",
    "                train_time < time_limit\n",
    "                and self._n_estimators < self.contract_max_n_estimators\n",
    "            ):\n",
    "                fit = Parallel(\n",
    "                    n_jobs=self._n_jobs,\n",
    "                    backend=self.parallel_backend,\n",
    "                    prefer=\"threads\",\n",
    "                )(\n",
    "                    delayed(self._fit_estimator)(\n",
    "                        Xt,\n",
    "                        y,\n",
    "                        rng.randint(np.iinfo(np.int32).max),\n",
    "                        save_transformed_data=save_transformed_data,\n",
    "                    )\n",
    "                    for _ in range(self._n_jobs)\n",
    "                )\n",
    "\n",
    "                (\n",
    "                    estimators,\n",
    "                    intervals,\n",
    "                    td,\n",
    "                ) = zip(*fit)\n",
    "\n",
    "                self.estimators_ += estimators\n",
    "                self.intervals_ += intervals\n",
    "                transformed_intervals += td\n",
    "\n",
    "                self._n_estimators += self._n_jobs\n",
    "                train_time = time.time() - start_time\n",
    "        else:\n",
    "            self._n_estimators = self.n_estimators\n",
    "\n",
    "            fit = Parallel(\n",
    "                n_jobs=self._n_jobs,\n",
    "                backend=self.parallel_backend,\n",
    "                prefer=\"threads\",\n",
    "            )(\n",
    "                delayed(self._fit_estimator)(\n",
    "                    Xt,\n",
    "                    y,\n",
    "                    rng.randint(np.iinfo(np.int32).max),\n",
    "                    save_transformed_data=save_transformed_data,\n",
    "                )\n",
    "                for _ in range(self._n_estimators)\n",
    "            )\n",
    "\n",
    "            (\n",
    "                self.estimators_,\n",
    "                self.intervals_,\n",
    "                transformed_intervals,\n",
    "            ) = zip(*fit)\n",
    "\n",
    "        return transformed_intervals\n",
    "\n",
    "    def _fit_estimator(self, Xt, y, seed, save_transformed_data=False):\n",
    "        # random state for this estimator\n",
    "        rng = check_random_state(seed)\n",
    "\n",
    "        intervals = []\n",
    "        transform_data_lengths = []\n",
    "        interval_features = np.empty((self.n_cases_, 0))\n",
    "\n",
    "        # for each transformed series\n",
    "        for r in range(len(Xt)):\n",
    "            # subsample attributes if enabled\n",
    "            if self._att_subsample_size[r] is not None:\n",
    "                # separate transformers and functions in separate lists\n",
    "                # add the feature names of transformers to a list to subsample from\n",
    "                # and calculate the total number of features\n",
    "                all_transformers = []\n",
    "                all_transformer_features = []\n",
    "                all_function_features = []\n",
    "                for feature in self._interval_features[r]:\n",
    "                    if isinstance(feature, BaseTransformer):\n",
    "                        all_transformer_features += getattr(\n",
    "                            feature,\n",
    "                            self._transformer_feature_names[r][len(all_transformers)],\n",
    "                        )\n",
    "                        all_transformers.append(feature)\n",
    "                    else:\n",
    "                        all_function_features.append(feature)\n",
    "\n",
    "                # handle float subsample size\n",
    "                num_features = len(all_transformer_features) + len(\n",
    "                    all_function_features\n",
    "                )\n",
    "                att_subsample_size = self._att_subsample_size[r]\n",
    "                if isinstance(self._att_subsample_size[r], float):\n",
    "                    att_subsample_size = int(att_subsample_size * num_features)\n",
    "\n",
    "                # if the att_subsample_size is greater than the number of features\n",
    "                # give a warning and add all features\n",
    "                features = []\n",
    "                if att_subsample_size < num_features:\n",
    "                    # subsample the transformer and function features by index\n",
    "                    atts = rng.choice(\n",
    "                        num_features,\n",
    "                        att_subsample_size,\n",
    "                        replace=False,\n",
    "                    )\n",
    "                    atts.sort()\n",
    "\n",
    "                    # subsample the feature transformers using the\n",
    "                    # transformer_feature_names and transformer_feature_selection\n",
    "                    # attributes.\n",
    "                    # the presence of valid attributes is verified in fit.\n",
    "                    count = 0\n",
    "                    length = 0\n",
    "                    for n, transformer in enumerate(all_transformers):\n",
    "                        this_len = len(\n",
    "                            getattr(transformer, self._transformer_feature_names[r][n])\n",
    "                        )\n",
    "                        length += this_len\n",
    "\n",
    "                        # subsample feature names from this transformer\n",
    "                        t_features = []\n",
    "                        while count < len(atts) and atts[count] < length:\n",
    "                            t_features.append(\n",
    "                                getattr(\n",
    "                                    transformer,\n",
    "                                    self._transformer_feature_names[r][n],\n",
    "                                )[atts[count] + this_len - length]\n",
    "                            )\n",
    "                            count += 1\n",
    "\n",
    "                        # tell this transformer to only transform the selected features\n",
    "                        if len(t_features) > 0:\n",
    "                            new_transformer = _clone_estimator(transformer, seed)\n",
    "                            setattr(\n",
    "                                new_transformer,\n",
    "                                self._transformer_feature_selection[r][n],\n",
    "                                t_features,\n",
    "                            )\n",
    "                            features.append(new_transformer)\n",
    "\n",
    "                    # subsample the remaining function features\n",
    "                    for i in range(att_subsample_size - count):\n",
    "                        features.append(all_function_features[atts[count + i] - length])\n",
    "                else:\n",
    "                    # only warn if requested number of features is greater than actual\n",
    "                    if att_subsample_size > num_features:\n",
    "                        warnings.warn(\n",
    "                            f\"Attribute subsample size {att_subsample_size} is \"\n",
    "                            f\"larger than the number of attributes {num_features} \"\n",
    "                            f\"for series {self._series_transformers[r]}\",\n",
    "                            stacklevel=2,\n",
    "                        )\n",
    "\n",
    "                    self._att_subsample_size[r] = None\n",
    "\n",
    "                    for feature in self._interval_features[r]:\n",
    "                        if isinstance(feature, BaseTransformer):\n",
    "                            features.append(_clone_estimator(feature, seed))\n",
    "                        else:\n",
    "                            features.append(feature)\n",
    "            # add all features while cloning estimators if not subsampling\n",
    "            else:\n",
    "                features = []\n",
    "                for feature in self._interval_features[r]:\n",
    "                    if isinstance(feature, BaseTransformer):\n",
    "                        features.append(_clone_estimator(feature, seed))\n",
    "                    else:\n",
    "                        features.append(feature)\n",
    "\n",
    "            # create the selected interval selector and set its parameters\n",
    "            if self.interval_selection_method == \"random\":\n",
    "                selector = RandomIntervals(\n",
    "                    n_intervals=self._n_intervals[r],\n",
    "                    min_interval_length=self._min_interval_length[r],\n",
    "                    max_interval_length=self._max_interval_length[r],\n",
    "                    features=features,\n",
    "                    random_state=seed,\n",
    "                )\n",
    "            elif self.interval_selection_method == \"supervised\":\n",
    "                selector = SupervisedIntervals(\n",
    "                    n_intervals=self._n_intervals[r],\n",
    "                    min_interval_length=self._min_interval_length[r],\n",
    "                    features=features,\n",
    "                    randomised_split_point=False,\n",
    "                    random_state=seed,\n",
    "                )\n",
    "            elif self.interval_selection_method == \"random-supervised\":\n",
    "                selector = SupervisedIntervals(\n",
    "                    n_intervals=self._n_intervals[r],\n",
    "                    min_interval_length=self._min_interval_length[r],\n",
    "                    features=features,\n",
    "                    randomised_split_point=True,\n",
    "                    random_state=seed,\n",
    "                )\n",
    "\n",
    "            # fit the interval selector, transform the current series using it and save\n",
    "            # the transformer\n",
    "            intervals.append(selector)\n",
    "            f = intervals[r].fit_transform(Xt[r], y)\n",
    "\n",
    "            # concatenate the data and save this transforms number of attributes\n",
    "            transform_data_lengths.append(f.shape[1])\n",
    "            interval_features = np.hstack((interval_features, f))\n",
    "\n",
    "        if isinstance(self.replace_nan, str) and self.replace_nan.lower() == \"nan\":\n",
    "            interval_features = np.nan_to_num(\n",
    "                interval_features, False, np.nan, np.nan, np.nan\n",
    "            )\n",
    "        elif isinstance(self.replace_nan, (int, float)):\n",
    "            interval_features = np.nan_to_num(\n",
    "                interval_features,\n",
    "                False,\n",
    "                self.replace_nan,\n",
    "                self.replace_nan,\n",
    "                self.replace_nan,\n",
    "            )\n",
    "\n",
    "        # clone and fit the base estimator using the transformed data\n",
    "        tree = _clone_estimator(self._base_estimator, random_state=seed)\n",
    "        tree.fit(interval_features, y)\n",
    "\n",
    "        # find the features used in the tree and inform the interval selectors to not\n",
    "        # transform these features if possible\n",
    "        self._efficient_predictions = True\n",
    "        relevant_features = None\n",
    "        if isinstance(tree, BaseDecisionTree):\n",
    "            relevant_features = np.unique(tree.tree_.feature[tree.tree_.feature >= 0])\n",
    "        elif isinstance(tree, ContinuousIntervalTree):\n",
    "            relevant_features, _ = tree.tree_node_splits_and_gain()\n",
    "\n",
    "        if relevant_features is not None:\n",
    "            features_to_transform = [False] * interval_features.shape[1]\n",
    "            for i in relevant_features:\n",
    "                features_to_transform[i] = True\n",
    "\n",
    "            count = 0\n",
    "            for r in range(len(Xt)):\n",
    "                intervals[r].transformer_feature_skip = self.transformer_feature_skip\n",
    "\n",
    "                # if the transformers don't have valid attributes to skip False is\n",
    "                # returned\n",
    "                completed = intervals[r].set_features_to_transform(\n",
    "                    features_to_transform[count : count + transform_data_lengths[r]],\n",
    "                    raise_error=False,\n",
    "                )\n",
    "                count += transform_data_lengths[r]\n",
    "\n",
    "                if not completed:\n",
    "                    self._efficient_predictions = False\n",
    "        else:\n",
    "            self._efficient_predictions = False\n",
    "\n",
    "        return [\n",
    "            tree,\n",
    "            intervals,\n",
    "            interval_features if save_transformed_data else None,\n",
    "        ]\n",
    "\n",
    "    def _predict_setup(self, X):\n",
    "        Xt = []\n",
    "        for transformer in self._series_transformers:\n",
    "            if transformer is None:\n",
    "                Xt.append(X)\n",
    "            elif _is_transformer(transformer):\n",
    "                Xt.append(transformer.transform(X))\n",
    "\n",
    "        return Xt\n",
    "\n",
    "    def _predict_for_estimator(self, Xt, estimator, intervals, predict_proba=False):\n",
    "        interval_features = np.empty((Xt[0].shape[0], 0))\n",
    "\n",
    "        for r in range(len(Xt)):\n",
    "            f = intervals[r].transform(Xt[r])\n",
    "            interval_features = np.hstack((interval_features, f))\n",
    "\n",
    "        if isinstance(self.replace_nan, str) and self.replace_nan.lower() == \"nan\":\n",
    "            interval_features = np.nan_to_num(\n",
    "                interval_features, False, np.nan, np.nan, np.nan\n",
    "            )\n",
    "        elif isinstance(self.replace_nan, (int, float)):\n",
    "            interval_features = np.nan_to_num(\n",
    "                interval_features,\n",
    "                False,\n",
    "                self.replace_nan,\n",
    "                self.replace_nan,\n",
    "                self.replace_nan,\n",
    "            )\n",
    "\n",
    "        if predict_proba:\n",
    "            return estimator.predict_proba(interval_features)\n",
    "        else:\n",
    "            return estimator.predict(interval_features)\n",
    "\n",
    "    def _train_estimate_for_estimator(self, Xt, y, idx, rng, probas=False):\n",
    "        indices = range(self.n_cases_)\n",
    "        subsample = rng.choice(self.n_cases_, size=self.n_cases_)\n",
    "        oob = [n for n in indices if n not in subsample]\n",
    "\n",
    "        results = (\n",
    "            np.zeros((self.n_cases_, self.n_classes_))\n",
    "            if probas\n",
    "            else np.zeros(self.n_cases_)\n",
    "        )\n",
    "        if len(oob) == 0:\n",
    "            return [results, oob]\n",
    "\n",
    "        clf = _clone_estimator(self._base_estimator, rng)\n",
    "        clf.fit(Xt[idx][subsample], y[subsample])\n",
    "        preds = clf.predict_proba(Xt[idx][oob]) if probas else clf.predict(Xt[idx][oob])\n",
    "\n",
    "        if probas and preds.shape[1] != self.n_classes_:\n",
    "            new_probas = np.zeros((preds.shape[0], self.n_classes_))\n",
    "            for i, cls in enumerate(clf.classes_):\n",
    "                cls_idx = self._class_dictionary[cls]\n",
    "                new_probas[:, cls_idx] = preds[:, i]\n",
    "            preds = new_probas\n",
    "\n",
    "        if probas:\n",
    "            for n, proba in enumerate(preds):\n",
    "                results[oob[n]] += proba\n",
    "        else:\n",
    "            for n, pred in enumerate(preds):\n",
    "                results[oob[n]] = pred\n",
    "\n",
    "        return [results, oob]\n",
    "\n",
    "    def temporal_importance_curves(\n",
    "        self, return_dict=False, normalise_time_points=False\n",
    "    ):\n",
    "        \"\"\"Calculate the temporal importance curves for each feature.\n",
    "\n",
    "        Can be finicky with transformers currently.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        return_dict : bool, default=False\n",
    "            If True, return a dictionary of curves. If False, return a list of names\n",
    "            and a list of curves.\n",
    "        normalise_time_points : bool, default=False\n",
    "            If True, normalise the time points for each feature to the number of\n",
    "            splits that used that feature. If False, return the sum of the information\n",
    "            gain for each split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        names : list of str\n",
    "            The names of the features.\n",
    "        curves : list of np.ndarray\n",
    "            The temporal importance curves for each feature.\n",
    "        \"\"\"\n",
    "        if is_regressor(self):\n",
    "            raise NotImplementedError(\n",
    "                \"Temporal importance curves are not available for regression.\"\n",
    "            )\n",
    "        if not isinstance(self._base_estimator, ContinuousIntervalTree):\n",
    "            raise ValueError(\n",
    "                \"base_estimator for temporal importance curves must\"\n",
    "                \" be ContinuousIntervalTree.\"\n",
    "            )\n",
    "\n",
    "        curves = {}\n",
    "        if normalise_time_points:\n",
    "            counts = {}\n",
    "\n",
    "        for i, est in enumerate(self.estimators_):\n",
    "            splits, gains = est.tree_node_splits_and_gain()\n",
    "            split_features = []\n",
    "\n",
    "            for n, rep in enumerate(self.intervals_[i]):\n",
    "                t = 0\n",
    "                rep_name = (\n",
    "                    \"\"\n",
    "                    if self._series_transformers[n] is None\n",
    "                    else self._series_transformers[n].__class__.__name__\n",
    "                )\n",
    "\n",
    "                for interval in rep.intervals_:\n",
    "                    if _is_transformer(interval[3]):\n",
    "                        if self._att_subsample_size[n] is None:\n",
    "                            names = None\n",
    "                            for f in self.transformer_feature_names:\n",
    "                                if hasattr(interval[3], f) and isinstance(\n",
    "                                    getattr(interval[3], f), (list, tuple)\n",
    "                                ):\n",
    "                                    names = getattr(interval[3], f)\n",
    "                                    break\n",
    "\n",
    "                            if names is None:\n",
    "                                raise ValueError(\n",
    "                                    \"All transformers in interval_features must have \"\n",
    "                                    \"an attribute or property named in \"\n",
    "                                    \"transformer_feature_names to be used in temporal \"\n",
    "                                    \"importance curves.\"\n",
    "                                )\n",
    "                        else:\n",
    "                            if t % len(self._interval_features[n]) - 1 == 0:\n",
    "                                t = 0\n",
    "\n",
    "                            names = getattr(\n",
    "                                interval[3], self._transformer_feature_names[n][t]\n",
    "                            )\n",
    "                            t += 1\n",
    "\n",
    "                        split_features.extend(\n",
    "                            [\n",
    "                                (\n",
    "                                    rep_name,\n",
    "                                    interval[0],\n",
    "                                    interval[1],\n",
    "                                    interval[2],\n",
    "                                    feature_name,\n",
    "                                )\n",
    "                                for feature_name in names\n",
    "                            ]\n",
    "                        )\n",
    "                    else:\n",
    "                        split_features.append(\n",
    "                            (\n",
    "                                rep_name,\n",
    "                                interval[0],\n",
    "                                interval[1],\n",
    "                                interval[2],\n",
    "                                interval[3].__name__,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            for n, split in enumerate(splits):\n",
    "                feature = (\n",
    "                    split_features[split][0],\n",
    "                    split_features[split][3],\n",
    "                    split_features[split][4],\n",
    "                )\n",
    "\n",
    "                if feature not in curves:\n",
    "                    curves[feature] = np.zeros(self.n_timepoints_)\n",
    "                    curves[feature][\n",
    "                        split_features[split][1] : split_features[split][2]\n",
    "                    ] = gains[n]\n",
    "\n",
    "                    if normalise_time_points:\n",
    "                        counts[feature] = np.zeros(self.n_timepoints_)\n",
    "                        counts[feature][\n",
    "                            split_features[split][1] : split_features[split][2]\n",
    "                        ] = 1\n",
    "                else:\n",
    "                    curves[feature][\n",
    "                        split_features[split][1] : split_features[split][2]\n",
    "                    ] += gains[n]\n",
    "\n",
    "                    if normalise_time_points:\n",
    "                        counts[feature][\n",
    "                            split_features[split][1] : split_features[split][2]\n",
    "                        ] += 1\n",
    "\n",
    "        if normalise_time_points:\n",
    "            for feature in counts:\n",
    "                curves[feature] /= counts[feature]\n",
    "\n",
    "        if return_dict:\n",
    "            return curves\n",
    "        else:\n",
    "            names = []\n",
    "            values = []\n",
    "            for key, value in curves.items():\n",
    "                dim = f\"_dim{key[1]}\" if self.n_channels_ > 1 else \"\"\n",
    "                rep = f\"{key[0]}_\" if key[0] != \"\" else \"\"\n",
    "                names.append(f\"{rep}{key[2]}{dim}\")\n",
    "                values.append(value)\n",
    "\n",
    "            names, values = zip(*sorted(zip(names, values)))\n",
    "\n",
    "            return names, values\n",
    "\n",
    "\n",
    "def _is_transformer(obj):\n",
    "    if isinstance(obj, BaseTransformer) or isinstance(obj, FunctionTransformer):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d98574-3722-4c34-91e2-53cfb9ea810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Time Series Forest (TSF) Classifier.\n",
    "\n",
    "Interval-based TSF classifier, extracts basic summary features from random intervals.\n",
    "\"\"\"\n",
    "\n",
    "__maintainer__ = []\n",
    "__all__ = [\"TimeSeriesForestClassifier\"]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from aeon.base._estimators.interval_based.base_interval_forest import BaseIntervalForest\n",
    "from aeon.classification import BaseClassifier\n",
    "from aeon.classification.sklearn import ContinuousIntervalTree\n",
    "\n",
    "\n",
    "class TimeSeriesForestClassifier(BaseIntervalForest, BaseClassifier):\n",
    "    \"\"\"Time series forest (TSF) classifier.\n",
    "\n",
    "    Time series forest is an ensemble of decision trees built on random intervals [1]_.\n",
    "    Overview: Input n series length m.\n",
    "    For each tree\n",
    "        - sample sqrt(m) intervals,\n",
    "        - find mean, std and slope for each interval, concatenate to form new\n",
    "        data set,\n",
    "        - build a decision tree on new data set.\n",
    "    Ensemble the trees with averaged probability estimates.\n",
    "\n",
    "    This implementation deviates from the original in minor ways. It samples\n",
    "    intervals with replacement and does not use the tree splitting criteria\n",
    "    refinement described in [1] (this can be done with the CITClassifier base\n",
    "    estimator).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : BaseEstimator or None, default=None\n",
    "        scikit-learn BaseEstimator used to build the interval ensemble. If None, use a\n",
    "        simple decision tree.\n",
    "    n_estimators : int, default=200\n",
    "        Number of estimators to build for the ensemble.\n",
    "    n_intervals : int, str, list or tuple, default=\"sqrt\"\n",
    "        Number of intervals to extract per tree for each series_transformers series.\n",
    "\n",
    "        An int input will extract that number of intervals from the series, while a str\n",
    "        input will return a function of the series length (may differ per\n",
    "        series_transformers output) to extract that number of intervals.\n",
    "        Valid str inputs are:\n",
    "            - \"sqrt\": square root of the series length.\n",
    "            - \"sqrt-div\": sqrt of series length divided by the number\n",
    "                of series_transformers.\n",
    "\n",
    "        A list or tuple of ints and/or strs will extract the number of intervals using\n",
    "        the above rules and sum the results for the final n_intervals. i.e. [4, \"sqrt\"]\n",
    "        will extract sqrt(n_timepoints) + 4 intervals.\n",
    "\n",
    "        Different number of intervals for each series_transformers series can be\n",
    "        specified using a nested list or tuple. Any list or tuple input containing\n",
    "        another list or tuple must be the same length as the number of\n",
    "        series_transformers.\n",
    "\n",
    "        While random interval extraction will extract the n_intervals intervals total\n",
    "        (removing duplicates), supervised intervals will run the supervised extraction\n",
    "        process n_intervals times, returning more intervals than specified.\n",
    "    min_interval_length : int, float, list, or tuple, default=3\n",
    "        Minimum length of intervals to extract from series. float inputs take a\n",
    "        proportion of the series length to use as the minimum interval length.\n",
    "\n",
    "        Different minimum interval lengths for each series_transformers series can be\n",
    "        specified using a list or tuple. Any list or tuple input must be the same length\n",
    "        as the number of series_transformers.\n",
    "    max_interval_length : int, float, list, or tuple, default=np.inf\n",
    "        Maximum length of intervals to extract from series. float inputs take a\n",
    "        proportion of the series length to use as the maximum interval length.\n",
    "\n",
    "        Different maximum interval lengths for each series_transformers series can be\n",
    "        specified using a list or tuple. Any list or tuple input must be the same length\n",
    "        as the number of series_transformers.\n",
    "\n",
    "        Ignored for supervised interval_selection_method inputs.\n",
    "    time_limit_in_minutes : int, default=0\n",
    "        Time contract to limit build time in minutes, overriding n_estimators.\n",
    "        Default of 0 means n_estimators are used.\n",
    "    contract_max_n_estimators : int, default=500\n",
    "        Max number of estimators when time_limit_in_minutes is set.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        If `int`, random_state is the seed used by the random number generator;\n",
    "        If `RandomState` instance, random_state is the random number generator;\n",
    "        If `None`, the random number generator is the `RandomState` instance used\n",
    "        by `np.random`.\n",
    "    n_jobs : int, default=1\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``-1`` means using all processors.\n",
    "    parallel_backend : str, ParallelBackendBase instance or None, default=None\n",
    "        Specify the parallelisation backend implementation in joblib, if None a 'prefer'\n",
    "        value of \"threads\" is used by default.\n",
    "        Valid options are \"loky\", \"multiprocessing\", \"threading\" or a custom backend.\n",
    "        See the joblib Parallel documentation for more details.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_cases_ : int\n",
    "        The number of train cases in the training set.\n",
    "    n_channels_ : int\n",
    "        The number of dimensions per case in the training set.\n",
    "    n_timepoints_ : int\n",
    "        The length of each series in the training set.\n",
    "    n_classes_ : int\n",
    "        Number of classes. Extracted from the data.\n",
    "    classes_ : ndarray of shape (n_classes_)\n",
    "        Holds the label for each class.\n",
    "    total_intervals_ : int\n",
    "        Total number of intervals per tree from all representations.\n",
    "    estimators_ : list of shape (n_estimators) of BaseEstimator\n",
    "        The collections of estimators trained in fit.\n",
    "    intervals_ : list of shape (n_estimators) of BaseCollectionTransformer\n",
    "        Stores the interval extraction transformer for all estimators.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For the Java version, see\n",
    "    `TSML <https://github.com/uea-machine-learning/tsml/blob/master/src/main/\n",
    "     java/tsml/classifiers/interval_based/TSF.java>`_.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] H.Deng, G.Runger, E.Tuv and M.Vladimir, \"A time series forest for\n",
    "       classification and feature extraction\", Information Sciences, 239, 2013\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from aeon.classification.interval_based import TimeSeriesForestClassifier\n",
    "    >>> from aeon.testing.data_generation import make_example_3d_numpy\n",
    "    >>> X, y = make_example_3d_numpy(n_cases=10, n_channels=1, n_timepoints=12,\n",
    "    ...                              return_y=True, random_state=0)\n",
    "    >>> clf = TimeSeriesForestClassifier(n_estimators=10, random_state=0)\n",
    "    >>> clf.fit(X, y)\n",
    "    TimeSeriesForestClassifier(n_estimators=10, random_state=0)\n",
    "    >>> clf.predict(X)\n",
    "    array([0, 1, 0, 1, 0, 0, 1, 1, 1, 0])\n",
    "    \"\"\"\n",
    "\n",
    "    _tags = {\n",
    "        \"capability:multivariate\": True,\n",
    "        \"capability:train_estimate\": True,\n",
    "        \"capability:contractable\": True,\n",
    "        \"capability:multithreading\": True,\n",
    "        \"algorithm_type\": \"interval\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator=None,\n",
    "        n_estimators=200,\n",
    "        n_intervals=\"sqrt\",\n",
    "        min_interval_length=3,\n",
    "        max_interval_length=np.inf,\n",
    "        time_limit_in_minutes=None,\n",
    "        contract_max_n_estimators=500,\n",
    "        random_state=None,\n",
    "        n_jobs=1,\n",
    "        parallel_backend=None,\n",
    "    ):\n",
    "        if isinstance(base_estimator, ContinuousIntervalTree):\n",
    "            replace_nan = \"nan\"\n",
    "        else:\n",
    "            replace_nan = 0\n",
    "\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            interval_selection_method=\"random\",\n",
    "            n_intervals=n_intervals,\n",
    "            min_interval_length=min_interval_length,\n",
    "            max_interval_length=max_interval_length,\n",
    "            interval_features=None,\n",
    "            series_transformers=None,\n",
    "            att_subsample_size=None,\n",
    "            replace_nan=replace_nan,\n",
    "            time_limit_in_minutes=time_limit_in_minutes,\n",
    "            contract_max_n_estimators=contract_max_n_estimators,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs,\n",
    "            parallel_backend=parallel_backend,\n",
    "        )\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        return super()._fit(X, y)\n",
    "\n",
    "    def _predict(self, X) -> np.ndarray:\n",
    "        return super()._predict(X)\n",
    "\n",
    "    def _predict_proba(self, X) -> np.ndarray:\n",
    "        return super()._predict_proba(X)\n",
    "\n",
    "    def _fit_predict(self, X, y) -> np.ndarray:\n",
    "        return super()._fit_predict(X, y)\n",
    "\n",
    "    def _fit_predict_proba(self, X, y) -> np.ndarray:\n",
    "        return super()._fit_predict_proba(X, y)\n",
    "\n",
    "    @classmethod\n",
    "    def _get_test_params(cls, parameter_set=\"default\"):\n",
    "        \"\"\"Return testing parameter settings for the estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameter_set : str, default=\"default\"\n",
    "            Name of the set of test parameters to return, for use in tests. If no\n",
    "            special parameters are defined for a value, will return `\"default\"` set.\n",
    "            TimeSeriesForestClassifier provides the following special sets:\n",
    "                \"results_comparison\" - used in some classifiers to compare against\n",
    "                    previously generated results where the default set of parameters\n",
    "                    cannot produce suitable probability estimates\n",
    "                \"contracting\" - used in classifiers that set the\n",
    "                    \"capability:contractable\" tag to True to test contacting\n",
    "                    functionality\n",
    "                \"train_estimate\" - used in some classifiers that set the\n",
    "                    \"capability:train_estimate\" tag to True to allow for more efficient\n",
    "                    testing when relevant parameters are available\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        params : dict or list of dict, default={}\n",
    "            Parameters to create testing instances of the class.\n",
    "            Each dict are parameters to construct an \"interesting\" test instance, i.e.,\n",
    "            `MyClass(**params)` or `MyClass(**params[i])` creates a valid test instance.\n",
    "        \"\"\"\n",
    "        if parameter_set == \"results_comparison\":\n",
    "            return {\"n_estimators\": 10}\n",
    "        elif parameter_set == \"contracting\":\n",
    "            return {\n",
    "                \"time_limit_in_minutes\": 5,\n",
    "                \"contract_max_n_estimators\": 2,\n",
    "                \"n_intervals\": 2,\n",
    "            }\n",
    "        else:\n",
    "            return {\"n_estimators\": 2, \"n_intervals\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "785cca51-6597-46df-b258-368030718e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.backends.cudnn.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8296dc-0e4a-47d0-9872-2ebfdecf31bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature array shape: (27692, 66, 13)\n",
      "Labels array shape: (27692,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.fftpack import dct  # Import DCT from scipy\n",
    "import librosa  # Ensure librosa is imported for loading audio files\n",
    "\n",
    "# Custom Dataset Class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "def pre_emphasis(signal, alpha=0.97):\n",
    "    \"\"\"Apply pre-emphasis filter.\"\"\"\n",
    "    return np.append(signal[0], signal[1:] - alpha * signal[:-1])\n",
    "\n",
    "def framing(signal, frame_size, hop_size):\n",
    "    \"\"\"Split signal into overlapping frames.\"\"\"\n",
    "    num_frames = int(np.ceil(float(np.abs(len(signal) - frame_size)) / hop_size)) + 1\n",
    "    pad_signal_length = num_frames * hop_size + frame_size\n",
    "    z = np.zeros(pad_signal_length)\n",
    "    z[:len(signal)] = signal\n",
    "    \n",
    "    frames = np.lib.stride_tricks.as_strided(z,\n",
    "        shape=(num_frames, frame_size),\n",
    "        strides=(z.strides[0] * hop_size, z.strides[0])).copy()\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def hamming_window(frame):\n",
    "    \"\"\"Apply Hamming window to a frame.\"\"\"\n",
    "    return np.hamming(len(frame)) * frame\n",
    "\n",
    "def mel_filter_bank(num_filters, fft_size, sample_rate, low_freq=0, high_freq=None):\n",
    "    \"\"\"Create a Mel filter bank.\"\"\"\n",
    "    if high_freq is None:\n",
    "        high_freq = sample_rate / 2\n",
    "    \n",
    "    # Convert frequency to Mel scale\n",
    "    low_mel = 2595 * np.log10(1 + low_freq / 700)\n",
    "    high_mel = 2595 * np.log10(1 + high_freq / 700)\n",
    "    \n",
    "    mel_points = np.linspace(low_mel, high_mel, num_filters + 2)\n",
    "    hz_points = 700 * (10**(mel_points / 2595) - 1)\n",
    "    \n",
    "    bin_points = np.floor((fft_size + 1) * hz_points / sample_rate).astype(int)\n",
    "    \n",
    "    filters = np.zeros((num_filters, int(np.floor(fft_size / 2 + 1))))\n",
    "    \n",
    "    for n in range(1, num_filters + 1):\n",
    "        filters[n - 1, bin_points[n - 1]:bin_points[n]] = \\\n",
    "            (np.arange(bin_points[n - 1], bin_points[n]) - bin_points[n - 1]) / (bin_points[n] - bin_points[n - 1])\n",
    "        filters[n - 1, bin_points[n]:bin_points[n + 1]] = \\\n",
    "            (bin_points[n + 1] - np.arange(bin_points[n], bin_points[n + 1])) / (bin_points[n + 1] - bin_points[n])\n",
    "    \n",
    "    return filters\n",
    "\n",
    "def compute_mfcc(signal, sample_rate=16000, n_mfcc=13, n_fft=400, hop_length=240):\n",
    "    \"\"\"Compute MFCC from scratch.\"\"\"\n",
    "    # Step 1: Pre-emphasis\n",
    "    emphasized_signal = pre_emphasis(signal)\n",
    "\n",
    "    # Step 2: Framing\n",
    "    frames = framing(emphasized_signal, n_fft, hop_length)\n",
    "\n",
    "    # Step 3: Apply Hamming window\n",
    "    windowed_frames = np.array([hamming_window(frame) for frame in frames])\n",
    "\n",
    "    # Step 4: FFT and Power Spectrum\n",
    "    mag_frames = np.abs(np.fft.rfft(windowed_frames, n=n_fft)) ** 2\n",
    "\n",
    "    # Step 5: Mel Filter Bank\n",
    "    mel_filters = mel_filter_bank(n_mfcc, n_fft, sample_rate)\n",
    "    \n",
    "    # Step 6: Apply Mel filter bank to power spectrum\n",
    "    mel_energies = np.dot(mag_frames, mel_filters.T)\n",
    "\n",
    "    # Step 7: Logarithm of Mel energies\n",
    "    log_mel_energies = np.log(mel_energies + np.finfo(float).eps)\n",
    "\n",
    "    # Step 8: Discrete Cosine Transform (DCT)\n",
    "    mfccs = dct(log_mel_energies, type=2, axis=1, norm='ortho')[:, :n_mfcc]\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "def load_data_with_mfcc(directory, n_mfcc=13, n_fft=400, hop_size=240, target_length=16000):\n",
    "    \"\"\"Load data from a directory and extract MFCC features.\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    labels = sorted(os.listdir(directory))\n",
    "    label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    for label in labels:\n",
    "        class_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.endswith('.wav'):\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    signal, rate = librosa.load(file_path, sr=None)  # Load audio to get its length\n",
    "                    \n",
    "                    # Check if the audio signal length is less than the target length (16000 samples)\n",
    "                    if len(signal) < target_length:\n",
    "                        # Pad the signal to 16000 samples if it's too short\n",
    "                        padding = target_length - len(signal)\n",
    "                        signal = np.pad(signal, (0, padding), 'constant')\n",
    "\n",
    "                    # Check if the audio length is greater than the target length (16000 samples)\n",
    "                    if len(signal) > target_length:\n",
    "                        # Truncate the signal to 16000 samples if it's too long\n",
    "                        signal = signal[:target_length]\n",
    "\n",
    "                    audio_length = len(signal)  # Length in samples\n",
    "                    mfcc = compute_mfcc(signal, sample_rate=rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_size)\n",
    "                    num_frames = mfcc.shape[1]\n",
    "\n",
    "                    # Check if the first window is less than 25 ms (400 samples)\n",
    "                    if num_frames > 0 and (num_frames * hop_size < 400):  \n",
    "                        print(f\"Stopping processing for {file_name}: first window is less than 30 ms.\")\n",
    "                        break\n",
    "                    \n",
    "                    X.append(mfcc)\n",
    "                    y.append(label_map[label])\n",
    "                    \n",
    "                    # Display number of frames and audio length for each sample\n",
    "                    #print(f\"File: {file_name}, Label: {label}, Audio Length: {audio_length} samples, Number of frames: {num_frames}\")\n",
    "\n",
    "                    # Print total number of windows for each file\n",
    "                    #print(f\"Total number of windows for {file_name}: {num_frames}\")\n",
    "\n",
    "                    # Print shape of the feature vector (MFCC matrix)\n",
    "                    #print(f\"MFCC feature vector shape for {file_name}: {mfcc.shape}\")\n",
    "\n",
    "            else:\n",
    "                continue  \n",
    "            break  \n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(\"Feature array shape:\", X.shape)  \n",
    "    print(\"Labels array shape:\", y.shape)\n",
    "\n",
    "    return X, y, labels\n",
    "\n",
    "\n",
    "# Section 3: Data Loading and Preprocessing\n",
    "directory = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/12KWS\"\n",
    "X, y, labels = load_data_with_mfcc(directory)\n",
    "\n",
    "# Reshape X for CNN input (add channel dimension if needed)\n",
    "if X.size > 0:\n",
    "   X = X[:, :, :]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca84e72-d5ca-43b2-a17c-bd519c7ebe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X for CNN input (swap dimensions 1 and 2)\n",
    "if X.size > 0:\n",
    "   X = X.transpose(0, 2, 1)  # Change shape from (23682, 50, 13) to (23682, 13, 50)\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# You can add your model training and evaluation code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ae1395-55d6-4729-8226-33b8b9b64516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27692, 13, 66)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e6a176f-d37a-42ea-92ba-eb827786cead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22153, 13, 66)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0f55a3-b114-48a0-9e1b-0024076d3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TimeSeriesForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49cba261-582a-4b4f-b4bd-004c08d24b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TimeSeriesForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TimeSeriesForestClassifier</label><div class=\"sk-toggleable__content\"><pre>TimeSeriesForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TimeSeriesForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the classifier on the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd12230-31f0-48c5-a927-8df55447d54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.7613287597039177\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "# Predict on the test set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test data:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "382dfc69-1415-4ec6-8fea-e11a20167413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (macro): 0.7711869552399885\n",
      "Recall (macro): 0.7640264540921158\n",
      "F1 Score (macro): 0.7582961044251314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision, recall, and f1 score for each class (macro average)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Precision (macro): {precision}\")\n",
    "print(f\"Recall (macro): {recall}\")\n",
    "print(f\"F1 Score (macro): {f1}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29de87-10ed-4bc8-b7d7-33bb02feaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d3aa6-9887-495b-8b8f-dd893995e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: Define y_test and y_pred\n",
    "# Replace these with your actual labels\n",
    "# y_test = [0, 1, 2, 0, 1, 2]  # True labels\n",
    "# y_pred = [0, 2, 1, 0, 0, 1]  # Predicted labels\n",
    "\n",
    "# Get the unique classes and binarize labels\n",
    "classes = np.unique(y_test)\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "y_pred_bin = label_binarize(y_pred, classes=classes)\n",
    "\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(n_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_pred_bin[:, i])\n",
    "    plt.plot(recall, precision, lw=2, label=f'Class {classes[i]}')\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d907b-03cf-4436-a25c-f0fe74871df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(n_classes):\n",
    "    # Get ROC curve metrics for each class\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])\n",
    "    roc_auc = auc(fpr, tpr)  # Compute AUC for each class\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'Class {classes[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the diagonal line for random classifier (AUC = 0.5)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "# Plot details\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
