{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set a global random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A base class for interval extracting forest estimators.\"\"\"\n",
    "\n",
    "__maintainer__ = []\n",
    "__all__ = [\"BaseIntervalForest\"]\n",
    "\n",
    "import inspect\n",
    "import time\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import BaseEstimator, is_classifier, is_regressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.tree import BaseDecisionTree, DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from aeon.base._base import _clone_estimator\n",
    "from aeon.classification.sklearn import ContinuousIntervalTree\n",
    "from aeon.transformations.base import BaseTransformer\n",
    "from aeon.transformations.collection.interval_based import (\n",
    "    RandomIntervals,\n",
    "    SupervisedIntervals,\n",
    ")\n",
    "from aeon.utils.numba.stats import row_mean, row_slope, row_std\n",
    "from aeon.utils.validation import check_n_jobs\n",
    "\n",
    "\n",
    "class BaseIntervalForest(ABC):\n",
    "    \"\"\"A base class for interval extracting forest estimators.\n",
    "\n",
    "    Allows the implementation of classifiers and regressors along the lines of [1][2][3]\n",
    "    which extract intervals and create an ensemble from the subsequent features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : BaseEstimator or None, default=None\n",
    "        scikit-learn BaseEstimator used to build the interval ensemble. If None, use a\n",
    "        simple decision tree.\n",
    "    n_estimators : int, default=200\n",
    "        Number of estimators to build for the ensemble.\n",
    "    interval_selection_method : \"random\", \"supervised\" or \"random-supervised\",\n",
    "            default=\"random\"\n",
    "        The interval selection transformer to use.\n",
    "            - \"random\" uses a RandomIntervalTransformer.\n",
    "            - \"supervised\" uses a SupervisedIntervalTransformer.\n",
    "            - \"random-supervised\" uses a SupervisedIntervalTransformer with\n",
    "                randomised elements.\n",
    "\n",
    "        Supervised methods can only be used for classification tasks, and require\n",
    "        function inputs for interval_features rather than transformers.\n",
    "    n_intervals : int, str, list or tuple, default=\"sqrt\"\n",
    "        Number of intervals to extract per tree for each series_transformers series.\n",
    "\n",
    "        An int input will extract that number of intervals from the series, while a str\n",
    "        input will return a function of the series length (may differ per\n",
    "        series_transformers output) to extract that number of intervals.\n",
    "        Valid str inputs are:\n",
    "            - \"sqrt\": square root of the series length.\n",
    "            - \"sqrt-div\": sqrt of series length divided by the number\n",
    "                of series_transformers.\n",
    "\n",
    "        A list or tuple of ints and/or strs will extract the number of intervals using\n",
    "        the above rules and sum the results for the final n_intervals. i.e. [4, \"sqrt\"]\n",
    "        will extract sqrt(n_timepoints) + 4 intervals.\n",
    "\n",
    "        Different number of intervals for each series_transformers series can be\n",
    "        specified using a nested list or tuple. Any list or tuple input containing\n",
    "        another list or tuple must be the same length as the number of\n",
    "        series_transformers.\n",
    "\n",
    "        While random interval extraction will extract the n_intervals intervals total\n",
    "        (removing duplicates), supervised intervals will run the supervised extraction\n",
    "        process n_intervals times, returning more intervals than specified.\n",
    "    min_interval_length : int, float, list, or tuple, default=3\n",
    "        Minimum length of intervals to extract from series. float inputs take a\n",
    "        proportion of the series length to use as the minimum interval length.\n",
    "\n",
    "        Different minimum interval lengths for each series_transformers series can be\n",
    "        specified using a list or tuple. Any list or tuple input must be the same length\n",
    "        as the number of series_transformers.\n",
    "    max_interval_length : int, float, list, or tuple, default=np.inf\n",
    "        Maximum length of intervals to extract from series. float inputs take a\n",
    "        proportion of the series length to use as the maximum interval length.\n",
    "\n",
    "        Different maximum interval lengths for each series_transformers series can be\n",
    "        specified using a list or tuple. Any list or tuple input must be the same length\n",
    "        as the number of series_transformers.\n",
    "\n",
    "        Ignored for supervised interval_selection_method inputs.\n",
    "    interval_features : BaseTransformer, callable, list, tuple, or None, default=None\n",
    "        The features to extract from the intervals using transformers or callable\n",
    "        functions. If None, use the mean, standard deviation, and slope of the series.\n",
    "\n",
    "        Both transformers and functions should be able to take a 2D np.ndarray input.\n",
    "        Functions should output a 1d array (the feature for each series), and\n",
    "        transformers should output a 2d array where rows are the features for each\n",
    "        series. A list or tuple of transformers and/or functions will extract all\n",
    "        features and concatenate the output.\n",
    "\n",
    "        Different features for each series_transformers series can be specified using a\n",
    "        nested list or tuple. Any list or tuple input containing another list or tuple\n",
    "        must be the same length as the number of series_transformers.\n",
    "    series_transformers : BaseTransformer, list, tuple, or None, default=None\n",
    "        The transformers to apply to the series before extracting intervals. If None,\n",
    "        use the series as is.\n",
    "\n",
    "        A list or tuple of transformers will extract intervals from\n",
    "        all transformations concatenate the output. Including None in the list or tuple\n",
    "        will use the series as is for interval extraction.\n",
    "    att_subsample_size : int, float, list, tuple or None, default=None\n",
    "        The number of attributes to subsample for each estimator. If None, use all\n",
    "\n",
    "        If int, use that number of attributes for all estimators. If float, use that\n",
    "        proportion of attributes for all estimators.\n",
    "\n",
    "        Different subsample sizes for each series_transformers series can be specified\n",
    "        using a list or tuple. Any list or tuple input must be the same length as the\n",
    "        number of series_transformers.\n",
    "    replace_nan : \"nan\", int, float or None, default=None\n",
    "        The value to replace NaNs and infinite values with before fitting the base\n",
    "        estimator. int or float input will replace with the specified value, while\n",
    "        \"nan\" will replace infinite values with NaNs. If None, do not replace NaNs.\n",
    "    time_limit_in_minutes : int, default=0\n",
    "        Time contract to limit build time in minutes, overriding n_estimators.\n",
    "        Default of 0 means n_estimators are used.\n",
    "    contract_max_n_estimators : int, default=500\n",
    "        Max number of estimators when time_limit_in_minutes is set.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        If `int`, random_state is the seed used by the random number generator;\n",
    "        If `RandomState` instance, random_state is the random number generator;\n",
    "        If `None`, the random number generator is the `RandomState` instance used\n",
    "        by `np.random`.\n",
    "    n_jobs : int, default=1\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``-1`` means using all processors.\n",
    "    parallel_backend : str, ParallelBackendBase instance or None, default=None\n",
    "        Specify the parallelisation backend implementation in joblib, if None a 'prefer'\n",
    "        value of \"threads\" is used by default.\n",
    "        Valid options are \"loky\", \"multiprocessing\", \"threading\" or a custom backend.\n",
    "        See the joblib Parallel documentation for more details.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_cases_ : int\n",
    "        The number of train cases.\n",
    "    n_channels_ : int\n",
    "        The number of channels per case.\n",
    "    n_timepoints_ : int\n",
    "        The length of each series.\n",
    "    total_intervals_ : int\n",
    "        Total number of intervals per tree from all representations.\n",
    "    estimators_ : list of shape (n_estimators) of BaseEstimator\n",
    "        The collections of estimators trained in fit.\n",
    "    intervals_ : list of shape (n_estimators) of BaseTransformer\n",
    "        Stores the interval extraction transformer for all estimators.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] H.Deng, G.Runger, E.Tuv and M.Vladimir, \"A time series forest for\n",
    "       classification and feature extraction\", Information Sciences, 239, 2013\n",
    "    .. [2] Matthew Middlehurst and James Large and Anthony Bagnall. \"The Canonical\n",
    "       Interval Forest (CIF) Classifier for Time Series Classification.\"\n",
    "       IEEE International Conference on Big Data 2020\n",
    "    .. [3] Cabello, Nestor, et al. \"Fast and Accurate Time Series Classification\n",
    "       Through Supervised Interval Search.\" IEEE ICDM 2020\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator=None,\n",
    "        n_estimators=200,\n",
    "        interval_selection_method=\"random\",\n",
    "        n_intervals=\"sqrt\",\n",
    "        min_interval_length=3,\n",
    "        max_interval_length=np.inf,\n",
    "        interval_features=None,\n",
    "        series_transformers=None,\n",
    "        att_subsample_size=None,\n",
    "        replace_nan=None,\n",
    "        time_limit_in_minutes=None,\n",
    "        contract_max_n_estimators=500,\n",
    "        random_state=None,\n",
    "        n_jobs=1,\n",
    "        parallel_backend=None,\n",
    "    ):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.interval_selection_method = interval_selection_method\n",
    "        self.n_intervals = n_intervals\n",
    "        self.min_interval_length = min_interval_length\n",
    "        self.max_interval_length = max_interval_length\n",
    "        self.interval_features = interval_features\n",
    "        self.series_transformers = series_transformers\n",
    "        self.att_subsample_size = att_subsample_size\n",
    "        self.replace_nan = replace_nan\n",
    "        self.time_limit_in_minutes = time_limit_in_minutes\n",
    "        self.contract_max_n_estimators = contract_max_n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.parallel_backend = parallel_backend\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    # if subsampling attributes, an interval_features transformer must contain a\n",
    "    # parameter name from transformer_feature_selection and an attribute name\n",
    "    # (or property) from transformer_feature_names to allow features to be subsampled\n",
    "    transformer_feature_selection = [\"features\"]\n",
    "    transformer_feature_names = [\n",
    "        \"features_arguments_\",\n",
    "        \"_features_arguments\",\n",
    "        \"get_features_arguments\",\n",
    "        \"_get_features_arguments\",\n",
    "    ]\n",
    "    # an interval_features transformer must contain one of these attribute names to\n",
    "    # be able to skip transforming features in predict\n",
    "    transformer_feature_skip = [\"transform_features_\", \"_transform_features\"]\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        if getattr(self, \"__unit_test_flag\", False):\n",
    "            self._transformed_data = self._fit_forest(X, y, save_transformed_data=True)\n",
    "        else:\n",
    "            self._fit_forest(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict(self, X):\n",
    "        if is_regressor(self):\n",
    "            Xt = self._predict_setup(X)\n",
    "\n",
    "            y_preds = Parallel(\n",
    "                n_jobs=self._n_jobs,\n",
    "                backend=self.parallel_backend,\n",
    "                prefer=\"threads\",\n",
    "            )(\n",
    "                delayed(self._predict_for_estimator)(\n",
    "                    Xt,\n",
    "                    self.estimators_[i],\n",
    "                    self.intervals_[i],\n",
    "                    predict_proba=False,\n",
    "                )\n",
    "                for i in range(self._n_estimators)\n",
    "            )\n",
    "\n",
    "            return np.mean(y_preds, axis=0)\n",
    "        else:\n",
    "            return np.array(\n",
    "                [self.classes_[int(np.argmax(prob))] for prob in self._predict_proba(X)]\n",
    "            )\n",
    "\n",
    "    def _predict_proba(self, X):\n",
    "        Xt = self._predict_setup(X)\n",
    "\n",
    "        y_probas = Parallel(\n",
    "            n_jobs=self._n_jobs, backend=self.parallel_backend, prefer=\"threads\"\n",
    "        )(\n",
    "            delayed(self._predict_for_estimator)(\n",
    "                Xt,\n",
    "                self.estimators_[i],\n",
    "                self.intervals_[i],\n",
    "                predict_proba=True,\n",
    "            )\n",
    "            for i in range(self._n_estimators)\n",
    "        )\n",
    "\n",
    "        output = np.sum(y_probas, axis=0) / (\n",
    "            np.ones(self.n_classes_) * self._n_estimators\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def _fit_predict(self, X, y) -> np.ndarray:\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        if is_regressor(self):\n",
    "            Xt = self._fit_forest(X, y, save_transformed_data=True)\n",
    "\n",
    "            p = Parallel(\n",
    "                n_jobs=self._n_jobs, backend=self.parallel_backend, prefer=\"threads\"\n",
    "            )(\n",
    "                delayed(self._train_estimate_for_estimator)(\n",
    "                    Xt,\n",
    "                    y,\n",
    "                    i,\n",
    "                    check_random_state(rng.randint(np.iinfo(np.int32).max)),\n",
    "                )\n",
    "                for i in range(self._n_estimators)\n",
    "            )\n",
    "            y_preds, oobs = zip(*p)\n",
    "\n",
    "            results = np.sum(y_preds, axis=0)\n",
    "            divisors = np.zeros(self.n_cases_)\n",
    "            for oob in oobs:\n",
    "                for inst in oob:\n",
    "                    divisors[inst] += 1\n",
    "\n",
    "            label_average = np.mean(y)\n",
    "            for i in range(self.n_cases_):\n",
    "                results[i] = (\n",
    "                    label_average if divisors[i] == 0 else results[i] / divisors[i]\n",
    "                )\n",
    "        else:\n",
    "            return np.array(\n",
    "                [\n",
    "                    self.classes_[int(rng.choice(np.flatnonzero(prob == prob.max())))]\n",
    "                    for prob in self._fit_predict_proba(X, y)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _fit_predict_proba(self, X, y) -> np.ndarray:\n",
    "        if is_regressor(self):\n",
    "            raise ValueError(\n",
    "                \"Train probability estimates are only available for classification\"\n",
    "            )\n",
    "\n",
    "        Xt = self._fit_forest(X, y, save_transformed_data=True)\n",
    "\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        p = Parallel(\n",
    "            n_jobs=self._n_jobs, backend=self.parallel_backend, prefer=\"threads\"\n",
    "        )(\n",
    "            delayed(self._train_estimate_for_estimator)(\n",
    "                Xt,\n",
    "                y,\n",
    "                i,\n",
    "                check_random_state(rng.randint(np.iinfo(np.int32).max)),\n",
    "                probas=True,\n",
    "            )\n",
    "            for i in range(self._n_estimators)\n",
    "        )\n",
    "        y_probas, oobs = zip(*p)\n",
    "\n",
    "        results = np.sum(y_probas, axis=0)\n",
    "        divisors = np.zeros(self.n_cases_)\n",
    "        for oob in oobs:\n",
    "            for inst in oob:\n",
    "                divisors[inst] += 1\n",
    "\n",
    "        for i in range(self.n_cases_):\n",
    "            results[i] = (\n",
    "                np.ones(self.n_classes_) * (1 / self.n_classes_)\n",
    "                if divisors[i] == 0\n",
    "                else results[i] / (np.ones(self.n_classes_) * divisors[i])\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _fit_forest(self, X, y, save_transformed_data=False):\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        self.n_cases_, self.n_channels_, self.n_timepoints_ = X.shape\n",
    "\n",
    "        self._base_estimator = self.base_estimator\n",
    "        if self.base_estimator is None:\n",
    "            if is_classifier(self):\n",
    "                self._base_estimator = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "            elif is_regressor(self):\n",
    "                self._base_estimator = DecisionTreeRegressor(criterion=\"absolute_error\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"{self} must be a scikit-learn compatible classifier or \"\n",
    "                    \"regressor.\"\n",
    "                )\n",
    "        # base_estimator must be an sklearn estimator\n",
    "        elif not isinstance(self.base_estimator, BaseEstimator):\n",
    "            raise ValueError(\n",
    "                \"base_estimator must be a scikit-learn BaseEstimator or None. \"\n",
    "                f\"Found: {self.base_estimator}\"\n",
    "            )\n",
    "\n",
    "        # use the base series if series_transformers is None\n",
    "        if self.series_transformers is None or self.series_transformers == []:\n",
    "            Xt = [X]\n",
    "            self._series_transformers = [None]\n",
    "        # clone series_transformers if it is a transformer and transform the input data\n",
    "        elif _is_transformer(self.series_transformers):\n",
    "            t = _clone_estimator(self.series_transformers, random_state=rng)\n",
    "            Xt = [t.fit_transform(X, y)]\n",
    "            self._series_transformers = [t]\n",
    "        # clone each series_transformers transformer and include the base series if None\n",
    "        # is in the list\n",
    "        elif isinstance(self.series_transformers, (list, tuple)):\n",
    "            Xt = []\n",
    "            self._series_transformers = []\n",
    "\n",
    "            for transformer in self.series_transformers:\n",
    "                if transformer is None:\n",
    "                    Xt.append(X)\n",
    "                    self._series_transformers.append(None)\n",
    "                elif _is_transformer(transformer):\n",
    "                    t = _clone_estimator(transformer, random_state=rng)\n",
    "                    Xt.append(t.fit_transform(X, y))\n",
    "                    self._series_transformers.append(t)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid series_transformers list input. Found {transformer}\"\n",
    "                    )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid series_transformers input. Found {self.series_transformers}\"\n",
    "            )\n",
    "\n",
    "        # if only a single n_intervals value is passed it must be an int or str\n",
    "        if isinstance(self.n_intervals, (int, str)):\n",
    "            n_intervals = [[self.n_intervals]] * len(Xt)\n",
    "        elif isinstance(self.n_intervals, (list, tuple)):\n",
    "            # if input is a list and only contains ints or strs, use the list for all\n",
    "            # series in Xt\n",
    "            if all(isinstance(item, (int, str)) for item in self.n_intervals):\n",
    "                n_intervals = [self.n_intervals] * len(Xt)\n",
    "            # other lists must be the same length as Xt\n",
    "            elif len(self.n_intervals) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"n_intervals as a list or tuple containing other lists or tuples \"\n",
    "                    \"must be the same length as series_transformers.\"\n",
    "                )\n",
    "            # list items can be a list of items or a single item for each\n",
    "            # series_transformer, but each individual item must be an int or str\n",
    "            else:\n",
    "                n_intervals = []\n",
    "                for items in self.n_intervals:\n",
    "                    if isinstance(items, (list, tuple)):\n",
    "                        if not all(isinstance(item, (int, str)) for item in items):\n",
    "                            raise ValueError(\n",
    "                                \"Individual items in a n_intervals list or tuple must \"\n",
    "                                f\"be an int or str. Input {items} does not contain \"\n",
    "                                \"only ints or strs\"\n",
    "                            )\n",
    "                        n_intervals.append(items)\n",
    "                    elif isinstance(items, (int, str)):\n",
    "                        n_intervals.append([items])\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Individual items in a n_intervals list or tuple must be \"\n",
    "                            f\"an int or str. Found: {items}\"\n",
    "                        )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid n_intervals input. Found {self.n_intervals}\")\n",
    "\n",
    "        # add together the number of intervals for each series_transformer\n",
    "        # str input must be one of a set valid options\n",
    "        self._n_intervals = [0] * len(Xt)\n",
    "        for i, series in enumerate(Xt):\n",
    "            for method in n_intervals[i]:\n",
    "                if isinstance(method, int):\n",
    "                    self._n_intervals[i] += method\n",
    "                elif isinstance(method, str):\n",
    "                    # sqrt of series length\n",
    "                    if method.lower() == \"sqrt\":\n",
    "                        self._n_intervals[i] += int(\n",
    "                            np.sqrt(series.shape[2]) * np.sqrt(series.shape[1])\n",
    "                        )\n",
    "                    # sqrt of series length divided by the number of series_transformers\n",
    "                    elif method.lower() == \"sqrt-div\":\n",
    "                        self._n_intervals[i] += int(\n",
    "                            (np.sqrt(series.shape[2]) * np.sqrt(series.shape[1]))\n",
    "                            / len(Xt)\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Invalid str input for n_intervals. Must be \"\n",
    "                            f'(\"sqrt\",\"sqrt-div\"). Found {method}'\n",
    "                        )\n",
    "\n",
    "        # each series_transformer must have at least 1 interval extracted\n",
    "        for i, n in enumerate(self._n_intervals):\n",
    "            if n <= 0:\n",
    "                self._n_intervals[i] = 1\n",
    "\n",
    "        self.total_intervals_ = sum(self._n_intervals)\n",
    "\n",
    "        # minimum interval length\n",
    "        if isinstance(self.min_interval_length, int):\n",
    "            self._min_interval_length = [self.min_interval_length] * len(Xt)\n",
    "        # min_interval_length must be less than one if it is a float (proportion of\n",
    "        # of the series length)\n",
    "        elif (\n",
    "            isinstance(self.min_interval_length, float)\n",
    "            and self.min_interval_length <= 1\n",
    "        ):\n",
    "            self._min_interval_length = [\n",
    "                int(self.min_interval_length * t.shape[2]) for t in Xt\n",
    "            ]\n",
    "        # if the input is a list, it must be the same length as the number of\n",
    "        # series_transformers\n",
    "        # list values must be ints or floats. The same checks as above are performed\n",
    "        elif isinstance(self.min_interval_length, (list, tuple)):\n",
    "            if len(self.min_interval_length) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"min_interval_length as a list or tuple must be the same length \"\n",
    "                    \"as series_transformers.\"\n",
    "                )\n",
    "\n",
    "            self._min_interval_length = []\n",
    "            for i, length in enumerate(self.min_interval_length):\n",
    "                if isinstance(length, float) and length <= 1:\n",
    "                    self._min_interval_length.append(int(length * Xt[i].shape[2]))\n",
    "                elif isinstance(length, int):\n",
    "                    self._min_interval_length.append(length)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"min_interval_length list items must be int or floats. \"\n",
    "                        f\"Found {length}\"\n",
    "                    )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid min_interval_length input. Found {self.min_interval_length}\"\n",
    "            )\n",
    "\n",
    "        # min_interval_length cannot be less than 3 or greater than the series length\n",
    "        for i, n in enumerate(self._min_interval_length):\n",
    "            if n > Xt[i].shape[2]:\n",
    "                self._min_interval_length[i] = Xt[i].shape[2]\n",
    "            elif n < 3:\n",
    "                self._min_interval_length[i] = 3\n",
    "\n",
    "        # maximum interval length\n",
    "        if (\n",
    "            isinstance(self.max_interval_length, int)\n",
    "            or self.max_interval_length == np.inf\n",
    "        ):\n",
    "            self._max_interval_length = [self.max_interval_length] * len(Xt)\n",
    "        # max_interval_length must be at less than one if it is a float  (proportion of\n",
    "        # of the series length)\n",
    "        elif (\n",
    "            isinstance(self.max_interval_length, float)\n",
    "            and self.max_interval_length <= 1\n",
    "        ):\n",
    "            self._max_interval_length = [\n",
    "                int(self.max_interval_length * t.shape[2]) for t in Xt\n",
    "            ]\n",
    "        # if the input is a list, it must be the same length as the number of\n",
    "        # series_transformers\n",
    "        # list values must be ints or floats. The same checks as above are performed\n",
    "        elif isinstance(self.max_interval_length, (list, tuple)):\n",
    "            if len(self.max_interval_length) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"max_interval_length as a list or tuple must be the same length \"\n",
    "                    \"as series_transformers.\"\n",
    "                )\n",
    "\n",
    "            self._max_interval_length = []\n",
    "            for i, length in enumerate(self.max_interval_length):\n",
    "                if isinstance(length, float) and length <= 1:\n",
    "                    self._max_interval_length.append(int(length * Xt[i].shape[2]))\n",
    "                elif isinstance(length, int):\n",
    "                    self._max_interval_length.append(length)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"max_interval_length list items must be int or floats. \"\n",
    "                        f\"Found {length}\"\n",
    "                    )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid max_interval_length input. Found {self.max_interval_length}\"\n",
    "            )\n",
    "\n",
    "        # max_interval_length cannot be less than min_interval_length or greater than\n",
    "        # the series length\n",
    "        for i, n in enumerate(self._max_interval_length):\n",
    "            if n < self._min_interval_length[i]:\n",
    "                self._max_interval_length[i] = self._min_interval_length[i]\n",
    "            elif n > Xt[i].shape[2]:\n",
    "                self._max_interval_length[i] = Xt[i].shape[2]\n",
    "\n",
    "        # we store whether each series_transformer contains a transformer and/or\n",
    "        # function in its interval_features\n",
    "        self._interval_transformer = [False] * len(Xt)\n",
    "        self._interval_function = [False] * len(Xt)\n",
    "        # single transformer or function for all series_transformers\n",
    "        if isinstance(self.interval_features, BaseTransformer):\n",
    "            self._interval_transformer = [True] * len(Xt)\n",
    "            transformer = _clone_estimator(self.interval_features, random_state=rng)\n",
    "            self._interval_features = [[transformer]] * len(Xt)\n",
    "        elif callable(self.interval_features):\n",
    "            self._interval_function = [True] * len(Xt)\n",
    "            self._interval_features = [[self.interval_features]] * len(Xt)\n",
    "        elif isinstance(self.interval_features, (list, tuple)):\n",
    "            # if input is a list and only contains transformers or functions, use the\n",
    "            # list for all series in Xt\n",
    "            if all(\n",
    "                isinstance(item, BaseTransformer) or callable(item)\n",
    "                for item in self.interval_features\n",
    "            ):\n",
    "                for feature in self.interval_features:\n",
    "                    if isinstance(feature, BaseTransformer):\n",
    "                        self._interval_transformer[0] = True\n",
    "                    elif callable(feature):\n",
    "                        self._interval_function[0] = True\n",
    "                self._interval_features = [self.interval_features] * len(Xt)\n",
    "            # other lists must be the same length as Xt\n",
    "            elif len(self.interval_features) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"interval_features as a list or tuple containing other lists or \"\n",
    "                    \"tuples must be the same length as series_transformers.\"\n",
    "                )\n",
    "            # list items can be a list of items or a single item for each\n",
    "            # series_transformer, but each individual item must be a transformer\n",
    "            # or function\n",
    "            else:\n",
    "                self._interval_features = []\n",
    "                for i, feature in enumerate(self.interval_features):\n",
    "                    if isinstance(feature, (list, tuple)):\n",
    "                        for method in feature:\n",
    "                            if isinstance(method, BaseTransformer):\n",
    "                                self._interval_transformer[i] = True\n",
    "                                feature = _clone_estimator(feature, random_state=rng)\n",
    "                            elif callable(method):\n",
    "                                self._interval_function[i] = True\n",
    "                            else:\n",
    "                                raise ValueError(\n",
    "                                    \"Individual items in a interval_features list or \"\n",
    "                                    \"tuple must be a transformer or function. Input \"\n",
    "                                    f\"{feature} does not contain only transformers and \"\n",
    "                                    f\"functions.\"\n",
    "                                )\n",
    "                        self._interval_features.append(feature)\n",
    "                    elif isinstance(feature, BaseTransformer):\n",
    "                        self._interval_transformer[i] = True\n",
    "                        feature = _clone_estimator(feature, random_state=rng)\n",
    "                        self._interval_features.append([feature])\n",
    "                    elif callable(feature):\n",
    "                        self._interval_function[i] = True\n",
    "                        self._interval_features.append([feature])\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Individual items in a interval_features list or tuple \"\n",
    "                            f\"must be a transformer or function. Found {feature}\"\n",
    "                        )\n",
    "        # use basic summary stats by default if None\n",
    "        elif self.interval_features is None:\n",
    "            self._interval_function = [True] * len(Xt)\n",
    "            self._interval_features = [[row_mean, row_std, row_slope]] * len(Xt)\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid interval_features input. Found {self.interval_features}\"\n",
    "            )\n",
    "\n",
    "        # att_subsample_size must be at least one if it is an int\n",
    "        if isinstance(self.att_subsample_size, int):\n",
    "            if self.att_subsample_size < 1:\n",
    "                raise ValueError(\n",
    "                    \"att_subsample_size must be at least one if it is an int.\"\n",
    "                )\n",
    "\n",
    "            self._att_subsample_size = [self.att_subsample_size] * len(Xt)\n",
    "        # att_subsample_size must be at less than one if it is a float (proportion of\n",
    "        # total attributed to subsample)\n",
    "        elif isinstance(self.att_subsample_size, float):\n",
    "            if self.att_subsample_size > 1 or self.att_subsample_size <= 0:\n",
    "                raise ValueError(\n",
    "                    \"att_subsample_size must be between 0 and 1 if it is a float.\"\n",
    "                )\n",
    "\n",
    "            self._att_subsample_size = [self.att_subsample_size] * len(Xt)\n",
    "        # default is no attribute subsampling with None\n",
    "        elif self.att_subsample_size is None:\n",
    "            self._att_subsample_size = [self.att_subsample_size] * len(Xt)\n",
    "        # if the input is a list, it must be the same length as the number of\n",
    "        # series_transformers\n",
    "        # list values must be ints, floats or None. The same checks as above are\n",
    "        # performed\n",
    "        elif isinstance(self.att_subsample_size, (list, tuple)):\n",
    "            if len(self.att_subsample_size) != len(Xt):\n",
    "                raise ValueError(\n",
    "                    \"att_subsample_size as a list or tuple must be the same length as \"\n",
    "                    \"series_transformers.\"\n",
    "                )\n",
    "\n",
    "            self._att_subsample_size = []\n",
    "            for ssize in self.att_subsample_size:\n",
    "                if isinstance(ssize, int):\n",
    "                    if ssize < 1:\n",
    "                        raise ValueError(\n",
    "                            \"att_subsample_size in list must be at least one if it is \"\n",
    "                            \"an int.\"\n",
    "                        )\n",
    "\n",
    "                    self._att_subsample_size.append(ssize)\n",
    "                elif isinstance(ssize, float):\n",
    "                    if ssize > 1:\n",
    "                        raise ValueError(\n",
    "                            \"att_subsample_size in list must be between 0 and 1 if it \"\n",
    "                            \"is a \"\n",
    "                            \"float.\"\n",
    "                        )\n",
    "\n",
    "                    self._att_subsample_size.append(ssize)\n",
    "                elif ssize is None:\n",
    "                    self._att_subsample_size.append(ssize)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Invalid interval_features input in list. Found \"\n",
    "                        f\"{self.att_subsample_size}\"\n",
    "                    )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid interval_features input. Found {self.att_subsample_size}\"\n",
    "            )\n",
    "\n",
    "        # if we are subsampling attributes for a series_transformer and it uses a\n",
    "        # BaseTransformer, we must ensure it has the required parameters and\n",
    "        # attributes to do so\n",
    "        self._transformer_feature_selection = [[]] * len(Xt)\n",
    "        self._transformer_feature_names = [[]] * len(Xt)\n",
    "        for r, att_subsample in enumerate(self._att_subsample_size):\n",
    "            if att_subsample is not None:\n",
    "                for transformer in self._interval_features[r]:\n",
    "                    if isinstance(transformer, BaseTransformer):\n",
    "                        params = inspect.signature(transformer.__init__).parameters\n",
    "\n",
    "                        # the transformer must have a parameter with one of the\n",
    "                        # names listed in transformer_feature_selection as a way to\n",
    "                        # select which features the transformer should transform\n",
    "                        has_params = False\n",
    "                        for n in self.transformer_feature_selection:\n",
    "                            if params.get(n, None) is not None:\n",
    "                                has_params = True\n",
    "                                self._transformer_feature_selection[r].append(n)\n",
    "                                break\n",
    "\n",
    "                        if not has_params:\n",
    "                            raise ValueError(\n",
    "                                \"All transformers in interval_features must have a \"\n",
    "                                \"parameter named in transformer_feature_selection to \"\n",
    "                                \"be used in attribute subsampling.\"\n",
    "                            )\n",
    "\n",
    "                        # the transformer must have an attribute with one of the\n",
    "                        # names listed in transformer_feature_names as a list or tuple\n",
    "                        # of valid options for the previous parameter\n",
    "                        has_feature_names = False\n",
    "                        for n in self.transformer_feature_names:\n",
    "                            if hasattr(transformer, n) and isinstance(\n",
    "                                getattr(transformer, n), (list, tuple)\n",
    "                            ):\n",
    "                                has_feature_names = True\n",
    "                                self._transformer_feature_names[r].append(n)\n",
    "                                break\n",
    "\n",
    "                        if not has_feature_names:\n",
    "                            raise ValueError(\n",
    "                                \"All transformers in interval_features must have an \"\n",
    "                                \"attribute or property named in \"\n",
    "                                \"transformer_feature_names to be used in attribute \"\n",
    "                                \"subsampling.\"\n",
    "                            )\n",
    "\n",
    "        # verify the interval_selection_method is a valid string\n",
    "        if isinstance(self.interval_selection_method, str):\n",
    "            # SupervisedIntervals cannot currently handle transformers or regression\n",
    "            if (\n",
    "                self.interval_selection_method.lower() == \"supervised\"\n",
    "                or self.interval_selection_method.lower() == \"random-supervised\"\n",
    "            ):\n",
    "                if any(self._interval_transformer):\n",
    "                    raise ValueError(\n",
    "                        \"Supervised interval_selection_method must only have function \"\n",
    "                        \"inputs for interval_features.\"\n",
    "                    )\n",
    "\n",
    "                if is_regressor(self):\n",
    "                    raise ValueError(\n",
    "                        \"Supervised interval_selection_method cannot be used for \"\n",
    "                        \"regression.\"\n",
    "                    )\n",
    "            # RandomIntervals\n",
    "            elif not self.interval_selection_method.lower() == \"random\":\n",
    "                raise ValueError(\n",
    "                    'Unknown interval_selection_method, must be one of (\"random\",'\n",
    "                    '\"supervised\",\"random-supervised\"). '\n",
    "                    f\"Found: {self.interval_selection_method}\"\n",
    "                )\n",
    "        # other inputs are invalid\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Unknown interval_selection_method, must be one of (\"random\",'\n",
    "                '\"supervised\",\"random-supervised\"). '\n",
    "                f\"Found: {self.interval_selection_method}\"\n",
    "            )\n",
    "\n",
    "        # verify replace_nan is a valid string, number or None\n",
    "        if (\n",
    "            (not isinstance(self.replace_nan, str) or self.replace_nan.lower() != \"nan\")\n",
    "            and not isinstance(self.replace_nan, (int, float))\n",
    "            and self.replace_nan is not None\n",
    "        ):\n",
    "            raise ValueError(f\"Invalid replace_nan input. Found {self.replace_nan}\")\n",
    "\n",
    "        self._n_jobs = check_n_jobs(self.n_jobs)\n",
    "\n",
    "        if self.time_limit_in_minutes is not None and self.time_limit_in_minutes > 0:\n",
    "            time_limit = self.time_limit_in_minutes * 60\n",
    "            start_time = time.time()\n",
    "            train_time = 0\n",
    "\n",
    "            self._n_estimators = 0\n",
    "            self.estimators_ = []\n",
    "            self.intervals_ = []\n",
    "            transformed_intervals = []\n",
    "\n",
    "            while (\n",
    "                train_time < time_limit\n",
    "                and self._n_estimators < self.contract_max_n_estimators\n",
    "            ):\n",
    "                fit = Parallel(\n",
    "                    n_jobs=self._n_jobs,\n",
    "                    backend=self.parallel_backend,\n",
    "                    prefer=\"threads\",\n",
    "                )(\n",
    "                    delayed(self._fit_estimator)(\n",
    "                        Xt,\n",
    "                        y,\n",
    "                        rng.randint(np.iinfo(np.int32).max),\n",
    "                        save_transformed_data=save_transformed_data,\n",
    "                    )\n",
    "                    for _ in range(self._n_jobs)\n",
    "                )\n",
    "\n",
    "                (\n",
    "                    estimators,\n",
    "                    intervals,\n",
    "                    td,\n",
    "                ) = zip(*fit)\n",
    "\n",
    "                self.estimators_ += estimators\n",
    "                self.intervals_ += intervals\n",
    "                transformed_intervals += td\n",
    "\n",
    "                self._n_estimators += self._n_jobs\n",
    "                train_time = time.time() - start_time\n",
    "        else:\n",
    "            self._n_estimators = self.n_estimators\n",
    "\n",
    "            fit = Parallel(\n",
    "                n_jobs=self._n_jobs,\n",
    "                backend=self.parallel_backend,\n",
    "                prefer=\"threads\",\n",
    "            )(\n",
    "                delayed(self._fit_estimator)(\n",
    "                    Xt,\n",
    "                    y,\n",
    "                    rng.randint(np.iinfo(np.int32).max),\n",
    "                    save_transformed_data=save_transformed_data,\n",
    "                )\n",
    "                for _ in range(self._n_estimators)\n",
    "            )\n",
    "\n",
    "            (\n",
    "                self.estimators_,\n",
    "                self.intervals_,\n",
    "                transformed_intervals,\n",
    "            ) = zip(*fit)\n",
    "\n",
    "        return transformed_intervals\n",
    "\n",
    "    def _fit_estimator(self, Xt, y, seed, save_transformed_data=False):\n",
    "        # random state for this estimator\n",
    "        rng = check_random_state(seed)\n",
    "\n",
    "        intervals = []\n",
    "        transform_data_lengths = []\n",
    "        interval_features = np.empty((self.n_cases_, 0))\n",
    "\n",
    "        # for each transformed series\n",
    "        for r in range(len(Xt)):\n",
    "            # subsample attributes if enabled\n",
    "            if self._att_subsample_size[r] is not None:\n",
    "                # separate transformers and functions in separate lists\n",
    "                # add the feature names of transformers to a list to subsample from\n",
    "                # and calculate the total number of features\n",
    "                all_transformers = []\n",
    "                all_transformer_features = []\n",
    "                all_function_features = []\n",
    "                for feature in self._interval_features[r]:\n",
    "                    if isinstance(feature, BaseTransformer):\n",
    "                        all_transformer_features += getattr(\n",
    "                            feature,\n",
    "                            self._transformer_feature_names[r][len(all_transformers)],\n",
    "                        )\n",
    "                        all_transformers.append(feature)\n",
    "                    else:\n",
    "                        all_function_features.append(feature)\n",
    "\n",
    "                # handle float subsample size\n",
    "                num_features = len(all_transformer_features) + len(\n",
    "                    all_function_features\n",
    "                )\n",
    "                att_subsample_size = self._att_subsample_size[r]\n",
    "                if isinstance(self._att_subsample_size[r], float):\n",
    "                    att_subsample_size = int(att_subsample_size * num_features)\n",
    "\n",
    "                # if the att_subsample_size is greater than the number of features\n",
    "                # give a warning and add all features\n",
    "                features = []\n",
    "                if att_subsample_size < num_features:\n",
    "                    # subsample the transformer and function features by index\n",
    "                    atts = rng.choice(\n",
    "                        num_features,\n",
    "                        att_subsample_size,\n",
    "                        replace=False,\n",
    "                    )\n",
    "                    atts.sort()\n",
    "\n",
    "                    # subsample the feature transformers using the\n",
    "                    # transformer_feature_names and transformer_feature_selection\n",
    "                    # attributes.\n",
    "                    # the presence of valid attributes is verified in fit.\n",
    "                    count = 0\n",
    "                    length = 0\n",
    "                    for n, transformer in enumerate(all_transformers):\n",
    "                        this_len = len(\n",
    "                            getattr(transformer, self._transformer_feature_names[r][n])\n",
    "                        )\n",
    "                        length += this_len\n",
    "\n",
    "                        # subsample feature names from this transformer\n",
    "                        t_features = []\n",
    "                        while count < len(atts) and atts[count] < length:\n",
    "                            t_features.append(\n",
    "                                getattr(\n",
    "                                    transformer,\n",
    "                                    self._transformer_feature_names[r][n],\n",
    "                                )[atts[count] + this_len - length]\n",
    "                            )\n",
    "                            count += 1\n",
    "\n",
    "                        # tell this transformer to only transform the selected features\n",
    "                        if len(t_features) > 0:\n",
    "                            new_transformer = _clone_estimator(transformer, seed)\n",
    "                            setattr(\n",
    "                                new_transformer,\n",
    "                                self._transformer_feature_selection[r][n],\n",
    "                                t_features,\n",
    "                            )\n",
    "                            features.append(new_transformer)\n",
    "\n",
    "                    # subsample the remaining function features\n",
    "                    for i in range(att_subsample_size - count):\n",
    "                        features.append(all_function_features[atts[count + i] - length])\n",
    "                else:\n",
    "                    # only warn if requested number of features is greater than actual\n",
    "                    if att_subsample_size > num_features:\n",
    "                        warnings.warn(\n",
    "                            f\"Attribute subsample size {att_subsample_size} is \"\n",
    "                            f\"larger than the number of attributes {num_features} \"\n",
    "                            f\"for series {self._series_transformers[r]}\",\n",
    "                            stacklevel=2,\n",
    "                        )\n",
    "\n",
    "                    self._att_subsample_size[r] = None\n",
    "\n",
    "                    for feature in self._interval_features[r]:\n",
    "                        if isinstance(feature, BaseTransformer):\n",
    "                            features.append(_clone_estimator(feature, seed))\n",
    "                        else:\n",
    "                            features.append(feature)\n",
    "            # add all features while cloning estimators if not subsampling\n",
    "            else:\n",
    "                features = []\n",
    "                for feature in self._interval_features[r]:\n",
    "                    if isinstance(feature, BaseTransformer):\n",
    "                        features.append(_clone_estimator(feature, seed))\n",
    "                    else:\n",
    "                        features.append(feature)\n",
    "\n",
    "            # create the selected interval selector and set its parameters\n",
    "            if self.interval_selection_method == \"random\":\n",
    "                selector = RandomIntervals(\n",
    "                    n_intervals=self._n_intervals[r],\n",
    "                    min_interval_length=self._min_interval_length[r],\n",
    "                    max_interval_length=self._max_interval_length[r],\n",
    "                    features=features,\n",
    "                    random_state=seed,\n",
    "                )\n",
    "            elif self.interval_selection_method == \"supervised\":\n",
    "                selector = SupervisedIntervals(\n",
    "                    n_intervals=self._n_intervals[r],\n",
    "                    min_interval_length=self._min_interval_length[r],\n",
    "                    features=features,\n",
    "                    randomised_split_point=False,\n",
    "                    random_state=seed,\n",
    "                )\n",
    "            elif self.interval_selection_method == \"random-supervised\":\n",
    "                selector = SupervisedIntervals(\n",
    "                    n_intervals=self._n_intervals[r],\n",
    "                    min_interval_length=self._min_interval_length[r],\n",
    "                    features=features,\n",
    "                    randomised_split_point=True,\n",
    "                    random_state=seed,\n",
    "                )\n",
    "\n",
    "            # fit the interval selector, transform the current series using it and save\n",
    "            # the transformer\n",
    "            intervals.append(selector)\n",
    "            f = intervals[r].fit_transform(Xt[r], y)\n",
    "\n",
    "            # concatenate the data and save this transforms number of attributes\n",
    "            transform_data_lengths.append(f.shape[1])\n",
    "            interval_features = np.hstack((interval_features, f))\n",
    "\n",
    "        if isinstance(self.replace_nan, str) and self.replace_nan.lower() == \"nan\":\n",
    "            interval_features = np.nan_to_num(\n",
    "                interval_features, False, np.nan, np.nan, np.nan\n",
    "            )\n",
    "        elif isinstance(self.replace_nan, (int, float)):\n",
    "            interval_features = np.nan_to_num(\n",
    "                interval_features,\n",
    "                False,\n",
    "                self.replace_nan,\n",
    "                self.replace_nan,\n",
    "                self.replace_nan,\n",
    "            )\n",
    "\n",
    "        # clone and fit the base estimator using the transformed data\n",
    "        tree = _clone_estimator(self._base_estimator, random_state=seed)\n",
    "        tree.fit(interval_features, y)\n",
    "\n",
    "        # find the features used in the tree and inform the interval selectors to not\n",
    "        # transform these features if possible\n",
    "        self._efficient_predictions = True\n",
    "        relevant_features = None\n",
    "        if isinstance(tree, BaseDecisionTree):\n",
    "            relevant_features = np.unique(tree.tree_.feature[tree.tree_.feature >= 0])\n",
    "        elif isinstance(tree, ContinuousIntervalTree):\n",
    "            relevant_features, _ = tree.tree_node_splits_and_gain()\n",
    "\n",
    "        if relevant_features is not None:\n",
    "            features_to_transform = [False] * interval_features.shape[1]\n",
    "            for i in relevant_features:\n",
    "                features_to_transform[i] = True\n",
    "\n",
    "            count = 0\n",
    "            for r in range(len(Xt)):\n",
    "                intervals[r].transformer_feature_skip = self.transformer_feature_skip\n",
    "\n",
    "                # if the transformers don't have valid attributes to skip False is\n",
    "                # returned\n",
    "                completed = intervals[r].set_features_to_transform(\n",
    "                    features_to_transform[count : count + transform_data_lengths[r]],\n",
    "                    raise_error=False,\n",
    "                )\n",
    "                count += transform_data_lengths[r]\n",
    "\n",
    "                if not completed:\n",
    "                    self._efficient_predictions = False\n",
    "        else:\n",
    "            self._efficient_predictions = False\n",
    "\n",
    "        return [\n",
    "            tree,\n",
    "            intervals,\n",
    "            interval_features if save_transformed_data else None,\n",
    "        ]\n",
    "\n",
    "    def _predict_setup(self, X):\n",
    "        Xt = []\n",
    "        for transformer in self._series_transformers:\n",
    "            if transformer is None:\n",
    "                Xt.append(X)\n",
    "            elif _is_transformer(transformer):\n",
    "                Xt.append(transformer.transform(X))\n",
    "\n",
    "        return Xt\n",
    "\n",
    "    def _predict_for_estimator(self, Xt, estimator, intervals, predict_proba=False):\n",
    "        interval_features = np.empty((Xt[0].shape[0], 0))\n",
    "\n",
    "        for r in range(len(Xt)):\n",
    "            f = intervals[r].transform(Xt[r])\n",
    "            interval_features = np.hstack((interval_features, f))\n",
    "\n",
    "        if isinstance(self.replace_nan, str) and self.replace_nan.lower() == \"nan\":\n",
    "            interval_features = np.nan_to_num(\n",
    "                interval_features, False, np.nan, np.nan, np.nan\n",
    "            )\n",
    "        elif isinstance(self.replace_nan, (int, float)):\n",
    "            interval_features = np.nan_to_num(\n",
    "                interval_features,\n",
    "                False,\n",
    "                self.replace_nan,\n",
    "                self.replace_nan,\n",
    "                self.replace_nan,\n",
    "            )\n",
    "\n",
    "        if predict_proba:\n",
    "            return estimator.predict_proba(interval_features)\n",
    "        else:\n",
    "            return estimator.predict(interval_features)\n",
    "\n",
    "    def _train_estimate_for_estimator(self, Xt, y, idx, rng, probas=False):\n",
    "        indices = range(self.n_cases_)\n",
    "        subsample = rng.choice(self.n_cases_, size=self.n_cases_)\n",
    "        oob = [n for n in indices if n not in subsample]\n",
    "\n",
    "        results = (\n",
    "            np.zeros((self.n_cases_, self.n_classes_))\n",
    "            if probas\n",
    "            else np.zeros(self.n_cases_)\n",
    "        )\n",
    "        if len(oob) == 0:\n",
    "            return [results, oob]\n",
    "\n",
    "        clf = _clone_estimator(self._base_estimator, rng)\n",
    "        clf.fit(Xt[idx][subsample], y[subsample])\n",
    "        preds = clf.predict_proba(Xt[idx][oob]) if probas else clf.predict(Xt[idx][oob])\n",
    "\n",
    "        if probas and preds.shape[1] != self.n_classes_:\n",
    "            new_probas = np.zeros((preds.shape[0], self.n_classes_))\n",
    "            for i, cls in enumerate(clf.classes_):\n",
    "                cls_idx = self._class_dictionary[cls]\n",
    "                new_probas[:, cls_idx] = preds[:, i]\n",
    "            preds = new_probas\n",
    "\n",
    "        if probas:\n",
    "            for n, proba in enumerate(preds):\n",
    "                results[oob[n]] += proba\n",
    "        else:\n",
    "            for n, pred in enumerate(preds):\n",
    "                results[oob[n]] = pred\n",
    "\n",
    "        return [results, oob]\n",
    "\n",
    "    def temporal_importance_curves(\n",
    "        self, return_dict=False, normalise_time_points=False\n",
    "    ):\n",
    "        \"\"\"Calculate the temporal importance curves for each feature.\n",
    "\n",
    "        Can be finicky with transformers currently.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        return_dict : bool, default=False\n",
    "            If True, return a dictionary of curves. If False, return a list of names\n",
    "            and a list of curves.\n",
    "        normalise_time_points : bool, default=False\n",
    "            If True, normalise the time points for each feature to the number of\n",
    "            splits that used that feature. If False, return the sum of the information\n",
    "            gain for each split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        names : list of str\n",
    "            The names of the features.\n",
    "        curves : list of np.ndarray\n",
    "            The temporal importance curves for each feature.\n",
    "        \"\"\"\n",
    "        if is_regressor(self):\n",
    "            raise NotImplementedError(\n",
    "                \"Temporal importance curves are not available for regression.\"\n",
    "            )\n",
    "        if not isinstance(self._base_estimator, ContinuousIntervalTree):\n",
    "            raise ValueError(\n",
    "                \"base_estimator for temporal importance curves must\"\n",
    "                \" be ContinuousIntervalTree.\"\n",
    "            )\n",
    "\n",
    "        curves = {}\n",
    "        if normalise_time_points:\n",
    "            counts = {}\n",
    "\n",
    "        for i, est in enumerate(self.estimators_):\n",
    "            splits, gains = est.tree_node_splits_and_gain()\n",
    "            split_features = []\n",
    "\n",
    "            for n, rep in enumerate(self.intervals_[i]):\n",
    "                t = 0\n",
    "                rep_name = (\n",
    "                    \"\"\n",
    "                    if self._series_transformers[n] is None\n",
    "                    else self._series_transformers[n].__class__.__name__\n",
    "                )\n",
    "\n",
    "                for interval in rep.intervals_:\n",
    "                    if _is_transformer(interval[3]):\n",
    "                        if self._att_subsample_size[n] is None:\n",
    "                            names = None\n",
    "                            for f in self.transformer_feature_names:\n",
    "                                if hasattr(interval[3], f) and isinstance(\n",
    "                                    getattr(interval[3], f), (list, tuple)\n",
    "                                ):\n",
    "                                    names = getattr(interval[3], f)\n",
    "                                    break\n",
    "\n",
    "                            if names is None:\n",
    "                                raise ValueError(\n",
    "                                    \"All transformers in interval_features must have \"\n",
    "                                    \"an attribute or property named in \"\n",
    "                                    \"transformer_feature_names to be used in temporal \"\n",
    "                                    \"importance curves.\"\n",
    "                                )\n",
    "                        else:\n",
    "                            if t % len(self._interval_features[n]) - 1 == 0:\n",
    "                                t = 0\n",
    "\n",
    "                            names = getattr(\n",
    "                                interval[3], self._transformer_feature_names[n][t]\n",
    "                            )\n",
    "                            t += 1\n",
    "\n",
    "                        split_features.extend(\n",
    "                            [\n",
    "                                (\n",
    "                                    rep_name,\n",
    "                                    interval[0],\n",
    "                                    interval[1],\n",
    "                                    interval[2],\n",
    "                                    feature_name,\n",
    "                                )\n",
    "                                for feature_name in names\n",
    "                            ]\n",
    "                        )\n",
    "                    else:\n",
    "                        split_features.append(\n",
    "                            (\n",
    "                                rep_name,\n",
    "                                interval[0],\n",
    "                                interval[1],\n",
    "                                interval[2],\n",
    "                                interval[3].__name__,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            for n, split in enumerate(splits):\n",
    "                feature = (\n",
    "                    split_features[split][0],\n",
    "                    split_features[split][3],\n",
    "                    split_features[split][4],\n",
    "                )\n",
    "\n",
    "                if feature not in curves:\n",
    "                    curves[feature] = np.zeros(self.n_timepoints_)\n",
    "                    curves[feature][\n",
    "                        split_features[split][1] : split_features[split][2]\n",
    "                    ] = gains[n]\n",
    "\n",
    "                    if normalise_time_points:\n",
    "                        counts[feature] = np.zeros(self.n_timepoints_)\n",
    "                        counts[feature][\n",
    "                            split_features[split][1] : split_features[split][2]\n",
    "                        ] = 1\n",
    "                else:\n",
    "                    curves[feature][\n",
    "                        split_features[split][1] : split_features[split][2]\n",
    "                    ] += gains[n]\n",
    "\n",
    "                    if normalise_time_points:\n",
    "                        counts[feature][\n",
    "                            split_features[split][1] : split_features[split][2]\n",
    "                        ] += 1\n",
    "\n",
    "        if normalise_time_points:\n",
    "            for feature in counts:\n",
    "                curves[feature] /= counts[feature]\n",
    "\n",
    "        if return_dict:\n",
    "            return curves\n",
    "        else:\n",
    "            names = []\n",
    "            values = []\n",
    "            for key, value in curves.items():\n",
    "                dim = f\"_dim{key[1]}\" if self.n_channels_ > 1 else \"\"\n",
    "                rep = f\"{key[0]}_\" if key[0] != \"\" else \"\"\n",
    "                names.append(f\"{rep}{key[2]}{dim}\")\n",
    "                values.append(value)\n",
    "\n",
    "            names, values = zip(*sorted(zip(names, values)))\n",
    "\n",
    "            return names, values\n",
    "\n",
    "\n",
    "def _is_transformer(obj):\n",
    "    if isinstance(obj, BaseTransformer) or isinstance(obj, FunctionTransformer):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 173737,
     "status": "ok",
     "timestamp": 1724485826929,
     "user": {
      "displayName": "Bijoyashree Das",
      "userId": "11723202937662517391"
     },
     "user_tz": -330
    },
    "id": "0dmQSVsBg39H",
    "outputId": "caf40d1c-ee21-44c3-a28f-899e5801dd24"
   },
   "outputs": [],
   "source": [
    "\"\"\"DrCIF classifier.\n",
    "\n",
    "Interval-based DrCIF classifier extracting catch22 features from random intervals on\n",
    "periodogram and differences representations as well as the base series.\n",
    "\"\"\"\n",
    "\n",
    "__maintainer__ = []\n",
    "__all__ = [\"DrCIFClassifier\"]\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#from aeon.base._estimators.interval_based import BaseIntervalForest\n",
    "from aeon.classification.base import BaseClassifier\n",
    "from aeon.classification.sklearn._continuous_interval_tree import ContinuousIntervalTree\n",
    "from aeon.transformations.collection import PeriodogramTransformer\n",
    "from aeon.transformations.collection.feature_based import Catch22\n",
    "from aeon.utils.numba.general import first_order_differences_3d\n",
    "from aeon.utils.numba.stats import (\n",
    "    row_iqr,\n",
    "    row_mean,\n",
    "    row_median,\n",
    "    row_numba_max,\n",
    "    row_numba_min,\n",
    "    row_slope,\n",
    "    row_std,\n",
    ")\n",
    "\n",
    "\n",
    "class DrCIFClassifier(BaseIntervalForest, BaseClassifier):\n",
    "    \"\"\"\n",
    "    Diverse Representation Canonical Interval Forest Classifier (DrCIF).\n",
    "\n",
    "    Extension of the CIF algorithm using multiple representations. Implementation of the\n",
    "    interval-based forest making use of the catch22 feature set on randomly selected\n",
    "    intervals on the base series, periodogram representation and differences\n",
    "    representation described in the HIVE-COTE 2.0 paper Middlehurst et al (2021). [1]_\n",
    "\n",
    "    Overview: Input \"n\" series with \"d\" dimensions of length \"m\".\n",
    "    For each tree\n",
    "        - Sample n_intervals intervals per representation of random position and length\n",
    "        - Subsample att_subsample_size catch22 or summary statistic attributes randomly\n",
    "        - Randomly select dimension for each interval\n",
    "        - Calculate attributes for each interval from its representation, concatenate\n",
    "          to form new data set\n",
    "        - Build a decision tree on new data set\n",
    "    Ensemble the trees with averaged probability estimates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : BaseEstimator or None, default=None\n",
    "        scikit-learn BaseEstimator used to build the interval ensemble. If None, use a\n",
    "        simple decision tree.\n",
    "    n_estimators : int, default=200\n",
    "        Number of estimators to build for the ensemble.\n",
    "    n_intervals : int, str, list or tuple, default=\"sqrt\"\n",
    "        Number of intervals to extract per tree for each series_transformers series.\n",
    "\n",
    "        An int input will extract that number of intervals from the series, while a str\n",
    "        input will return a function of the series length (may differ per\n",
    "        series_transformers output) to extract that number of intervals.\n",
    "        Valid str inputs are:\n",
    "            - \"sqrt\": square root of the series length.\n",
    "            - \"sqrt-div\": sqrt of series length divided by the number\n",
    "                of series_transformers.\n",
    "\n",
    "        A list or tuple of ints and/or strs will extract the number of intervals using\n",
    "        the above rules and sum the results for the final n_intervals. i.e. [4, \"sqrt\"]\n",
    "        will extract sqrt(n_timepoints) + 4 intervals.\n",
    "\n",
    "        Different number of intervals for each series_transformers series can be\n",
    "        specified using a nested list or tuple. Any list or tuple input containing\n",
    "        another list or tuple must be the same length as the number of\n",
    "        series_transformers.\n",
    "\n",
    "        While random interval extraction will extract the n_intervals intervals total\n",
    "        (removing duplicates), supervised intervals will run the supervised extraction\n",
    "        process n_intervals times, returning more intervals than specified.\n",
    "    min_interval_length : int, float, list, or tuple, default=3\n",
    "        Minimum length of intervals to extract from series. float inputs take a\n",
    "        proportion of the series length to use as the minimum interval length.\n",
    "\n",
    "        Different minimum interval lengths for each series_transformers series can be\n",
    "        specified using a list or tuple. Any list or tuple input must be the same length\n",
    "        as the number of series_transformers.\n",
    "    max_interval_length : int, float, list, or tuple, default=np.inf\n",
    "        Maximum length of intervals to extract from series. float inputs take a\n",
    "        proportion of the series length to use as the maximum interval length.\n",
    "\n",
    "        Different maximum interval lengths for each series_transformers series can be\n",
    "        specified using a list or tuple. Any list or tuple input must be the same length\n",
    "        as the number of series_transformers.\n",
    "\n",
    "        Ignored for supervised interval_selection_method inputs.\n",
    "    att_subsample_size : int, float, list, tuple or None, default=None\n",
    "        The number of attributes to subsample for each estimator. If None, use all\n",
    "\n",
    "        If int, use that number of attributes for all estimators. If float, use that\n",
    "        proportion of attributes for all estimators.\n",
    "\n",
    "        Different subsample sizes for each series_transformers series can be specified\n",
    "        using a list or tuple. Any list or tuple input must be the same length as the\n",
    "        number of series_transformers.\n",
    "    time_limit_in_minutes : int, default=0\n",
    "        Time contract to limit build time in minutes, overriding n_estimators.\n",
    "        Default of 0 means n_estimators are used.\n",
    "    contract_max_n_estimators : int, default=500\n",
    "        Max number of estimators when time_limit_in_minutes is set.\n",
    "    use_pycatch22 : bool, optional, default=False\n",
    "        Wraps the C based pycatch22 implementation for aeon.\n",
    "        (https://github.com/DynamicsAndNeuralSystems/pycatch22). This requires the\n",
    "        ``pycatch22`` package to be installed if True.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        If `int`, random_state is the seed used by the random number generator;\n",
    "        If `RandomState` instance, random_state is the random number generator;\n",
    "        If `None`, the random number generator is the `RandomState` instance used\n",
    "        by `np.random`.\n",
    "    n_jobs : int, default=1\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``-1`` means using all processors.\n",
    "    parallel_backend : str, ParallelBackendBase instance or None, default=None\n",
    "        Specify the parallelisation backend implementation in joblib, if None a 'prefer'\n",
    "        value of \"threads\" is used by default.\n",
    "        Valid options are \"loky\", \"multiprocessing\", \"threading\" or a custom backend.\n",
    "        See the joblib Parallel documentation for more details.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_cases_ : int\n",
    "        The number of train cases in the training set.\n",
    "    n_channels_ : int\n",
    "        The number of dimensions per case in the training set.\n",
    "    n_timepoints_ : int\n",
    "        The length of each series in the training set.\n",
    "    n_classes_ : int\n",
    "        Number of classes. Extracted from the data.\n",
    "    classes_ : ndarray of shape (n_classes_)\n",
    "        Holds the label for each class.\n",
    "    total_intervals_ : int\n",
    "        Total number of intervals per tree from all representations.\n",
    "    estimators_ : list of shape (n_estimators) of BaseEstimator\n",
    "        The collections of estimators trained in fit.\n",
    "    intervals_ : list of shape (n_estimators) of TransformerMixin\n",
    "        Stores the interval extraction transformer for all estimators.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    DrCIFRegressor\n",
    "    CanonicalIntervalForestClassifier\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For the Java version, see\n",
    "    `TSML <https://github.com/uea-machine-learning/tsml/blob/master/src/main/java\n",
    "    /tsml/classifiers/interval_based/DrCIF.java>`_.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Middlehurst, Matthew, James Large, Michael Flynn, Jason Lines, Aaron Bostrom,\n",
    "       and Anthony Bagnall. \"HIVE-COTE 2.0: a new meta ensemble for time series\n",
    "       classification.\" arXiv preprint arXiv:2104.07551 (2021).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from aeon.classification.interval_based import DrCIFClassifier\n",
    "    >>> from aeon.testing.data_generation import make_example_3d_numpy\n",
    "    >>> X, y = make_example_3d_numpy(n_cases=10, n_channels=1, n_timepoints=12,\n",
    "    ...                              return_y=True, random_state=0)\n",
    "    >>> clf = DrCIFClassifier(n_estimators=10, random_state=0)\n",
    "    >>> clf.fit(X, y)\n",
    "    DrCIFClassifier(n_estimators=10, random_state=0)\n",
    "    >>> clf.predict(X)\n",
    "    array([0, 1, 0, 1, 0, 0, 1, 1, 1, 0])\n",
    "    \"\"\"\n",
    "\n",
    "    _tags = {\n",
    "        \"capability:multivariate\": True,\n",
    "        \"capability:train_estimate\": True,\n",
    "        \"capability:contractable\": True,\n",
    "        \"capability:multithreading\": True,\n",
    "        \"algorithm_type\": \"interval\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator=None,\n",
    "        n_estimators=200,\n",
    "        n_intervals=(4, \"sqrt-div\"),\n",
    "        min_interval_length=3,\n",
    "        max_interval_length=0.5,\n",
    "        att_subsample_size=10,\n",
    "        time_limit_in_minutes=None,\n",
    "        contract_max_n_estimators=500,\n",
    "        use_pycatch22=False,\n",
    "        random_state=None,\n",
    "        n_jobs=1,\n",
    "        parallel_backend=None,\n",
    "    ):\n",
    "        self.use_pycatch22 = use_pycatch22\n",
    "\n",
    "        if isinstance(base_estimator, ContinuousIntervalTree):\n",
    "            replace_nan = \"nan\"\n",
    "        else:\n",
    "            replace_nan = 0\n",
    "\n",
    "        series_transformers = [\n",
    "            None,\n",
    "            FunctionTransformer(func=first_order_differences_3d, validate=False),\n",
    "            PeriodogramTransformer(),\n",
    "        ]\n",
    "\n",
    "        interval_features = [\n",
    "            Catch22(outlier_norm=True, use_pycatch22=use_pycatch22),\n",
    "            row_mean,\n",
    "            row_std,\n",
    "            row_slope,\n",
    "            row_median,\n",
    "            row_iqr,\n",
    "            row_numba_min,\n",
    "            row_numba_max,\n",
    "        ]\n",
    "\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            interval_selection_method=\"random\",\n",
    "            n_intervals=n_intervals,\n",
    "            min_interval_length=min_interval_length,\n",
    "            max_interval_length=max_interval_length,\n",
    "            interval_features=interval_features,\n",
    "            series_transformers=series_transformers,\n",
    "            att_subsample_size=att_subsample_size,\n",
    "            replace_nan=replace_nan,\n",
    "            time_limit_in_minutes=time_limit_in_minutes,\n",
    "            contract_max_n_estimators=contract_max_n_estimators,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs,\n",
    "            parallel_backend=parallel_backend,\n",
    "        )\n",
    "\n",
    "        if use_pycatch22:\n",
    "            self.set_tags(**{\"python_dependencies\": \"pycatch22\"})\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        return super()._fit(X, y)\n",
    "\n",
    "    def _predict(self, X) -> np.ndarray:\n",
    "        return super()._predict(X)\n",
    "\n",
    "    def _predict_proba(self, X) -> np.ndarray:\n",
    "        return super()._predict_proba(X)\n",
    "\n",
    "    def _fit_predict(self, X, y) -> np.ndarray:\n",
    "        return super()._fit_predict(X, y)\n",
    "\n",
    "    def _fit_predict_proba(self, X, y) -> np.ndarray:\n",
    "        return super()._fit_predict_proba(X, y)\n",
    "\n",
    "    @classmethod\n",
    "    def _get_test_params(cls, parameter_set=\"default\"):\n",
    "        \"\"\"Return testing parameter settings for the estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameter_set : str, default=\"default\"\n",
    "            Name of the set of test parameters to return, for use in tests. If no\n",
    "            special parameters are defined for a value, will return `\"default\"` set.\n",
    "            DrCIFClassifier provides the following special sets:\n",
    "                 \"results_comparison\" - used in some classifiers to compare against\n",
    "                    previously generated results where the default set of parameters\n",
    "                    cannot produce suitable probability estimates\n",
    "                \"contracting\" - used in classifiers that set the\n",
    "                    \"capability:contractable\" tag to True to test contacting\n",
    "                    functionality\n",
    "                \"train_estimate\" - used in some classifiers that set the\n",
    "                    \"capability:train_estimate\" tag to True to allow for more efficient\n",
    "                    testing when relevant parameters are available\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        params : dict or list of dict, default={}\n",
    "            Parameters to create testing instances of the class.\n",
    "            Each dict are parameters to construct an \"interesting\" test instance, i.e.,\n",
    "            `MyClass(**params)` or `MyClass(**params[i])` creates a valid test instance.\n",
    "        \"\"\"\n",
    "        if parameter_set == \"results_comparison\":\n",
    "            return {\"n_estimators\": 10, \"n_intervals\": 2, \"att_subsample_size\": 4}\n",
    "        elif parameter_set == \"contracting\":\n",
    "            return {\n",
    "                \"time_limit_in_minutes\": 5,\n",
    "                \"contract_max_n_estimators\": 2,\n",
    "                \"n_intervals\": 2,\n",
    "                \"att_subsample_size\": 2,\n",
    "            }\n",
    "        else:\n",
    "            return {\"n_estimators\": 2, \"n_intervals\": 2, \"att_subsample_size\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqleLI-_g39L",
    "outputId": "0a9ce574-c0ec-44a8-895c-a0ffaf89dc15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in folder 'down': 2359\n",
      "Total number of files in folder 'go': 2372\n",
      "Total number of files in folder 'left': 2353\n",
      "Total number of files in folder 'no': 2375\n",
      "Total number of files in folder 'off': 2357\n",
      "Total number of files in folder 'on': 2367\n",
      "Total number of files in folder 'right': 2367\n",
      "Total number of files in folder 'silence': 2010\n",
      "Total number of files in folder 'stop': 2380\n",
      "Total number of files in folder 'unknown': 2000\n",
      "Total number of files in folder 'up': 2375\n",
      "Total number of files in folder 'yes': 2377\n",
      "(27692, 1, 16000)\n",
      "(27692,)\n",
      "[ 0  0  0 ... 11 11 11]\n",
      "Overall total number of files in the dataset: 27692\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def load_data_from_directory(directory, sample_length=16000, n_channels=1):\n",
    "    X = []\n",
    "    y = []\n",
    "    labels = sorted(os.listdir(directory))\n",
    "    label_map = {label: idx for idx, label in enumerate(labels)}  # Create a label to index mapping\n",
    "\n",
    "    total_files = 0  # Initialize a counter for total files\n",
    "\n",
    "    for label in labels:\n",
    "        class_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(class_dir):\n",
    "            file_count = 0  # Counter for files in the current class directory\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.endswith('.wav'):\n",
    "                    file_count += 1  # Increment file count for the current class\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    # Load audio\n",
    "                    signal, sr = librosa.load(file_path, sr=16000)\n",
    "                    # Ensure length is 1 second (16000 samples)\n",
    "                    if len(signal) != sample_length:\n",
    "                        # Pad or truncate to sample_length\n",
    "                        if len(signal) < sample_length:\n",
    "                            signal = np.pad(signal, (0, sample_length - len(signal)))\n",
    "                        else:\n",
    "                            signal = signal[:sample_length]\n",
    "                    # Reshape signal to match (n_channels, n_timepoints)\n",
    "                    X.append(signal.reshape(n_channels, -1))  # Reshape to (1, sample_length)\n",
    "                    y.append(label_map[label])  # Use the index of the label\n",
    "\n",
    "            total_files += file_count  # Add the current class file count to the total\n",
    "            print(f\"Total number of files in folder '{label}': {file_count}\")\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    print(f\"Overall total number of files in the dataset: {total_files}\")\n",
    "\n",
    "    return X, y, labels\n",
    "\n",
    "# Load and preprocess data\n",
    "directory = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/12KWS\"\n",
    "X, y, labels = load_data_from_directory(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Lc6AZsymg39M"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q5btkexOg39M"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Split data into training and testing sets with a fixed random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Classifier\n",
    "\n",
    "clf = DrCIFClassifier(\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier on the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11pB32yMg39N"
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test data:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision, recall, and f1 score for each class (macro average)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Precision (macro): {precision}\")\n",
    "print(f\"Recall (macro): {recall}\")\n",
    "print(f\"F1 Score (macro): {f1}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
