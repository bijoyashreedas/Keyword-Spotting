{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e40348-95a1-4cc1-930a-afab56e0bd21",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25235b-26dd-4e5a-8021-181221dfd169",
   "metadata": {},
   "source": [
    "Create conda environment->\n",
    "conda create -n whisenv\n",
    "\n",
    "Activate the environment->\n",
    "conda activate whisenv\n",
    "\n",
    "\n",
    "Install below packages in conda prompt once environment activated->\n",
    "\n",
    "conda install -c conda-forge pandas tqdm numpy -y\n",
    "\n",
    "conda install pytorch torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "pip install transformers datasets evaluate peft bitsandbytes accelerate\n",
    "\n",
    "pip install --no-cache-dir h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4329c8c8-8ff4-4e16-b017-ac5186face9e",
   "metadata": {},
   "source": [
    "(Refer environment.yml for exact package versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdceea1-d202-4d0d-a79c-e466b6d16aec",
   "metadata": {},
   "source": [
    "##### Model Setup for Hindi Speech-to-Text with Whisper. \n",
    "This script initializes the OpenAI Whisper large model for automatic speech recognition (ASR) in Hindi. It loads:\n",
    "\n",
    "WhisperTokenizer to tokenize Hindi audio transcripts,\n",
    "\n",
    "WhisperFeatureExtractor to process raw audio inputs,\n",
    "\n",
    "WhisperForConditionalGeneration as the core ASR model,\n",
    "\n",
    "The model is set up for the task of transcribing Hindi speech and is loaded onto the GPU (cuda) for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d711bc61-30aa-4c06-95f0-3a7fdb9edce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large\",language='hindi',task='transcribe')\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large\",language='hindi',task='transcribe')\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\").to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61bb576-ff8d-4411-bcd0-cbd65dac947e",
   "metadata": {},
   "source": [
    "##### Whisper Processor Initialization for Hindi ASR. \n",
    "This code loads the WhisperProcessor from Hugging Face’s transformers library, combining both the tokenizer and feature extractor into a single processor. It’s configured for transcribing Hindi audio using the OpenAI Whisper large model, streamlining the preprocessing and tokenization steps for speech-to-text tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bc822980-dab1-4789-9e43-770d5bf2540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\",language='hindi',task='transcribe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9e0f6-e6ef-4390-b67f-8489537f313e",
   "metadata": {},
   "source": [
    "## Dataset processing(train+dev,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fb31b-2e02-4194-9ddf-d9ea8b31b5e8",
   "metadata": {},
   "source": [
    "#### Dataset Preparation for Hindi Common Voice ASR\n",
    "This script loads the Hindi Common Voice dataset (train, dev, test splits) from local .tsv files and audio clips, merges train and dev sets for training, and converts them into Hugging Face DatasetDict format. It processes audio file paths to full paths, keeps only necessary columns (audio and sentence), and casts the audio column to the appropriate Audio type for easy use with Hugging Face pipelines.\n",
    "\n",
    "Train samples:7563\n",
    "\n",
    "\n",
    "Test samples:3337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e33063ba-3dbc-4bb5-9c55-b51330038f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 7563\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 3337\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the dataset path\n",
    "DATASET_PATH = r\"C:\\Users\\WORKSTATIONS\\Desktop\\BijoyashreeDas\\COMMON_VOICE_HI\\cv-corpus-21.0-2025-03-14\\hi\"\n",
    "CLIPS_PATH = os.path.join(DATASET_PATH, \"clips\")\n",
    "\n",
    "# Load train+dev as train\n",
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, \"train.tsv\"), sep=\"\\t\")\n",
    "dev_df = pd.read_csv(os.path.join(DATASET_PATH, \"dev.tsv\"), sep=\"\\t\")\n",
    "train_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, \"test.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Function to get full audio path\n",
    "def get_audio_path(filename):\n",
    "    return os.path.join(CLIPS_PATH, filename)\n",
    "\n",
    "# Convert data to Hugging Face dataset format\n",
    "def convert_to_hf_dataset(df):\n",
    "    df = df[['path', 'sentence']].dropna()  # Keep only required columns\n",
    "    df['audio'] = df['path'].apply(get_audio_path)  # Convert paths\n",
    "    return Dataset.from_pandas(df[['audio', 'sentence']])  # Create HF dataset\n",
    "\n",
    "# Convert train and test to Hugging Face format\n",
    "commonvoice_train = convert_to_hf_dataset(train_df)\n",
    "commonvoice_test = convert_to_hf_dataset(test_df)\n",
    "\n",
    "# Define dataset dictionary\n",
    "commonvoice_dataset = DatasetDict({\n",
    "    \"train\": commonvoice_train,\n",
    "    \"test\": commonvoice_test\n",
    "})\n",
    "\n",
    "# Cast the audio column to Hugging Face Audio format\n",
    "commonvoice_dataset = commonvoice_dataset.cast_column(\"audio\", Audio())\n",
    "\n",
    "# Print dataset structure\n",
    "print(commonvoice_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0740b3-f481-4f59-9e7a-0262681a0a72",
   "metadata": {},
   "source": [
    "##### Inspecting Dataset Samples\n",
    "This snippet demonstrates how to access and inspect individual samples from the prepared Hugging Face dataset. It retrieves the first example from the training split, printing the audio file path and its corresponding transcription text. This is useful for verifying the dataset loading and preprocessing steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "dc51c741-0878-42cb-821a-0acd43e75f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File: C:\\Users\\WORKSTATIONS\\Desktop\\BijoyashreeDas\\COMMON_VOICE_HI\\cv-corpus-21.0-2025-03-14\\hi\\clips\\common_voice_hi_26008353.mp3\n",
      "Transcription: हमने उसका जन्मदिन मनाया।\n"
     ]
    }
   ],
   "source": [
    "# Get the first sample from the train set\n",
    "first_sample = commonvoice_dataset[\"train\"][0]\n",
    "\n",
    "# Print the audio filename and transcription\n",
    "print(\"Audio File:\", first_sample[\"audio\"][\"path\"])\n",
    "print(\"Transcription:\", first_sample[\"sentence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e2095-e8a1-4c3e-a9de-0ec39d46758f",
   "metadata": {},
   "source": [
    "## Total hours in train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589b3b6-10d7-42c0-b3a9-3ed1026fee28",
   "metadata": {},
   "source": [
    "##### Calculating Total Duration of Train and Test Audio Files\n",
    "This code calculates the total duration of all audio files in the train and test splits of the dataset. Using torchaudio to load each audio file, it sums their lengths in seconds and converts the result to hours. This metric helps understand the amount of audio data available for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de454500-804e-4c2f-80cd-61de940f1b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating duration: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7563/7563 [01:33<00:00, 80.90it/s]\n",
      "Calculating duration: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3337/3337 [00:47<00:00, 70.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration of Train set: 9.38 hours\n",
      "Total duration of Test set: 4.73 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to calculate total duration of audio files\n",
    "def get_total_duration(dataset):\n",
    "    total_duration = 0.0  # In seconds\n",
    "    for sample in tqdm(dataset, desc=\"Calculating duration\"):\n",
    "        audio_path = sample[\"audio\"][\"path\"]\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)  # Load audio\n",
    "        total_duration += waveform.shape[1] / sample_rate  # Compute duration (seconds)\n",
    "    \n",
    "    return total_duration / 3600  # Convert seconds to hours\n",
    "\n",
    "# Compute total duration for train and test sets\n",
    "train_hours = get_total_duration(commonvoice_dataset[\"train\"])\n",
    "test_hours = get_total_duration(commonvoice_dataset[\"test\"])\n",
    "\n",
    "# Print results\n",
    "print(f\"Total duration of Train set: {train_hours:.2f} hours\")\n",
    "print(f\"Total duration of Test set: {test_hours:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e7279-a726-49e6-96ef-459ad5060398",
   "metadata": {},
   "source": [
    "## Resample audio files to 16kHz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575d66e-d355-40f2-b25d-3d3bc210f499",
   "metadata": {},
   "source": [
    "\n",
    "This script resamples all audio samples in the Common Voice dataset to a consistent 16 kHz sampling rate, which is commonly required for speech recognition models like Whisper. It uses torchaudio for resampling and processes both the training and testing splits to ensure uniform audio input quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ed998b68-61cf-4724-9e9d-ed0a768f9d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sampling Rate: 32000 Hz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab46a5a15e6491f9a04243398c45e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f74783d55544d02a78017cb37afddbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resampling complete. All audio is now at 16kHz.\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from datasets import Audio\n",
    "\n",
    "# Get the first audio sample in the train set\n",
    "sample = commonvoice_dataset[\"train\"][0][\"audio\"]\n",
    "\n",
    "# Print original sampling rate\n",
    "print(f\"Original Sampling Rate: {sample['sampling_rate']} Hz\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to resample audio to 16kHz\n",
    "def resample_audio(batch):\n",
    "    waveform = batch[\"audio\"][\"array\"]\n",
    "    orig_sr = batch[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "\n",
    "    # Resample if needed\n",
    "    if orig_sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_sr, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    return {\"audio\": {\"array\": waveform.numpy(), \"sampling_rate\": 16000}}  # Convert back to NumPy\n",
    "\n",
    "# Apply the resampling function to train and test sets\n",
    "commonvoice_dataset = commonvoice_dataset.map(resample_audio)\n",
    "\n",
    "print(\"✅ Resampling complete. All audio is now at 16kHz.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9add6-1325-4965-a851-e6d8dbcada0b",
   "metadata": {},
   "source": [
    "### Inspecting Sampling Rates of Random Audio Samples\n",
    "This utility function prints the sampling rates of a few randomly selected audio files from the dataset splits (train and test). It helps verify that audio resampling (to 16 kHz) was applied correctly and consistently across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4ab74de-57fe-426b-b99c-f5c6fb66038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling rates of 3 random files from 'train' set:\n",
      "Sample 3346: 16000 Hz\n",
      "Sample 3332: 16000 Hz\n",
      "Sample 4704: 16000 Hz\n",
      "\n",
      "Sampling rates of 3 random files from 'test' set:\n",
      "Sample 2137: 16000 Hz\n",
      "Sample 3064: 16000 Hz\n",
      "Sample 1557: 16000 Hz\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to print sampling rate of N random samples\n",
    "def print_random_sampling_rates(dataset, split, num_samples=3):\n",
    "    print(f\"\\nSampling rates of {num_samples} random files from '{split}' set:\")\n",
    "    \n",
    "    # Select random indices\n",
    "    random_indices = random.sample(range(len(dataset[split])), num_samples)\n",
    "    \n",
    "    # Fetch and print sampling rates\n",
    "    for idx in random_indices:\n",
    "        sample = dataset[split][idx][\"audio\"]\n",
    "        print(f\"Sample {idx}: {sample['sampling_rate']} Hz\")\n",
    "\n",
    "# Print sampling rates for train and test sets\n",
    "print_random_sampling_rates(commonvoice_dataset, \"train\")\n",
    "print_random_sampling_rates(commonvoice_dataset, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d985a0-1239-482c-9cb4-13500fc86584",
   "metadata": {},
   "source": [
    "## WER on train and test sets before fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5bb3b-cef8-42cb-87ef-5179363e9714",
   "metadata": {},
   "source": [
    "This code performs transcription of the audio dataset using a pretrained Whisper model and evaluates the transcription quality with the Word Error Rate (WER) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de565150-db6b-4040-a2f1-82578ed56b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d7fd935c0048a4a8c83a294c85ecfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c213e3ef0f8749dc9bb52ddbaa8edcb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WER on Train Set: 68.92%\n",
      "✅ WER on Test Set: 71.67%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Function to transcribe audio\n",
    "def transcribe_audio(batch):\n",
    "    audio = batch[\"audio\"][\"array\"]  # Get audio waveform\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")  # Process audio\n",
    "    input_features = inputs.input_features.to(\"cuda\")  # Move to GPU\n",
    "\n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "\n",
    "    # Decode predictions\n",
    "    transcription = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return {\"transcription\": transcription}\n",
    "\n",
    "# Apply transcription function to train and test sets\n",
    "commonvoice_dataset = commonvoice_dataset.map(transcribe_audio)\n",
    "\n",
    "# Compute WER\n",
    "def compute_wer(dataset):\n",
    "    references = [x[\"sentence\"] for x in dataset]  # Ground truth\n",
    "    predictions = [x[\"transcription\"] for x in dataset]  # Model output\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    return wer\n",
    "\n",
    "# Compute WER for train and test sets\n",
    "train_wer = compute_wer(commonvoice_dataset[\"train\"])\n",
    "test_wer = compute_wer(commonvoice_dataset[\"test\"])\n",
    "\n",
    "print(f\"✅ WER on Train Set: {train_wer:.2%}\")\n",
    "print(f\"✅ WER on Test Set: {test_wer:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61db818-8d14-485b-98d8-d5055c8e4767",
   "metadata": {},
   "source": [
    "## Actual vs Predicted transcription for 10 random audio samples from test (using baseline whisper model without finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "46a0e9a5-2c54-4b63-8154-5c502c9d30aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Comparing Actual vs. Predicted Transcriptions (First 10 Samples)\n",
      "\n",
      "🎧 **Sample 1:**\n",
      "📝 **Actual   :** अब रामपुर में अखिलेश बांटेंगे लैपटॉप का 'लॉलीपॉप'\n",
      "🤖 **Predicted:**  अब राम्पुर में अकिलेश बातेंगे लैप्टप का लोलीपॉप\n",
      "--------------------------------------------------------------------------------\n",
      "🎧 **Sample 2:**\n",
      "📝 **Actual   :** Flipkart: बंपर ऑफर्स के साथ बिक रहा है Lenovo का ये शानदार स्मार्टफोन\n",
      "🤖 **Predicted:**  Flipkart, Bumper offer के साथ बिग रहा है, Lenovo का ये शानदार smartphone\n",
      "--------------------------------------------------------------------------------\n",
      "🎧 **Sample 3:**\n",
      "📝 **Actual   :** मैं मुसीबत में पड़ गया।\n",
      "🤖 **Predicted:**  मैं मुसीवत में पढ़ गया\n",
      "--------------------------------------------------------------------------------\n",
      "🎧 **Sample 4:**\n",
      "📝 **Actual   :** सुशील मोदी है 'अफवाह मियां', बिगड़ चुका है मानसिक संतुलन: तेजस्वी यादव\n",
      "🤖 **Predicted:**  शुशील मोडी है अख्वा मियां बीगर चुका है मनसी संकुलं ते जस्पी आदा\n",
      "--------------------------------------------------------------------------------\n",
      "🎧 **Sample 5:**\n",
      "📝 **Actual   :** प. बंगाल में मुस्लिम आरक्षण को भाजपा देगी चुनौती\n",
      "🤖 **Predicted:**  पश्चब बंगाल में मुस्लिम आरक्षन को भाज़पा देगी चिनौती\n",
      "--------------------------------------------------------------------------------\n",
      "🎧 **Sample 6:**\n",
      "📝 **Actual   :** पाकिस्तानी कलाकार भले अच्छे हों, लेकिन देश पहले है: हेमा मालिनी\n",
      "🤖 **Predicted:**  पाकिस्टानी कालाकार बढ़े आच्छे हो लिखिन देस्पहल है, एमा मालिनी\n",
      "--------------------------------------------------------------------------------\n",
      "🎧 **Sample 7:**\n",
      "📝 **Actual   :** नाइजीरिया स्कूल हमला: पीएम नरेंद्र मोदी ने की घटना की निंदा\n",
      "🤖 **Predicted:**  नाइचीरिया स्कूल हमला पेम निनद्रबुतिने की केव्ना के निंदा\n",
      "--------------------------------------------------------------------------------\n",
      "🎧 **Sample 8:**\n",
      "📝 **Actual   :** मैं तुम्हें तुम्हारे होमवर्क के साथ मदद करना चाहता हूँ।\n",
      "🤖 **Predicted:**  मैं तुमे तुमारे होंगआ के साथ मदद करने चाहता हूं।\n",
      "--------------------------------------------------------------------------------\n",
      "🎧 **Sample 9:**\n",
      "📝 **Actual   :** मुझे लगता है कि आपको कॉलेज जाना चाहिये।\n",
      "🤖 **Predicted:**  मुझे लगता है कि आपको कॉलेड जाना चाहिए\n",
      "--------------------------------------------------------------------------------\n",
      "🎧 **Sample 10:**\n",
      "📝 **Actual   :** विपक्षी दलों ने मनाया काला दिवस\n",
      "🤖 **Predicted:**  विपक्षी दलों ए मनाया काला दीवस\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to transcribe audio using Whisper model\n",
    "def transcribe_audio(audio_array, sampling_rate):\n",
    "    # Preprocess audio with forced transcription mode (no translation)\n",
    "    inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(\"cuda\")\n",
    "\n",
    "    # Add forced transcription settings\n",
    "    forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "        language=\"hi\", task=\"transcribe\"  # Set language and force transcription\n",
    "    )\n",
    "\n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            forced_decoder_ids=forced_decoder_ids\n",
    "        )\n",
    "\n",
    "    # Decode predictions\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "# 🎧 Compare Actual vs. Predicted for First 5 Samples in Test Set\n",
    "print(\"\\n✅ Comparing Actual vs. Predicted Transcriptions (First 10 Samples)\\n\")\n",
    "\n",
    "for idx in range(min(10, len(commonvoice_dataset[\"test\"]))):\n",
    "    sample = commonvoice_dataset[\"test\"][idx]\n",
    "    predicted_transcription = transcribe_audio(sample[\"audio\"][\"array\"], sample[\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "    print(f\"🎧 **Sample {idx+1}:**\")\n",
    "    print(f\"📝 **Actual   :** {sample['sentence']}\")\n",
    "    print(f\"🤖 **Predicted:** {predicted_transcription}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b90b4-f58b-463c-af0e-a46125e02cf6",
   "metadata": {},
   "source": [
    "## Extract log-mel features and tokenize the transcriptions(for both train and test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594916fb-0bb8-4dc0-b7b9-5af1b2b68e69",
   "metadata": {},
   "source": [
    "This code applies feature extraction and tokenization to the entire dataset, including both training and testing splits. It uses the Whisper feature extractor to convert raw audio waveforms into log-Mel spectrogram features, which are suitable as model inputs. Simultaneously, it tokenizes the corresponding text transcriptions into token IDs for training targets. By calling .map() on the Hugging Face DatasetDict, these preprocessing steps are automatically performed on all dataset splits, streamlining the data preparation process for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "91a685aa-175d-45a1-a51e-bb38c0e5d340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a5b6a857d94e98828fd335668c3d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6681dbc5ac2843b7b266a159a1a8b307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature extraction and tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "# Load the Whisper feature extractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "def extract_features_and_encode(batch):\n",
    "    # Extract log-Mel spectrogram features\n",
    "    batch[\"input_features\"] = feature_extractor(batch[\"audio\"][\"array\"], sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # Encode target transcriptions\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the dataset\n",
    "commonvoice_dataset = commonvoice_dataset.map(extract_features_and_encode, num_proc=1)\n",
    "\n",
    "print(\"✅ Feature extraction and tokenization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc3f62-101c-411a-a94d-ddef0bf34664",
   "metadata": {},
   "source": [
    "{\n",
    "    'audio': {\n",
    "        'array': numpy_array, \n",
    "        'sampling_rate': 16000\n",
    "    },\n",
    "    'sentence': 'This is the transcription of the audio.',\n",
    "    'input_features': tensor_of_log_mel_spectrogram,\n",
    "    'labels': [tokenized_ids]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c6f02-ce9e-494c-88ae-8dae94a6b512",
   "metadata": {},
   "source": [
    "Current dataset looks like above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0934a-4fac-4938-ac81-dd10bde77ff2",
   "metadata": {},
   "source": [
    "This code prints the structure of the preprocessed dataset after feature extraction and tokenization, providing an overview of the dataset format. It also displays a sample data entry from the training set, showing the extracted audio features and corresponding tokenized labels. This helps verify that the preprocessing steps were correctly applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aa30576f-3d5e-4fe8-8379-0abe891282e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
      "        num_rows: 7563\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
      "        num_rows: 3337\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print dataset format after feature extraction\n",
    "print(commonvoice_dataset)\n",
    "\n",
    "# Print a sample entry from the dataset (first example from train set)\n",
    "print(commonvoice_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85264e98-b75c-438c-8318-29f2e6304792",
   "metadata": {},
   "source": [
    "## Change it to\n",
    "Dataset({\n",
    "    features: ['input_features', 'labels'],\n",
    "    num_rows: 6760\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbffcc7-2f30-4928-9c06-8e819451eb96",
   "metadata": {},
   "source": [
    "This snippet removes the original raw audio and transcription text columns from the dataset, keeping only the processed features and labels needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "15310a02-f41d-4b1f-afc9-a2c62ed1a62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 7563\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 3337\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "commonvoice_dataset = commonvoice_dataset.remove_columns([\"audio\", \"sentence\"])\n",
    "#commonvoice_dataset = commonvoice_dataset.remove_columns([\"transcription\"])\n",
    "\n",
    "# Print dataset structure\n",
    "print(commonvoice_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8199fce3-4171-4039-be9b-1ee7891a08d6",
   "metadata": {},
   "source": [
    "### input_features → Tensor of shape (batch_size, 80, time_steps)\n",
    "\n",
    "80 is the number of Mel frequency bins (Whisper feature size)\n",
    "\n",
    "time_steps varies based on the longest audio in the batch (others are padded)\n",
    "\n",
    "### labels → Tensor of shape (batch_size, label_length)\n",
    "\n",
    "The tokenized transcription sequences\n",
    "\n",
    "Padded to the longest sequence in the batch\n",
    "\n",
    "Padding tokens replaced with -100 (ignored during loss computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f8ccc-6e62-4dd1-99d7-4e124d4771ed",
   "metadata": {},
   "source": [
    "This class defines a custom data collator used during training to batch and pad variable-length audio input features and their corresponding tokenized labels. It ensures the audio features are padded correctly using the processor’s feature extractor, while the label sequences are padded separately with the tokenizer’s padding method. Padding tokens in labels are replaced with -100 to be ignored during loss computation. Additionally, if a beginning-of-sequence token is present at the start of labels, it is removed to avoid duplication during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e941d988-b94b-452c-b7be-41df5c2dba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9cae3-5bd8-46e0-9ce9-8b558e537164",
   "metadata": {},
   "source": [
    "This line initializes the custom data collator by passing the processor (which includes the feature extractor and tokenizer). The data_collator will be used during training to dynamically pad batches of audio features and labels, ensuring consistent input shapes and correct handling of padding tokens for the loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8e355d88-d0ac-4f5b-9c1a-006d13662095",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056ef6-1a68-4c3f-b52b-28abe2a50ff2",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "74e6afe8-797e-44a7-9873-09165aaf1954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\WORKSTATIONS\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--wer\\85bee9e4216a78bb09b2d0d500f6af5c23da58f9210e661add540f5df6630fcd (last modified on Wed Mar 19 12:07:22 2025) since it couldn't be found locally at evaluate-metric--wer, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa1e63-dcc6-4886-8065-020c0f394976",
   "metadata": {},
   "source": [
    "## Post-processing on the model\n",
    "\n",
    "To reduce our models memory footprint, we load the model in 8bit, this means we quantize the model to use 1/4th precision (when comapared to float32) with minimal loss to performance. Finally, we need to apply some post-processing steps on the 8-bit model to enable training. We do so by first freezing all the model layers, and then cast the layer-norm and the output layer in float32 for training and model stability. Since the Whisper model uses Convolutional layers in the Encoder, checkpointing disables grad computation to avoid this we specifically need to make the inputs trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54afd76b-3823-42ba-80f0-3f209b5e6ce7",
   "metadata": {},
   "source": [
    "This code loads the pretrained Whisper model (whisper-large) from Hugging Face and moves it to the GPU (cuda) for faster computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d08eeab0-77f1-4678-86c9-4133629170f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\").to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "06167b96-5069-4b03-881b-be65b93c1fe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.4-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.4.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.4-py3-none-win_amd64.whl (75.4 MB)\n",
      "   ---------------------------------------- 0.0/75.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.0/75.4 MB 6.3 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 3.4/75.4 MB 9.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 6.0/75.4 MB 10.0 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 8.4/75.4 MB 10.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 10.5/75.4 MB 10.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 11.8/75.4 MB 9.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 13.4/75.4 MB 9.0 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 14.7/75.4 MB 8.8 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 16.3/75.4 MB 8.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 18.1/75.4 MB 8.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 19.4/75.4 MB 8.2 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 21.0/75.4 MB 8.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 22.8/75.4 MB 8.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 24.9/75.4 MB 8.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 26.7/75.4 MB 8.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 28.8/75.4 MB 8.3 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 30.9/75.4 MB 8.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 33.0/75.4 MB 8.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 35.4/75.4 MB 8.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 37.7/75.4 MB 8.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 40.1/75.4 MB 8.8 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 42.5/75.4 MB 8.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 44.8/75.4 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 46.9/75.4 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 48.8/75.4 MB 9.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 50.6/75.4 MB 9.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 52.7/75.4 MB 9.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 54.3/75.4 MB 8.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 55.8/75.4 MB 8.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 57.4/75.4 MB 8.8 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 59.2/75.4 MB 8.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 60.8/75.4 MB 8.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 62.1/75.4 MB 8.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 63.4/75.4 MB 8.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 65.0/75.4 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 66.3/75.4 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 67.9/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 69.5/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 71.3/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 72.9/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  74.7/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  75.2/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 75.4/75.4 MB 8.2 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "773db515-254c-4399-b659-66c8e3c6e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\workstations\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (2.4.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (4.46.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (1.0.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (0.26.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers->peft) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7e788f4d-3640-4f89-b30c-9b876a0eb189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (2.4.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (0.26.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate>=0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"accelerate>=0.26.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f3f9c571-72c9-47a9-8c36-5b345a7c0cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5aeca0-127c-4fc9-92cb-7808b97d5564",
   "metadata": {},
   "source": [
    "This section demonstrates loading the Whisper model with 4-bit quantization using the bitsandbytes library to significantly reduce GPU memory usage during training or inference. The BitsAndBytesConfig configures the model to use 4-bit precision with optimized compute and quantization settings. After loading the quantized model, prepare_model_for_kbit_training from the peft library prepares it for efficient fine-tuning with low-bit precision, enabling faster experimentation on limited hardware resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bc798051-eeb2-4348-969b-5111b599d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Instead of 8-bit\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Ensure compatibility\n",
    "    bnb_4bit_use_double_quant=True  # Optional: Helps reduce memory usage\n",
    ")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"openai/whisper-large\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726c6ef-0f17-42fb-b64f-3370b8ff4249",
   "metadata": {},
   "source": [
    "This snippet registers a forward hook on the model’s first convolutional layer (conv1 in the encoder) to ensure that the output tensor requires gradients. This is useful when you want to enable gradient computation for specific intermediate outputs during backpropagation, which can be important for certain training or fine-tuning strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b5c5e264-f3a5-4084-9046-b7321827f280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x1bf2c814820>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_inputs_require_grad(module, input, output):\n",
    "    output.requires_grad_(True)\n",
    "\n",
    "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b42f6-1fd7-49e9-b6e2-232947b7ffaf",
   "metadata": {},
   "source": [
    "## Apply Low-rank adapters (LoRA) to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5eccdc89-82c6-4217-97b1-8ff4131d2b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\workstations\\anaconda3\\lib\\site-packages (0.45.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\workstations\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: peft in c:\\users\\workstations\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\workstations\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.4.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade bitsandbytes transformers peft accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba7c22-9cd2-47ca-af45-a0a766a108d1",
   "metadata": {},
   "source": [
    "This code applies LoRA (Low-Rank Adaptation) to the Whisper model for parameter-efficient fine-tuning. The LoraConfig sets the LoRA hyperparameters such as rank (r), scaling factor (lora_alpha), target modules (q_proj and v_proj layers), dropout, and bias handling. The get_peft_model function wraps the original model with LoRA adapters, enabling efficient fine-tuning by updating only a small subset of parameters. Finally, print_trainable_parameters() displays which model parameters will be updated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f09fc-f96f-4d98-8a2b-4f417378d3b6",
   "metadata": {},
   "source": [
    "This wraps your existing pretrained model (e.g., Whisper) with LoRA adapters and freezes the original model’s parameters.\n",
    "\n",
    "✅ Only the small number of new parameters in LoRA layers are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "616c4aaf-44be-405f-949b-1f82386ec62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,728,640 || all params: 1,559,033,600 || trainable%: 1.0089\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52a85b-a180-47d6-b823-ce327dfcd90d",
   "metadata": {},
   "source": [
    "We are ONLY using 1% of the total trainable parameters, thereby performing Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec1e5e-8091-475f-8ca3-329343f96c93",
   "metadata": {},
   "source": [
    "## Define the Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d482a29f-3a3c-4d54-b73e-906f6781da1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: h5py 3.11.0\n",
      "Uninstalling h5py-3.11.0:\n",
      "  Successfully uninstalled h5py-3.11.0\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.11.0-cp38-cp38-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from h5py) (1.24.4)\n",
      "Downloading h5py-3.11.0-cp38-cp38-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.8/3.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 11.6 MB/s eta 0:00:00\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-3.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.16.1 requires flatbuffers>=23.5.26, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires keras>=3.0.0, but you have keras 2.9.0 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires tensorboard<2.17,>=2.16, but you have tensorboard 2.9.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall h5py -y\n",
    "!pip install --no-cache-dir h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e9a2d86b-47e9-4f7e-a35d-3da22265f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=r\"C:\\Users\\WORKSTATIONS\\Desktop\\BijoyashreeDas\\WHISPER\",  \n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  \n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=20,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=100,\n",
    "    #max_steps=100,  # only for testing purposes, remove this in final run\n",
    "    remove_unused_columns=False,  \n",
    "    label_names=[\"labels\"],  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4d4b7-6513-4bb9-bf6c-4c748c0e09e5",
   "metadata": {},
   "source": [
    "## Train the model->save the adapter weights and trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b392-b48c-4655-a54e-5abdabbd761f",
   "metadata": {},
   "source": [
    "Ensures that only LoRA adapter weights (not the full model) are saved — which makes checkpointing lightweight and storage-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b78dbe9f-0f1e-4611-90a8-dfd9ba55f64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WORKSTATIONS\\AppData\\Local\\Temp\\ipykernel_31528\\4289590012.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import Seq2SeqTrainer, TrainerCallback, TrainerState, TrainerControl, TrainingArguments\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "# Define save path\n",
    "save_path = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER\"\n",
    "adapter_path = os.path.join(save_path, \"lora_adapter\")\n",
    "\n",
    "# Ensure save directories exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "\n",
    "# Callback to save only LoRA adapter weights\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=commonvoice_dataset[\"train\"],\n",
    "    eval_dataset=commonvoice_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "\n",
    "# Disable caching for training\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6e8e86f2-7023-4163-abfb-c44742cd7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18920' max='18920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18920/18920 60:34:42, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.413666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.400084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.305200</td>\n",
       "      <td>0.410437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.294800</td>\n",
       "      <td>0.402908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>0.380991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>0.382256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.394883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.287300</td>\n",
       "      <td>0.391826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.399232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.383539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.389126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.405511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.132300</td>\n",
       "      <td>3.828397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.332800</td>\n",
       "      <td>2.864469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.768500</td>\n",
       "      <td>2.640012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.587000</td>\n",
       "      <td>2.520831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.491100</td>\n",
       "      <td>2.440840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.426300</td>\n",
       "      <td>2.389422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.407000</td>\n",
       "      <td>2.329651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.341800</td>\n",
       "      <td>2.314102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.327100</td>\n",
       "      <td>2.244779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.305800</td>\n",
       "      <td>2.222084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.254900</td>\n",
       "      <td>2.225258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.241300</td>\n",
       "      <td>2.159856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.209200</td>\n",
       "      <td>2.154295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.194600</td>\n",
       "      <td>2.116607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.162200</td>\n",
       "      <td>2.083102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.128100</td>\n",
       "      <td>2.050066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.553500</td>\n",
       "      <td>2.142075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.211900</td>\n",
       "      <td>2.072388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.104000</td>\n",
       "      <td>2.031888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.082200</td>\n",
       "      <td>2.010835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.046200</td>\n",
       "      <td>2.009107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.063200</td>\n",
       "      <td>1.966156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.005000</td>\n",
       "      <td>1.964277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.003800</td>\n",
       "      <td>1.932343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.990600</td>\n",
       "      <td>1.924283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.951900</td>\n",
       "      <td>1.910097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.940600</td>\n",
       "      <td>1.906713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.924000</td>\n",
       "      <td>1.882261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.910000</td>\n",
       "      <td>1.879152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.891200</td>\n",
       "      <td>1.861621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.909100</td>\n",
       "      <td>1.846660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.868200</td>\n",
       "      <td>1.843445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.850500</td>\n",
       "      <td>1.828251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.862300</td>\n",
       "      <td>1.829620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.853400</td>\n",
       "      <td>1.826359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.818200</td>\n",
       "      <td>1.800678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.787000</td>\n",
       "      <td>1.785967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.806500</td>\n",
       "      <td>1.771944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.783000</td>\n",
       "      <td>1.770747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.804500</td>\n",
       "      <td>1.760938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.775300</td>\n",
       "      <td>1.744457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.750200</td>\n",
       "      <td>1.737582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.742200</td>\n",
       "      <td>1.732952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.756200</td>\n",
       "      <td>1.724103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.724500</td>\n",
       "      <td>1.720389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.668700</td>\n",
       "      <td>1.708004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.704100</td>\n",
       "      <td>1.715684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.702600</td>\n",
       "      <td>1.699327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.679800</td>\n",
       "      <td>1.689632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.690400</td>\n",
       "      <td>1.684985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.672700</td>\n",
       "      <td>1.668937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.669800</td>\n",
       "      <td>1.668064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.658600</td>\n",
       "      <td>1.658150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.653200</td>\n",
       "      <td>1.657522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.609900</td>\n",
       "      <td>1.666036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.600400</td>\n",
       "      <td>1.652284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.595700</td>\n",
       "      <td>1.642385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.613500</td>\n",
       "      <td>1.642588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.596900</td>\n",
       "      <td>1.635253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.591000</td>\n",
       "      <td>1.634106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.578700</td>\n",
       "      <td>1.621448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.586200</td>\n",
       "      <td>1.608907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.548100</td>\n",
       "      <td>1.610424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.566200</td>\n",
       "      <td>1.623532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.505700</td>\n",
       "      <td>1.607117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.509100</td>\n",
       "      <td>1.605825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.512500</td>\n",
       "      <td>1.600697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.512400</td>\n",
       "      <td>1.610315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.528100</td>\n",
       "      <td>1.593834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.527400</td>\n",
       "      <td>1.583287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.527400</td>\n",
       "      <td>1.577702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>1.577765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.515300</td>\n",
       "      <td>1.564718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>1.582693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.459000</td>\n",
       "      <td>1.570881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.435800</td>\n",
       "      <td>1.568912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.443600</td>\n",
       "      <td>1.571170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.445700</td>\n",
       "      <td>1.559973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.459200</td>\n",
       "      <td>1.552618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.429000</td>\n",
       "      <td>1.545335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>1.548806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.439900</td>\n",
       "      <td>1.553651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.424100</td>\n",
       "      <td>1.536461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.374900</td>\n",
       "      <td>1.547011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.384400</td>\n",
       "      <td>1.547108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.412700</td>\n",
       "      <td>1.552262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.374100</td>\n",
       "      <td>1.548519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.368400</td>\n",
       "      <td>1.536998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.382800</td>\n",
       "      <td>1.527838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.377300</td>\n",
       "      <td>1.525919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.385300</td>\n",
       "      <td>1.517491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.350700</td>\n",
       "      <td>1.524326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.301000</td>\n",
       "      <td>1.543730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.308900</td>\n",
       "      <td>1.533829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.311200</td>\n",
       "      <td>1.526930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.310600</td>\n",
       "      <td>1.525315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.313700</td>\n",
       "      <td>1.514763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.527521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.312800</td>\n",
       "      <td>1.512521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.318300</td>\n",
       "      <td>1.518190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.313300</td>\n",
       "      <td>1.510563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.259000</td>\n",
       "      <td>1.519751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.211100</td>\n",
       "      <td>1.517437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.230200</td>\n",
       "      <td>1.528409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.240800</td>\n",
       "      <td>1.522148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.244000</td>\n",
       "      <td>1.508881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.239700</td>\n",
       "      <td>1.509496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.238000</td>\n",
       "      <td>1.513252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.255400</td>\n",
       "      <td>1.505149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.245100</td>\n",
       "      <td>1.493179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.261300</td>\n",
       "      <td>1.505209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.144400</td>\n",
       "      <td>1.519773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.144400</td>\n",
       "      <td>1.531608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.171500</td>\n",
       "      <td>1.522689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.171800</td>\n",
       "      <td>1.519406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.179700</td>\n",
       "      <td>1.517529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>1.173200</td>\n",
       "      <td>1.510744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.187100</td>\n",
       "      <td>1.506919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>1.185900</td>\n",
       "      <td>1.516325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>1.173500</td>\n",
       "      <td>1.509533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>1.131800</td>\n",
       "      <td>1.538103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.091500</td>\n",
       "      <td>1.525568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.094900</td>\n",
       "      <td>1.523761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.106200</td>\n",
       "      <td>1.537251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>1.097600</td>\n",
       "      <td>1.529973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.089200</td>\n",
       "      <td>1.529247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>1.091300</td>\n",
       "      <td>1.519475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.115000</td>\n",
       "      <td>1.523442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>1.119000</td>\n",
       "      <td>1.517147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>1.106800</td>\n",
       "      <td>1.528661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>1.001700</td>\n",
       "      <td>1.547639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.024800</td>\n",
       "      <td>1.550367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.038400</td>\n",
       "      <td>1.536889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.028100</td>\n",
       "      <td>1.547528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>1.039100</td>\n",
       "      <td>1.541511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>1.043800</td>\n",
       "      <td>1.546034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>1.031700</td>\n",
       "      <td>1.529296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.045900</td>\n",
       "      <td>1.536874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>1.048500</td>\n",
       "      <td>1.530037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.973400</td>\n",
       "      <td>1.556280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.950300</td>\n",
       "      <td>1.568114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>1.560220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.975900</td>\n",
       "      <td>1.565502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.976500</td>\n",
       "      <td>1.563170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.973700</td>\n",
       "      <td>1.551646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.979200</td>\n",
       "      <td>1.557028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.969800</td>\n",
       "      <td>1.565707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.974500</td>\n",
       "      <td>1.560911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.958300</td>\n",
       "      <td>1.573445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.895400</td>\n",
       "      <td>1.584391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>1.588266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>1.587873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.895800</td>\n",
       "      <td>1.589885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>1.584645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>1.581406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>1.584318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.921200</td>\n",
       "      <td>1.582516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.913300</td>\n",
       "      <td>1.587363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.852200</td>\n",
       "      <td>1.606435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>1.614078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.847700</td>\n",
       "      <td>1.613985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>1.613307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.850500</td>\n",
       "      <td>1.607229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>1.612014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>1.610930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.838300</td>\n",
       "      <td>1.615876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.860200</td>\n",
       "      <td>1.616563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>1.620405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.781300</td>\n",
       "      <td>1.628663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>1.632987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>1.634380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.814600</td>\n",
       "      <td>1.629748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>1.631332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.805200</td>\n",
       "      <td>1.631569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.800400</td>\n",
       "      <td>1.631744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>1.633926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.813400</td>\n",
       "      <td>1.633332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BCD92B2670>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 4a9c9cce-42e0-46c8-9fae-e7151045b922)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1120B550>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: bf2cca63-d342-4536-9694-45c20d121e37)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE111DA1F0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 48db4702-b535-43eb-8209-41397337026d)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1079B370>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 7fb4bb30-5b11-4f6a-8ddd-cadf85c850ed)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10E32FA0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 27be7f6f-d157-4691-8a73-bed5da4e155c)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE11012640>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d3b89d62-abe2-4858-bb26-c69f933e6d24)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10B0DCD0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: deae0198-36ed-4018-930f-2706a9434716)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1120BB80>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 53750809-44d7-4255-ba34-f1889e58c1ca)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1104D640>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 3e4adbfd-b370-4379-8589-571f24e470e3)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE107C5A60>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 33d14616-302c-473f-8e4b-07c4bc07a7bc)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10B0DB80>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 9f957f8e-8ab5-4cfb-b706-fd455e89f2e4)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE11207D00>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 98157f2a-6e76-47c1-b80f-627b47d4e87c)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10E325E0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 689ed82c-ce09-4cd7-87c1-2256742fae7b)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE11027370>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 9c5a46b4-ec63-4e9c-bf16-36818564701c)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10B0D1F0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: b67a738f-b391-4786-aba3-532bc9c5be0a)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1104D280>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 26fe32c5-eeda-45a8-a466-123503415f40)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1120F340>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: a63d256d-cd10-43dd-a922-03a7d5f9b300)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE107CF3A0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: fc15b03f-753a-4825-a6ad-c32002fdf66b)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1102F460>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 78c40eee-60ff-43c8-ae24-8a0a5c37f776)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10B0DE80>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: a7c220f7-e438-4dbd-81b4-7a7f93191762)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE5340>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d7e28449-d985-49da-ba20-6cb20abc0106)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1102FD60>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 5f788d26-2ed7-4999-8edb-d8badd96a96f)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1120DA60>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 4f2f82cd-86a3-45db-b958-f0e10c404359)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE11207400>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: eb624dc1-77b9-4199-9ec4-6014ee25a4f6)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE53A0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: a5d2adaf-ab39-4286-b853-0bbe92d6af21)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1102BD90>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: b96723eb-c292-48dc-91c6-a47c9c4c81fa)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE107A4550>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 6b912a45-53ac-4f3f-a5c6-e7196dc011d3)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1102F7C0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 3819df56-69c2-4096-a017-61c942b34c0d)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AEAA60>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 2a7506b8-0f61-4ccc-8cb9-ae9e47da7f4f)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE6E80>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: f73ef73c-ae26-4eec-932f-28a71b7107c9)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE11020190>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: cb20187d-35da-4746-95fa-b2a8acd06b76)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE107A4C10>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 2178fa3b-3487-480f-9868-c53be325eea9)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE58B0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 4a20d423-9291-4460-a4d4-f1890d6ec26f)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE107A4EB0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 96f112d2-2561-4c89-8618-8f3f28dace8f)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1120D2B0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 20cdb314-dbdf-4233-b012-6be573894c2d)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE107A1B50>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 3efceb74-5f38-4df7-88b8-a24c4fa5935d)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE5DC0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: b1ac0f8d-da86-4c87-ae29-b4acbd6095a0)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1120D040>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: cc1c30b4-17d1-4593-b39d-69618c23cd3b)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE110209A0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 0d7a59e0-904e-4adf-a23b-1a5d1b02d5a4)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1120F3A0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: a899fa2d-4cb9-4f41-b734-982606068387)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AEAB80>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 70e5adc9-60fe-4bc0-9501-cae4d7390122)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BCD7591A00>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: cdbe659d-528a-489b-9a8b-0195be751050)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D966D0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: c08a1088-0efd-4555-a384-35aa3d3a9048)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE54C0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 262e9f4f-4749-4668-a78e-9a7d614d0bda)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AEACD0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: aefd779c-cf0d-4860-a1f3-5ff183082d5b)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE53D0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 8d767dba-4fb2-437a-8aec-d8be1eef0f08)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D96040>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 31f04588-9ae3-4fd4-8dff-82b199146be0)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AEA850>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 7ca5d8bb-b40b-4758-a534-9929e9f8a3a6)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE86A0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: f0e1c0d0-0c3b-4fc8-a57b-e6d974ceac3d)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE8370>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: de8fcdf9-c625-4233-abf5-1da1993d41d1)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D96F40>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: f5b5dbe4-3e51-4a63-9c98-e724a0b5afbd)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10AE8190>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 1827c3db-0fc3-4813-8ef5-5132dd5ab118)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D96790>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d10c55ba-04dc-4a67-a84c-aa22206095c0)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D96EB0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 196f5850-64bb-4ff7-a39d-a2fd39c5a778)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BF854AD580>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: de65116e-a82f-48f5-bae0-3a652d13efed)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE1102EB50>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: cfba2472-d526-49e8-bde8-89ccb597b13c)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D902E0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: e57a66e6-acad-45d2-8721-8bcc4543e1f6)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D90E50>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: ad62fe95-bbfc-4ae1-b3dc-c2b241f3b453)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BF854ADA60>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 67a4d12d-f1cc-4f99-bad1-a33e16d4a272)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D907F0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 7627cab5-34dc-43a7-b6f6-5a759e9d0c4c)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10DCC4C0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 0ae70bee-1f35-401b-be6f-ba4129091878)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10DCC3A0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 1fa55370-6ee5-4fde-9080-24b84cbc96db)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BF854AD130>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 6de3325a-f1d6-4d9b-a0ff-dedd3e067557)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D28790>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: f3cd1799-1009-4c08-bd67-9a3fb3953852)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BF854ADCA0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 5b39a5ba-7341-44eb-9b3c-58f6237117e1)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10DCC550>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 81dedb29-5531-453e-8d8b-c4154c77df0f)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D285E0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: c4c8a92d-9cff-4d6f-8244-05cf90af8d66)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D28C70>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 5142f643-c417-4f42-b5c4-91bf33374d01)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BD27542520>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 6e35d5dd-233c-4bb4-88b0-e0d592193083)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BD2757CD90>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: b1405929-9fe7-4c5f-a16a-0119dd13a70c)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10B03610>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 49836413-ce24-4db8-ae96-f29d2a2db353)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10D96160>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 439c929c-7d1c-4400-a0a3-fd8a78d65374)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10DC9B50>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: bcd9a6df-b383-4ab8-8596-50922412b858)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10B03040>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 5ed2bf99-a537-413f-9a04-d56edd63454a)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10DC9E80>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 69b13934-294c-4625-9aae-3aba08fc5066)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10DC98B0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 5a6f4038-5956-4470-83c3-384eac5f1dab)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in openai/whisper-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18920, training_loss=1.3686554358827134, metrics={'train_runtime': 218087.6325, 'train_samples_per_second': 0.694, 'train_steps_per_second': 0.087, 'total_flos': 3.24576772890624e+20, 'train_loss': 1.3686554358827134, 'epoch': 20.0})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b9d53308-ccb4-44a9-9766-08e1fc69ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA adapter saved separately at: C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER\\lora_adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BE10DC9130>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: c8793420-e90d-428f-a3ad-3559e7c26579)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save the full fine-tuned model (Whisper + LoRA)\n",
    "#model.save_pretrained(save_path)\n",
    "#processor.save_pretrained(save_path)\n",
    "from peft import PeftModel\n",
    "\n",
    "# Save LoRA adapter separately\n",
    "model.save_pretrained(adapter_path)\n",
    "\n",
    "\n",
    "#print(f\"✅ Model and weights saved at: {save_path}\")\n",
    "print(f\"✅ LoRA adapter saved separately at: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e6976-3632-4f2e-af15-95411bf5b7d3",
   "metadata": {},
   "source": [
    "## Print steps-train loss-test loss (Cross-Entropy Loss (CE Loss))\n",
    "\n",
    "If you have 1,000 training samples and use batch_size=8, then you'll have:\n",
    "\n",
    "1000/8=125 steps per epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d93f6c8d-db58-42ba-bdf6-3eeca999a9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step\tTraining Loss\tValidation Loss\n",
      "100\t0.469100\tN/A\n",
      "100\tN/A\t0.413666\n",
      "200\t0.322000\tN/A\n",
      "200\tN/A\t0.400084\n",
      "300\t0.305200\tN/A\n",
      "300\tN/A\t0.410437\n",
      "400\t0.294800\tN/A\n",
      "400\tN/A\t0.402908\n",
      "500\t0.300800\tN/A\n",
      "500\tN/A\t0.380991\n",
      "600\t0.307500\tN/A\n",
      "600\tN/A\t0.382256\n",
      "700\t0.278000\tN/A\n",
      "700\tN/A\t0.394883\n",
      "800\t0.287300\tN/A\n",
      "800\tN/A\t0.391826\n",
      "900\t0.280300\tN/A\n",
      "900\tN/A\t0.399232\n",
      "1000\t0.261000\tN/A\n",
      "1000\tN/A\t0.383539\n",
      "1100\t0.239000\tN/A\n",
      "1100\tN/A\t0.389126\n",
      "1200\t0.242700\tN/A\n",
      "1200\tN/A\t0.405511\n",
      "1300\t3.132300\tN/A\n",
      "1300\tN/A\t3.828397\n",
      "1400\t3.332800\tN/A\n",
      "1400\tN/A\t2.864469\n",
      "1500\t2.768500\tN/A\n",
      "1500\tN/A\t2.640012\n",
      "1600\t2.587000\tN/A\n",
      "1600\tN/A\t2.520831\n",
      "1700\t2.491100\tN/A\n",
      "1700\tN/A\t2.440840\n",
      "1800\t2.426300\tN/A\n",
      "1800\tN/A\t2.389422\n",
      "1900\t2.407000\tN/A\n",
      "1900\tN/A\t2.329651\n",
      "2000\t2.341800\tN/A\n",
      "2000\tN/A\t2.314102\n",
      "2100\t2.327100\tN/A\n",
      "2100\tN/A\t2.244779\n",
      "2200\t2.305800\tN/A\n",
      "2200\tN/A\t2.222084\n",
      "2300\t2.254900\tN/A\n",
      "2300\tN/A\t2.225258\n",
      "2400\t2.241300\tN/A\n",
      "2400\tN/A\t2.159856\n",
      "2500\t2.209200\tN/A\n",
      "2500\tN/A\t2.154295\n",
      "2600\t2.194600\tN/A\n",
      "2600\tN/A\t2.116607\n",
      "2700\t2.162200\tN/A\n",
      "2700\tN/A\t2.083102\n",
      "2800\t2.128100\tN/A\n",
      "2800\tN/A\t2.050066\n",
      "2900\t2.553500\tN/A\n",
      "2900\tN/A\t2.142075\n",
      "3000\t2.211900\tN/A\n",
      "3000\tN/A\t2.072388\n",
      "3100\t2.104000\tN/A\n",
      "3100\tN/A\t2.031888\n",
      "3200\t2.082200\tN/A\n",
      "3200\tN/A\t2.010835\n",
      "3300\t2.046200\tN/A\n",
      "3300\tN/A\t2.009107\n",
      "3400\t2.063200\tN/A\n",
      "3400\tN/A\t1.966156\n",
      "3500\t2.005000\tN/A\n",
      "3500\tN/A\t1.964277\n",
      "3600\t2.003800\tN/A\n",
      "3600\tN/A\t1.932343\n",
      "3700\t1.990600\tN/A\n",
      "3700\tN/A\t1.924283\n",
      "3800\t1.951900\tN/A\n",
      "3800\tN/A\t1.910097\n",
      "3900\t1.940600\tN/A\n",
      "3900\tN/A\t1.906713\n",
      "4000\t1.924000\tN/A\n",
      "4000\tN/A\t1.882261\n",
      "4100\t1.910000\tN/A\n",
      "4100\tN/A\t1.879152\n",
      "4200\t1.891200\tN/A\n",
      "4200\tN/A\t1.861621\n",
      "4300\t1.909100\tN/A\n",
      "4300\tN/A\t1.846660\n",
      "4400\t1.868200\tN/A\n",
      "4400\tN/A\t1.843445\n",
      "4500\t1.850500\tN/A\n",
      "4500\tN/A\t1.828251\n",
      "4600\t1.862300\tN/A\n",
      "4600\tN/A\t1.829620\n",
      "4700\t1.853400\tN/A\n",
      "4700\tN/A\t1.826359\n",
      "4800\t1.818200\tN/A\n",
      "4800\tN/A\t1.800678\n",
      "4900\t1.787000\tN/A\n",
      "4900\tN/A\t1.785967\n",
      "5000\t1.806500\tN/A\n",
      "5000\tN/A\t1.771944\n",
      "5100\t1.783000\tN/A\n",
      "5100\tN/A\t1.770747\n",
      "5200\t1.804500\tN/A\n",
      "5200\tN/A\t1.760938\n",
      "5300\t1.775300\tN/A\n",
      "5300\tN/A\t1.744457\n",
      "5400\t1.750200\tN/A\n",
      "5400\tN/A\t1.737582\n",
      "5500\t1.742200\tN/A\n",
      "5500\tN/A\t1.732952\n",
      "5600\t1.756200\tN/A\n",
      "5600\tN/A\t1.724103\n",
      "5700\t1.724500\tN/A\n",
      "5700\tN/A\t1.720389\n",
      "5800\t1.668700\tN/A\n",
      "5800\tN/A\t1.708004\n",
      "5900\t1.704100\tN/A\n",
      "5900\tN/A\t1.715684\n",
      "6000\t1.702600\tN/A\n",
      "6000\tN/A\t1.699327\n",
      "6100\t1.679800\tN/A\n",
      "6100\tN/A\t1.689632\n",
      "6200\t1.690400\tN/A\n",
      "6200\tN/A\t1.684985\n",
      "6300\t1.672700\tN/A\n",
      "6300\tN/A\t1.668937\n",
      "6400\t1.669800\tN/A\n",
      "6400\tN/A\t1.668064\n",
      "6500\t1.658600\tN/A\n",
      "6500\tN/A\t1.658150\n",
      "6600\t1.653200\tN/A\n",
      "6600\tN/A\t1.657522\n",
      "6700\t1.609900\tN/A\n",
      "6700\tN/A\t1.666036\n",
      "6800\t1.600400\tN/A\n",
      "6800\tN/A\t1.652284\n",
      "6900\t1.595700\tN/A\n",
      "6900\tN/A\t1.642385\n",
      "7000\t1.613500\tN/A\n",
      "7000\tN/A\t1.642588\n",
      "7100\t1.596900\tN/A\n",
      "7100\tN/A\t1.635253\n",
      "7200\t1.591000\tN/A\n",
      "7200\tN/A\t1.634106\n",
      "7300\t1.578700\tN/A\n",
      "7300\tN/A\t1.621448\n",
      "7400\t1.586200\tN/A\n",
      "7400\tN/A\t1.608907\n",
      "7500\t1.548100\tN/A\n",
      "7500\tN/A\t1.610424\n",
      "7600\t1.566200\tN/A\n",
      "7600\tN/A\t1.623532\n",
      "7700\t1.505700\tN/A\n",
      "7700\tN/A\t1.607117\n",
      "7800\t1.509100\tN/A\n",
      "7800\tN/A\t1.605825\n",
      "7900\t1.512500\tN/A\n",
      "7900\tN/A\t1.600697\n",
      "8000\t1.512400\tN/A\n",
      "8000\tN/A\t1.610315\n",
      "8100\t1.528100\tN/A\n",
      "8100\tN/A\t1.593834\n",
      "8200\t1.527400\tN/A\n",
      "8200\tN/A\t1.583287\n",
      "8300\t1.527400\tN/A\n",
      "8300\tN/A\t1.577702\n",
      "8400\t1.509300\tN/A\n",
      "8400\tN/A\t1.577765\n",
      "8500\t1.515300\tN/A\n",
      "8500\tN/A\t1.564718\n",
      "8600\t1.431000\tN/A\n",
      "8600\tN/A\t1.582693\n",
      "8700\t1.459000\tN/A\n",
      "8700\tN/A\t1.570881\n",
      "8800\t1.435800\tN/A\n",
      "8800\tN/A\t1.568912\n",
      "8900\t1.443600\tN/A\n",
      "8900\tN/A\t1.571170\n",
      "9000\t1.445700\tN/A\n",
      "9000\tN/A\t1.559973\n",
      "9100\t1.459200\tN/A\n",
      "9100\tN/A\t1.552618\n",
      "9200\t1.429000\tN/A\n",
      "9200\tN/A\t1.545335\n",
      "9300\t1.460000\tN/A\n",
      "9300\tN/A\t1.548806\n",
      "9400\t1.439900\tN/A\n",
      "9400\tN/A\t1.553651\n",
      "9500\t1.424100\tN/A\n",
      "9500\tN/A\t1.536461\n",
      "9600\t1.374900\tN/A\n",
      "9600\tN/A\t1.547011\n",
      "9700\t1.384400\tN/A\n",
      "9700\tN/A\t1.547108\n",
      "9800\t1.412700\tN/A\n",
      "9800\tN/A\t1.552262\n",
      "9900\t1.374100\tN/A\n",
      "9900\tN/A\t1.548519\n",
      "10000\t1.368400\tN/A\n",
      "10000\tN/A\t1.536998\n",
      "10100\t1.382800\tN/A\n",
      "10100\tN/A\t1.527838\n",
      "10200\t1.377300\tN/A\n",
      "10200\tN/A\t1.525919\n",
      "10300\t1.385300\tN/A\n",
      "10300\tN/A\t1.517491\n",
      "10400\t1.350700\tN/A\n",
      "10400\tN/A\t1.524326\n",
      "10500\t1.301000\tN/A\n",
      "10500\tN/A\t1.543730\n",
      "10600\t1.308900\tN/A\n",
      "10600\tN/A\t1.533829\n",
      "10700\t1.311200\tN/A\n",
      "10700\tN/A\t1.526930\n",
      "10800\t1.310600\tN/A\n",
      "10800\tN/A\t1.525315\n",
      "10900\t1.313700\tN/A\n",
      "10900\tN/A\t1.514763\n",
      "11000\t1.315000\tN/A\n",
      "11000\tN/A\t1.527521\n",
      "11100\t1.312800\tN/A\n",
      "11100\tN/A\t1.512521\n",
      "11200\t1.318300\tN/A\n",
      "11200\tN/A\t1.518190\n",
      "11300\t1.313300\tN/A\n",
      "11300\tN/A\t1.510563\n",
      "11400\t1.259000\tN/A\n",
      "11400\tN/A\t1.519751\n",
      "11500\t1.211100\tN/A\n",
      "11500\tN/A\t1.517437\n",
      "11600\t1.230200\tN/A\n",
      "11600\tN/A\t1.528409\n",
      "11700\t1.240800\tN/A\n",
      "11700\tN/A\t1.522148\n",
      "11800\t1.244000\tN/A\n",
      "11800\tN/A\t1.508881\n",
      "11900\t1.239700\tN/A\n",
      "11900\tN/A\t1.509496\n",
      "12000\t1.238000\tN/A\n",
      "12000\tN/A\t1.513252\n",
      "12100\t1.255400\tN/A\n",
      "12100\tN/A\t1.505149\n",
      "12200\t1.245100\tN/A\n",
      "12200\tN/A\t1.493179\n",
      "12300\t1.261300\tN/A\n",
      "12300\tN/A\t1.505209\n",
      "12400\t1.144400\tN/A\n",
      "12400\tN/A\t1.519773\n",
      "12500\t1.144400\tN/A\n",
      "12500\tN/A\t1.531608\n",
      "12600\t1.171500\tN/A\n",
      "12600\tN/A\t1.522689\n",
      "12700\t1.171800\tN/A\n",
      "12700\tN/A\t1.519406\n",
      "12800\t1.179700\tN/A\n",
      "12800\tN/A\t1.517529\n",
      "12900\t1.173200\tN/A\n",
      "12900\tN/A\t1.510744\n",
      "13000\t1.187100\tN/A\n",
      "13000\tN/A\t1.506919\n",
      "13100\t1.185900\tN/A\n",
      "13100\tN/A\t1.516325\n",
      "13200\t1.173500\tN/A\n",
      "13200\tN/A\t1.509533\n",
      "13300\t1.131800\tN/A\n",
      "13300\tN/A\t1.538103\n",
      "13400\t1.091500\tN/A\n",
      "13400\tN/A\t1.525568\n",
      "13500\t1.094900\tN/A\n",
      "13500\tN/A\t1.523761\n",
      "13600\t1.106200\tN/A\n",
      "13600\tN/A\t1.537251\n",
      "13700\t1.097600\tN/A\n",
      "13700\tN/A\t1.529973\n",
      "13800\t1.089200\tN/A\n",
      "13800\tN/A\t1.529247\n",
      "13900\t1.091300\tN/A\n",
      "13900\tN/A\t1.519475\n",
      "14000\t1.115000\tN/A\n",
      "14000\tN/A\t1.523442\n",
      "14100\t1.119000\tN/A\n",
      "14100\tN/A\t1.517147\n",
      "14200\t1.106800\tN/A\n",
      "14200\tN/A\t1.528661\n",
      "14300\t1.001700\tN/A\n",
      "14300\tN/A\t1.547639\n",
      "14400\t1.024800\tN/A\n",
      "14400\tN/A\t1.550367\n",
      "14500\t1.038400\tN/A\n",
      "14500\tN/A\t1.536889\n",
      "14600\t1.028100\tN/A\n",
      "14600\tN/A\t1.547528\n",
      "14700\t1.039100\tN/A\n",
      "14700\tN/A\t1.541511\n",
      "14800\t1.043800\tN/A\n",
      "14800\tN/A\t1.546034\n",
      "14900\t1.031700\tN/A\n",
      "14900\tN/A\t1.529296\n",
      "15000\t1.045900\tN/A\n",
      "15000\tN/A\t1.536874\n",
      "15100\t1.048500\tN/A\n",
      "15100\tN/A\t1.530037\n",
      "15200\t0.973400\tN/A\n",
      "15200\tN/A\t1.556280\n",
      "15300\t0.950300\tN/A\n",
      "15300\tN/A\t1.568114\n",
      "15400\t0.959000\tN/A\n",
      "15400\tN/A\t1.560220\n",
      "15500\t0.975900\tN/A\n",
      "15500\tN/A\t1.565502\n",
      "15600\t0.976500\tN/A\n",
      "15600\tN/A\t1.563170\n",
      "15700\t0.973700\tN/A\n",
      "15700\tN/A\t1.551646\n",
      "15800\t0.979200\tN/A\n",
      "15800\tN/A\t1.557028\n",
      "15900\t0.969800\tN/A\n",
      "15900\tN/A\t1.565707\n",
      "16000\t0.974500\tN/A\n",
      "16000\tN/A\t1.560911\n",
      "16100\t0.958300\tN/A\n",
      "16100\tN/A\t1.573445\n",
      "16200\t0.895400\tN/A\n",
      "16200\tN/A\t1.584391\n",
      "16300\t0.901400\tN/A\n",
      "16300\tN/A\t1.588266\n",
      "16400\t0.899900\tN/A\n",
      "16400\tN/A\t1.587873\n",
      "16500\t0.895800\tN/A\n",
      "16500\tN/A\t1.589885\n",
      "16600\t0.914300\tN/A\n",
      "16600\tN/A\t1.584645\n",
      "16700\t0.898500\tN/A\n",
      "16700\tN/A\t1.581406\n",
      "16800\t0.918000\tN/A\n",
      "16800\tN/A\t1.584318\n",
      "16900\t0.921200\tN/A\n",
      "16900\tN/A\t1.582516\n",
      "17000\t0.913300\tN/A\n",
      "17000\tN/A\t1.587363\n",
      "17100\t0.852200\tN/A\n",
      "17100\tN/A\t1.606435\n",
      "17200\t0.852600\tN/A\n",
      "17200\tN/A\t1.614078\n",
      "17300\t0.847700\tN/A\n",
      "17300\tN/A\t1.613985\n",
      "17400\t0.845800\tN/A\n",
      "17400\tN/A\t1.613307\n",
      "17500\t0.850500\tN/A\n",
      "17500\tN/A\t1.607229\n",
      "17600\t0.842300\tN/A\n",
      "17600\tN/A\t1.612014\n",
      "17700\t0.850900\tN/A\n",
      "17700\tN/A\t1.610930\n",
      "17800\t0.838300\tN/A\n",
      "17800\tN/A\t1.615876\n",
      "17900\t0.860200\tN/A\n",
      "17900\tN/A\t1.616563\n",
      "18000\t0.840400\tN/A\n",
      "18000\tN/A\t1.620405\n",
      "18100\t0.781300\tN/A\n",
      "18100\tN/A\t1.628663\n",
      "18200\t0.806300\tN/A\n",
      "18200\tN/A\t1.632987\n",
      "18300\t0.794300\tN/A\n",
      "18300\tN/A\t1.634380\n",
      "18400\t0.814600\tN/A\n",
      "18400\tN/A\t1.629748\n",
      "18500\t0.806300\tN/A\n",
      "18500\tN/A\t1.631332\n",
      "18600\t0.805200\tN/A\n",
      "18600\tN/A\t1.631569\n",
      "18700\t0.800400\tN/A\n",
      "18700\tN/A\t1.631744\n",
      "18800\t0.798600\tN/A\n",
      "18800\tN/A\t1.633926\n",
      "18900\t0.813400\tN/A\n",
      "18900\tN/A\t1.633332\n",
      "18920\tN/A\tN/A\n"
     ]
    }
   ],
   "source": [
    "print(\"Step\\tTraining Loss\\tValidation Loss\")\n",
    "for log in trainer.state.log_history:\n",
    "    step = log.get(\"step\", \"N/A\")\n",
    "    train_loss = log.get(\"loss\", None)  # Training loss\n",
    "    val_loss = log.get(\"eval_loss\", None)  # Validation loss\n",
    "\n",
    "    if step != \"N/A\":  # Only print if it's a valid step\n",
    "        train_loss_str = f\"{train_loss:.6f}\" if train_loss is not None else \"N/A\"\n",
    "        val_loss_str = f\"{val_loss:.6f}\" if val_loss is not None else \"N/A\"\n",
    "        print(f\"{step}\\t{train_loss_str}\\t{val_loss_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ec267-d6bf-47a4-a600-b795002e2fc5",
   "metadata": {},
   "source": [
    "## Evaluation and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778ebc2-f346-4ed7-a498-82128b394ce1",
   "metadata": {},
   "source": [
    " Loads the PEFT/LoRA configuration\n",
    "✅ Loads the base Whisper model in 8-bit mode for efficiency\n",
    "✅ Merges the fine-tuned LoRA weights with the base model\n",
    "✅ Enables caching for faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da41ad-a378-41af-a6a4-92914835bf5a",
   "metadata": {},
   "source": [
    "Loads the base Whisper model (openai/whisper-large) pretrained checkpoint.\n",
    "\n",
    "Loads a LoRA adapter (Low-Rank Adaptation), which is a lightweight fine-tuning method that adjusts only a small subset of model parameters. The adapter is applied on top of the base model.\n",
    "\n",
    "Enables caching for improved speed during generation.\n",
    "\n",
    "Saves the combined model (base + LoRA adapter) locally.\n",
    "\n",
    "Loads and saves the processor associated with the Whisper model, which includes audio feature extraction and tokenization logic needed for input/output processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1a5fb969-ed0e-41ff-bcc6-ca3341999840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and LoRA adapter loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# Define paths\n",
    "base_model_path = \"openai/whisper-large\"  # Change if needed\n",
    "adapter_path = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/lora_adapter\"\n",
    "\n",
    "# Load base Whisper model\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(base_model_path).to(\"cuda\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Enable cache for inference\n",
    "model.config.use_cache = True\n",
    "\n",
    "print(\"✅ Model and LoRA adapter loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee75f2-1468-486a-bbea-a854785066a3",
   "metadata": {},
   "source": [
    "## Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f45b2539-b562-44a8-90f2-d74754ec59cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/final_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BD2757C7F0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 288e7efa-d91b-4c36-b8ed-52ddd3d983c9)') - silently ignoring the lookup for the file config.json in openai/whisper-large.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_path = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/final_model\"\n",
    "\n",
    "# Save the full model with LoRA adapter\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model saved successfully at {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd827ae-cf1b-4077-8fb0-eae16aa245fc",
   "metadata": {},
   "source": [
    "## Compute WER on Train and Test Sets on saved model after finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4595bba2-7199-4283-9c03-23ec141cfb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Reload the processor from the base model and save it\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")  # Change to your base model\n",
    "processor.save_pretrained(\"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "645b3549-c450-47b1-9389-503d0876250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\WORKSTATIONS\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--wer\\85bee9e4216a78bb09b2d0d500f6af5c23da58f9210e661add540f5df6630fcd (last modified on Wed Mar 19 12:07:22 2025) since it couldn't be found locally at evaluate-metric--wer, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate  # ✅ Use `evaluate` instead of `datasets.load_metric`\n",
    "\n",
    "# Define paths\n",
    "model_path = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/final_model\"\n",
    "\n",
    "# Load the model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Set to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load WER metric\n",
    "metric = evaluate.load(\"wer\")  # ✅ Correct way to load the WER metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e16d868e-3d0a-453c-a3db-73c079aa7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wer(dataset, batch_size=8, max_new_tokens=255):\n",
    "    \"\"\"Generates transcriptions and computes WER for the given dataset.\"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    predictions, references = [], []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating WER...\"):\n",
    "        with torch.no_grad():\n",
    "            input_features = batch[\"input_features\"].to(\"cuda\")\n",
    "\n",
    "            # Generate transcription\n",
    "            generated_tokens = model.generate(input_features, max_new_tokens=max_new_tokens)\n",
    "\n",
    "            # Decode predictions and references\n",
    "            decoded_preds = processor.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "\n",
    "        # Free memory\n",
    "        del generated_tokens, batch\n",
    "        gc.collect()\n",
    "\n",
    "    # Compute WER\n",
    "    wer = 100 * metric.compute(predictions=predictions, references=references)\n",
    "    return wer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "be62c4db-67f7-4c0a-b10e-50eb32addc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WER...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 946/946 [2:27:43<00:00,  9.37s/it]\n",
      "Evaluating WER...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 418/418 [1:05:44<00:00,  9.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train WER: 103.68%\n",
      "Test WER: 104.47%\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'common_voice' is your dataset\n",
    "train_wer = compute_wer(commonvoice_dataset[\"train\"])\n",
    "test_wer = compute_wer(commonvoice_dataset[\"test\"])\n",
    "\n",
    "print(f\"Train WER: {train_wer:.2f}%\")\n",
    "print(f\"Test WER: {test_wer:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b89673-4252-4f94-9e39-472dc39f96c0",
   "metadata": {},
   "source": [
    "If your dataset has 7,568 samples, then:\n",
    "\n",
    "7568/8 = 946 batches\n",
    "\n",
    "This means your dataset has 946 mini-batches, and each iteration processes one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ebec6-2c80-43e4-b17c-6b74c6a0fd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12b853-9fb8-4c14-babc-39374640045d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
