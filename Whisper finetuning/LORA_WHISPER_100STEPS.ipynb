{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d711bc61-30aa-4c06-95f0-3a7fdb9edce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large\",language='hindi',task='transcribe')\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large\",language='hindi',task='transcribe')\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc822980-dab1-4789-9e43-770d5bf2540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\",language='hindi',task='transcribe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9e0f6-e6ef-4390-b67f-8489537f313e",
   "metadata": {},
   "source": [
    "## Dataset processing(train+dev,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33063ba-3dbc-4bb5-9c55-b51330038f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 7563\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 3337\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the dataset path\n",
    "DATASET_PATH = r\"C:\\Users\\WORKSTATIONS\\Desktop\\BijoyashreeDas\\COMMON_VOICE_HI\\cv-corpus-21.0-2025-03-14\\hi\"\n",
    "CLIPS_PATH = os.path.join(DATASET_PATH, \"clips\")\n",
    "\n",
    "# Load train+dev as train\n",
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, \"train.tsv\"), sep=\"\\t\")\n",
    "dev_df = pd.read_csv(os.path.join(DATASET_PATH, \"dev.tsv\"), sep=\"\\t\")\n",
    "train_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, \"test.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Function to get full audio path\n",
    "def get_audio_path(filename):\n",
    "    return os.path.join(CLIPS_PATH, filename)\n",
    "\n",
    "# Convert data to Hugging Face dataset format\n",
    "def convert_to_hf_dataset(df):\n",
    "    df = df[['path', 'sentence']].dropna()  # Keep only required columns\n",
    "    df['audio'] = df['path'].apply(get_audio_path)  # Convert paths\n",
    "    return Dataset.from_pandas(df[['audio', 'sentence']])  # Create HF dataset\n",
    "\n",
    "# Convert train and test to Hugging Face format\n",
    "commonvoice_train = convert_to_hf_dataset(train_df)\n",
    "commonvoice_test = convert_to_hf_dataset(test_df)\n",
    "\n",
    "# Define dataset dictionary\n",
    "commonvoice_dataset = DatasetDict({\n",
    "    \"train\": commonvoice_train,\n",
    "    \"test\": commonvoice_test\n",
    "})\n",
    "\n",
    "# Cast the audio column to Hugging Face Audio format\n",
    "commonvoice_dataset = commonvoice_dataset.cast_column(\"audio\", Audio())\n",
    "\n",
    "# Print dataset structure\n",
    "print(commonvoice_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc51c741-0878-42cb-821a-0acd43e75f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File: C:\\Users\\WORKSTATIONS\\Desktop\\BijoyashreeDas\\COMMON_VOICE_HI\\cv-corpus-21.0-2025-03-14\\hi\\clips\\common_voice_hi_26008353.mp3\n",
      "Transcription: ‡§π‡§Æ‡§®‡•á ‡§â‡§∏‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§Æ‡§®‡§æ‡§Ø‡§æ‡•§\n"
     ]
    }
   ],
   "source": [
    "# Get the first sample from the train set\n",
    "first_sample = commonvoice_dataset[\"train\"][0]\n",
    "\n",
    "# Print the audio filename and transcription\n",
    "print(\"Audio File:\", first_sample[\"audio\"][\"path\"])\n",
    "print(\"Transcription:\", first_sample[\"sentence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e2095-e8a1-4c3e-a9de-0ec39d46758f",
   "metadata": {},
   "source": [
    "## Total hours in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de454500-804e-4c2f-80cd-61de940f1b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating duration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7563/7563 [01:33<00:00, 80.48it/s]\n",
      "Calculating duration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3337/3337 [00:46<00:00, 71.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration of Train set: 9.38 hours\n",
      "Total duration of Test set: 4.73 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to calculate total duration of audio files\n",
    "def get_total_duration(dataset):\n",
    "    total_duration = 0.0  # In seconds\n",
    "    for sample in tqdm(dataset, desc=\"Calculating duration\"):\n",
    "        audio_path = sample[\"audio\"][\"path\"]\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)  # Load audio\n",
    "        total_duration += waveform.shape[1] / sample_rate  # Compute duration (seconds)\n",
    "    \n",
    "    return total_duration / 3600  # Convert seconds to hours\n",
    "\n",
    "# Compute total duration for train and test sets\n",
    "train_hours = get_total_duration(commonvoice_dataset[\"train\"])\n",
    "test_hours = get_total_duration(commonvoice_dataset[\"test\"])\n",
    "\n",
    "# Print results\n",
    "print(f\"Total duration of Train set: {train_hours:.2f} hours\")\n",
    "print(f\"Total duration of Test set: {test_hours:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "676b680a-2745-4f7c-a517-bd447ff7ce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating duration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10979/10979 [00:30<00:00, 363.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration of validated set: 14.18 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set dataset path\n",
    "DATASET_PATH = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/COMMON_VOICE_HI/cv-corpus-21.0-2025-03-14/hi\"\n",
    "\n",
    "# Load validated.tsv\n",
    "validated_df = pd.read_csv(os.path.join(DATASET_PATH, \"validated.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Get full path of each audio file\n",
    "validated_df[\"audio_path\"] = validated_df[\"path\"].apply(lambda x: os.path.join(DATASET_PATH, \"clips\", x))\n",
    "\n",
    "# Function to calculate total duration\n",
    "def get_total_duration(file_paths):\n",
    "    total_duration = 0.0  # In seconds\n",
    "    for audio_path in tqdm(file_paths, desc=\"Calculating duration\"):\n",
    "        if os.path.exists(audio_path):\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)  # Load audio\n",
    "            total_duration += waveform.shape[1] / sample_rate  # Compute duration (seconds)\n",
    "    \n",
    "    return total_duration / 3600  # Convert seconds to hours\n",
    "\n",
    "# Compute total duration\n",
    "validated_hours = get_total_duration(validated_df[\"audio_path\"])\n",
    "\n",
    "# Print results\n",
    "print(f\"Total duration of validated set: {validated_hours:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e7279-a726-49e6-96ef-459ad5060398",
   "metadata": {},
   "source": [
    "## Resample audio files to 16kHz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed998b68-61cf-4724-9e9d-ed0a768f9d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sampling Rate: 32000 Hz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a4eea85f59434fa5b6fe7abf816bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7860d694af4747ec8e47c38d8d2f7fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resampling complete. All audio is now at 16kHz.\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from datasets import Audio\n",
    "\n",
    "# Get the first audio sample in the train set\n",
    "sample = commonvoice_dataset[\"train\"][0][\"audio\"]\n",
    "\n",
    "# Print original sampling rate\n",
    "print(f\"Original Sampling Rate: {sample['sampling_rate']} Hz\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to resample audio to 16kHz\n",
    "def resample_audio(batch):\n",
    "    waveform = batch[\"audio\"][\"array\"]\n",
    "    orig_sr = batch[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "\n",
    "    # Resample if needed\n",
    "    if orig_sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_sr, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    return {\"audio\": {\"array\": waveform.numpy(), \"sampling_rate\": 16000}}  # Convert back to NumPy\n",
    "\n",
    "# Apply the resampling function to train and test sets\n",
    "commonvoice_dataset = commonvoice_dataset.map(resample_audio)\n",
    "\n",
    "print(\"‚úÖ Resampling complete. All audio is now at 16kHz.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ab74de-57fe-426b-b99c-f5c6fb66038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling rates of 3 random files from 'train' set:\n",
      "Sample 5003: 16000 Hz\n",
      "Sample 2440: 16000 Hz\n",
      "Sample 5144: 16000 Hz\n",
      "\n",
      "Sampling rates of 3 random files from 'test' set:\n",
      "Sample 1293: 16000 Hz\n",
      "Sample 450: 16000 Hz\n",
      "Sample 2008: 16000 Hz\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to print sampling rate of N random samples\n",
    "def print_random_sampling_rates(dataset, split, num_samples=3):\n",
    "    print(f\"\\nSampling rates of {num_samples} random files from '{split}' set:\")\n",
    "    \n",
    "    # Select random indices\n",
    "    random_indices = random.sample(range(len(dataset[split])), num_samples)\n",
    "    \n",
    "    # Fetch and print sampling rates\n",
    "    for idx in random_indices:\n",
    "        sample = dataset[split][idx][\"audio\"]\n",
    "        print(f\"Sample {idx}: {sample['sampling_rate']} Hz\")\n",
    "\n",
    "# Print sampling rates for train and test sets\n",
    "print_random_sampling_rates(commonvoice_dataset, \"train\")\n",
    "print_random_sampling_rates(commonvoice_dataset, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d985a0-1239-482c-9cb4-13500fc86584",
   "metadata": {},
   "source": [
    "## WER on train and test sets before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de565150-db6b-4040-a2f1-82578ed56b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47680f7d29d14902b4ce442e74d2f71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08b052dd52b42f79d984289670b4699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WER on Train Set: 68.92%\n",
      "‚úÖ WER on Test Set: 71.67%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Function to transcribe audio\n",
    "def transcribe_audio(batch):\n",
    "    audio = batch[\"audio\"][\"array\"]  # Get audio waveform\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")  # Process audio\n",
    "    input_features = inputs.input_features.to(\"cuda\")  # Move to GPU\n",
    "\n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "\n",
    "    # Decode predictions\n",
    "    transcription = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return {\"transcription\": transcription}\n",
    "\n",
    "# Apply transcription function to train and test sets\n",
    "commonvoice_dataset = commonvoice_dataset.map(transcribe_audio)\n",
    "\n",
    "# Compute WER\n",
    "def compute_wer(dataset):\n",
    "    references = [x[\"sentence\"] for x in dataset]  # Ground truth\n",
    "    predictions = [x[\"transcription\"] for x in dataset]  # Model output\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    return wer\n",
    "\n",
    "# Compute WER for train and test sets\n",
    "train_wer = compute_wer(commonvoice_dataset[\"train\"])\n",
    "test_wer = compute_wer(commonvoice_dataset[\"test\"])\n",
    "\n",
    "print(f\"‚úÖ WER on Train Set: {train_wer:.2%}\")\n",
    "print(f\"‚úÖ WER on Test Set: {test_wer:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61db818-8d14-485b-98d8-d5055c8e4767",
   "metadata": {},
   "source": [
    "## Actual vs Predicted transcription for 5 random audio samples from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70f75b4e-bfb4-4ca9-ade8-eeac3b548895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Comparing Actual vs. Predicted Transcriptions (First 20 Samples)\n",
      "\n",
      "üéß **Sample 1:**\n",
      "üìù **Actual   :** ‡§Ö‡§¨ ‡§∞‡§æ‡§Æ‡§™‡•Å‡§∞ ‡§Æ‡•á‡§Ç ‡§Ö‡§ñ‡§ø‡§≤‡•á‡§∂ ‡§¨‡§æ‡§Ç‡§ü‡•á‡§Ç‡§ó‡•á ‡§≤‡•à‡§™‡§ü‡•â‡§™ ‡§ï‡§æ '‡§≤‡•â‡§≤‡•Ä‡§™‡•â‡§™'\n",
      "ü§ñ **Predicted:**  ‡§Ö‡§¨ ‡§∞‡§æ‡§Æ‡•ç‡§™‡•Å‡§∞ ‡§Æ‡•á‡§Ç ‡§Ö‡§ï‡§ø‡§≤‡•á‡§∂ ‡§¨‡§æ‡§§‡•á‡§Ç‡§ó‡•á ‡§≤‡•à‡§™‡•ç‡§ü‡§™ ‡§ï‡§æ ‡§≤‡•ã‡§≤‡•Ä‡§™‡•â‡§™\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 2:**\n",
      "üìù **Actual   :** Flipkart: ‡§¨‡§Ç‡§™‡§∞ ‡§ë‡§´‡§∞‡•ç‡§∏ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§ø‡§ï ‡§∞‡§π‡§æ ‡§π‡•à Lenovo ‡§ï‡§æ ‡§Ø‡•á ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü‡§´‡•ã‡§®\n",
      "ü§ñ **Predicted:**  Flipkart, Bumper offer ‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§ø‡§ó ‡§∞‡§π‡§æ ‡§π‡•à, Lenovo ‡§ï‡§æ ‡§Ø‡•á ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ smartphone\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 3:**\n",
      "üìù **Actual   :** ‡§Æ‡•à‡§Ç ‡§Æ‡•Å‡§∏‡•Ä‡§¨‡§§ ‡§Æ‡•á‡§Ç ‡§™‡§°‡§º ‡§ó‡§Ø‡§æ‡•§\n",
      "ü§ñ **Predicted:**  ŸÖ€å⁄∫ ŸÖÿ≥€åŸàÿ™ ŸÖ€å⁄∫ Ÿæ⁄ë ⁄Ø€åÿß\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 4:**\n",
      "üìù **Actual   :** ‡§∏‡•Å‡§∂‡•Ä‡§≤ ‡§Æ‡•ã‡§¶‡•Ä ‡§π‡•à '‡§Ö‡§´‡§µ‡§æ‡§π ‡§Æ‡§ø‡§Ø‡§æ‡§Ç', ‡§¨‡§ø‡§ó‡§°‡§º ‡§ö‡•Å‡§ï‡§æ ‡§π‡•à ‡§Æ‡§æ‡§®‡§∏‡§ø‡§ï ‡§∏‡§Ç‡§§‡•Å‡§≤‡§®: ‡§§‡•á‡§ú‡§∏‡•ç‡§µ‡•Ä ‡§Ø‡§æ‡§¶‡§µ\n",
      "ü§ñ **Predicted:**  ‡§∂‡•Å‡§∂‡§ø‡§≤ ‡§Æ‡•ã‡§°‡§ø ‡§π‡•à ‡§Ö‡§ñ‡•ç‡§µ‡§æ ‡§Æ‡§ø‡§Ø‡§æ‡§Ç ‡§¨‡•Ä‡§ó‡§∞ ‡§ö‡•Å‡§ï‡§æ ‡§π‡•à ‡§Æ‡§®‡§∏‡•Ä ‡§∏‡§®‡§ï‡•Å‡§≤‡§Ç ‡§§‡•á ‡§ú‡§∏‡•ç‡§™‡•Ä ‡§Ü‡§¶‡§æ\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 5:**\n",
      "üìù **Actual   :** ‡§™. ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§Ü‡§∞‡§ï‡•ç‡§∑‡§£ ‡§ï‡•ã ‡§≠‡§æ‡§ú‡§™‡§æ ‡§¶‡•á‡§ó‡•Ä ‡§ö‡•Å‡§®‡•å‡§§‡•Ä\n",
      "ü§ñ **Predicted:**  ‡§™‡§∂‡•ç‡§ö‡§¨ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§Ü‡§∞‡§ï‡•ç‡§∑‡§® ‡§ï‡•ã ‡§≠‡§æ‡§ú‡§º‡§™‡§æ ‡§¶‡•á‡§ó‡•Ä ‡§ö‡§ø‡§®‡•å‡§§‡•Ä\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 6:**\n",
      "üìù **Actual   :** ‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§®‡•Ä ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞ ‡§≠‡§≤‡•á ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§π‡•ã‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§¶‡•á‡§∂ ‡§™‡§π‡§≤‡•á ‡§π‡•à: ‡§π‡•á‡§Æ‡§æ ‡§Æ‡§æ‡§≤‡§ø‡§®‡•Ä\n",
      "ü§ñ **Predicted:**  Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ€å ⁄©ÿßŸÑÿß⁄©ÿßÿ± ÿ®⁄æÿßŸÑ€í ÿß⁄Ü⁄æ€í €ÅŸà ŸÑ€å⁄©ŸÜ ÿØ€åÿ≥ Ÿæ€ÅŸÑ€í €Å€í ÿß€í ŸÖÿß ŸÖÿßŸÑ€åŸÜ€å\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 7:**\n",
      "üìù **Actual   :** ‡§®‡§æ‡§á‡§ú‡•Ä‡§∞‡§ø‡§Ø‡§æ ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§π‡§Æ‡§≤‡§æ: ‡§™‡•Ä‡§è‡§Æ ‡§®‡§∞‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Æ‡•ã‡§¶‡•Ä ‡§®‡•á ‡§ï‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§ï‡•Ä ‡§®‡§ø‡§Ç‡§¶‡§æ\n",
      "ü§ñ **Predicted:**  ‡§®‡§æ‡§á‡§ö‡•Ä‡§∞‡§ø‡§Ø‡§æ ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§π‡§Æ‡§≤‡§æ ‡§™‡•á‡§Æ ‡§®‡§ø‡§®‡§¶‡•ç‡§∞‡§¨‡•Å‡§§‡§ø‡§®‡•á ‡§ï‡•Ä ‡§ï‡•á‡§µ‡•ç‡§®‡§æ ‡§ï‡•á ‡§®‡§ø‡§Ç‡§¶‡§æ\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 8:**\n",
      "üìù **Actual   :** ‡§Æ‡•à‡§Ç ‡§§‡•Å‡§Æ‡•ç‡§π‡•á‡§Ç ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡•á ‡§π‡•ã‡§Æ‡§µ‡§∞‡•ç‡§ï ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡§æ ‡§π‡•Ç‡§Å‡•§\n",
      "ü§ñ **Predicted:**  ŸÖ€å⁄∫ ÿ™ŸÖ€Å€å⁄∫ ÿ™ŸÖ€Åÿßÿ±€í €ÅŸàŸÖ ŸàŸÇÿ™ ⁄©€í ÿ≥ÿßÿ™⁄æ ŸÖÿØÿØ ⁄©ÿ±ŸÜÿß ⁄Üÿß€Åÿ™ÿß €ÅŸà⁄∫€î\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 9:**\n",
      "üìù **Actual   :** ‡§Æ‡•Å‡§ù‡•á ‡§≤‡§ó‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§ï‡•â‡§≤‡•á‡§ú ‡§ú‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§Ø‡•á‡•§\n",
      "ü§ñ **Predicted:**  ‡§Æ‡•Å‡§ù‡•á ‡§≤‡§ó‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§ï‡•â‡§≤‡•á‡§° ‡§ú‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 10:**\n",
      "üìù **Actual   :** ‡§µ‡§ø‡§™‡§ï‡•ç‡§∑‡•Ä ‡§¶‡§≤‡•ã‡§Ç ‡§®‡•á ‡§Æ‡§®‡§æ‡§Ø‡§æ ‡§ï‡§æ‡§≤‡§æ ‡§¶‡§ø‡§µ‡§∏\n",
      "ü§ñ **Predicted:**  ‡§µ‡§ø‡§™‡§ï‡•ç‡§∑‡§ø ‡§¶‡§≤‡•ã‡§Ç ‡§è ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ï‡§æ‡§≤‡§æ ‡§¶‡•Ä‡§µ‡§∏\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 11:**\n",
      "üìù **Actual   :** ‡§µ‡§π ‡§Æ‡•á‡§∞‡•Ä ‡§ö‡•Ç‡§§ ‡§π‡•à‡•§\n",
      "ü§ñ **Predicted:**  ‡§Ø‡§æ ‡§Æ‡•á‡§∞‡•á ‡§õ‡•Ç‡§† ‡§π‡•à\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 12:**\n",
      "üìù **Actual   :** Box office ‡§ï‡§æ ‡§®‡§Ø‡§æ ‡§π‡•Ä‡§∞‡•ã ‡§¨‡§®‡§æ ‡§π‡•â‡§∞‡§∞, '‡§≤‡•à‡§≤‡§æ ‡§Æ‡§ú‡§®‡•Ç', '‡§™‡§≤‡§ü‡§®' ‡§™‡•Ä‡§õ‡•á\n",
      "ü§ñ **Predicted:**  ÿ®ÿßÿ±⁄©ÿ≥ ÿ¢ŸÅ€åÿ≥ ⁄©ÿß ŸÜ€åÿß €Å€åÿ±Ÿà ÿ®ŸÜÿß €ÅŸàÿ±ÿ± ŸÑ€åŸÑÿß ŸÖÿ¨ŸÜŸà Ÿæÿ±ŸπŸÜ Ÿæ€å⁄Ü⁄æ€í\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 13:**\n",
      "üìù **Actual   :** ‡§Æ‡•à‡§Ç ‡§â‡§†‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•Ä ‡§π‡•Ç‡§Å‡•§\n",
      "ü§ñ **Predicted:**  ‡§Æ‡•à‡§Ç ‡§â‡§ü‡§®‡§æ ‡§ö‡§æ‡§§‡•Ä ‡§π‡•Ç‡§Å\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 14:**\n",
      "üìù **Actual   :** ‡§≤‡§æ‡§≤ ‡§¨‡§π‡§æ‡§¶‡•Å‡§∞ ‡§∂‡§æ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡•Ä ‡§°‡•á‡§• ‡§Æ‡§ø‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä ‡§™‡§∞ ‡§´‡§ø‡§≤‡•ç‡§Æ, ‡§á‡§∏ ‡§∞‡•ã‡§≤ ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡•á‡§Ç‡§ó‡•á ‡§®‡§∏‡•Ä‡§∞‡•Å‡§¶‡•ç‡§¶‡•Ä‡§® ‡§∂‡§æ‡§π\n",
      "ü§ñ **Predicted:**  ŸÑÿßŸÑ ŸàÿßÿØŸà ÿ¥ÿßÿ≥ÿ™ÿ±€å ⁄©€å ⁄à€åÿ™ ŸÖ€åÿ≥Ÿπÿ±€å Ÿæÿ± ŸÅŸÑŸÖ ÿßÿ≥ ÿ±ŸàŸÑ ŸÖ€å⁄∫ ÿ¨⁄©⁄æ€å⁄∫ ⁄Ø€í ŸÜÿµŸàÿ±ÿ™€åŸÜ ÿ¥ÿß€Å\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 15:**\n",
      "üìù **Actual   :** ...‡§§‡•ã ‡§≤‡§Ç‡§¶‡§® ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§π‡•Ä ‡§¶‡§ø‡§® ‡§¶‡•ã ‡§Æ‡•à‡§¶‡§æ‡§®‡•ã‡§Ç ‡§™‡§∞ ‡§≠‡§ø‡§°‡§º‡•á‡§Ç‡§ó‡•á ‡§≠‡§æ‡§∞‡§§ ‡§î‡§∞ ‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§®\n",
      "ü§ñ **Predicted:**  ‡§§‡•ã ‡§≤‡§Ç‡§¶‡§® ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§π‡•Ä ‡§¶‡§ø‡§® ‡§¶‡•ã ‡§Æ‡§ú‡§æ‡§®‡•ã‡§Ç ‡§™‡§∞ ‡§¨‡§ø‡§¢‡•á‡§Ç‡§ó‡•á ‡§≠‡§æ‡§∞‡§æ‡§• ‡§â ‡§™‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§®\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 16:**\n",
      "üìù **Actual   :** Opinion: ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§ï‡•ã‡§π‡§≤‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¨‡•á‡§π‡§§‡§∞‡•Ä‡§® ‡§Æ‡•å‡§ï‡§æ\n",
      "ü§ñ **Predicted:**  ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§ï‡•ã‡§≤‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¨‡•á‡§π‡§§ ‡§∞‡§ø‡§® ‡§Æ‡•å‡§ï‡§æ\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 17:**\n",
      "üìù **Actual   :** ‡§ú‡§Ø‡§≤‡§≤‡§ø‡§§‡§æ ‡§¨‡§æ‡§Ø‡•ã‡§™‡§ø‡§ï ‡§∏‡•á ‡§ï‡§Ç‡§ó‡§®‡§æ ‡§ï‡§æ ‡§≤‡•Å‡§ï ‡§Ü‡§â‡§ü, ‡§∏‡•ã‡§∂‡§≤ ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§™‡§∞ ‡§π‡•ã ‡§ó‡§à‡§Ç ‡§ü‡•ç‡§∞‡•ã‡§≤\n",
      "ü§ñ **Predicted:**  ÿ¨ŸÑÿØ€Å ÿ®ÿßÿ¶€åŸà Ÿæ⁄©ÿ≥ ÿ≥€í ⁄©ŸÜ⁄ØŸÜÿß ⁄©ÿß ŸÑ⁄© ÿ¢ŸàŸπ ÿ≥Ÿàÿ¥ŸÑ ŸÖ€å⁄à€åÿß Ÿæÿ± €ÅŸà ⁄Øÿ¶€å⁄∫ Ÿπÿ±ÿßŸÑ\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 18:**\n",
      "üìù **Actual   :** ‡§™‡•ç‡§∞‡•á‡§ó‡•ç‡§®‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ä‡§®‡§æ ‡§ï‡•Ä ‡§§‡§¨‡•Ä‡§Ø‡§§ ‡§¨‡§ø‡§ó‡§°‡§º‡•Ä, ‡§ò‡§¨‡§∞‡§æ‡§è ‡§∏‡•à‡§´ ‡§®‡•á ‡§∞‡§æ‡§§ ‡§Æ‡•á‡§Ç ‡§¨‡•Å‡§≤‡§æ‡§Ø‡§æ ‡§®‡§∞‡•ç‡§∏ ‡§ï‡•ã\n",
      "ü§ñ **Predicted:**  ‡§™‡•ç‡§∞‡•á‡§ó‡§®‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ä‡§®‡§æ ‡§ï‡•Ä ‡§§‡§¨‡§ø‡§¶ ‡§¨‡§ø‡§ó‡§°‡•Ä, ‡§ó‡§¨‡§∞‡§æ‡§à ‡§∏‡•à‡§´ ‡§®‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç ‡§¨‡•Å‡§≤‡§æ‡§Ø‡§æ ‡§®‡§∞‡•ç‡§∏ ‡§ï‡•ã\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 19:**\n",
      "üìù **Actual   :** ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡•á ‡§¨‡§π‡•Å‡§§ ‡§∏‡§æ‡§∞‡•á ‡§¶‡•Å‡§∂‡§Æ‡§® ‡§π‡•à‡§Ç‡•§\n",
      "ü§ñ **Predicted:**  ‡§§‡•Å‡§Æ‡§æ‡§∞‡•á ‡§¨‡§π‡•Å‡§§ ‡§∏‡§π‡§∞‡•á ‡§¶‡•Å‡§∏‡•ç‡§Æ‡•á‡§Ç ‡§π‡•à\n",
      "--------------------------------------------------------------------------------\n",
      "üéß **Sample 20:**\n",
      "üìù **Actual   :** ‡§¨‡•ç‡§∞‡•á‡§∏‡•ç‡§ü ‡§ï‡•à‡§Ç‡§∏‡§∞ ‡§ï‡•Ä ‡§ú‡§æ‡§ó‡§∞‡•Ç‡§ï‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡•à ‡§™‡§ø‡§Ç‡§ï ‡§∞‡§ø‡§¨‡§® ‡§¨‡•ç‡§∞‡§æ...\n",
      "ü§ñ **Predicted:**  ‡§¨‡•ç‡§∞‡•á‡§∏‡•ç‡§ü ‡§ï‡•à‡§Ç‡§∏‡§∞ ‡§ï‡•Ä ‡§ú‡§æ‡§ó‡§∞‡•Å‡§ï‡•ç‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡•à ‡§ü‡§ø‡§Ç‡§ï ‡§∞‡§ø‡§¨‡§® ‡§¨‡•ç‡§∞‡§æ‡§´\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to transcribe audio using Whisper model\n",
    "def transcribe_audio(audio_array, sampling_rate):\n",
    "    inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")  # Process audio\n",
    "    input_features = inputs.input_features.to(\"cuda\")  # Move to GPU\n",
    "\n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "\n",
    "    # Decode predictions\n",
    "    transcription = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "# üéß Compare Actual vs. Predicted for First 5 Samples in Test Set\n",
    "print(\"\\n‚úÖ Comparing Actual vs. Predicted Transcriptions (First 20 Samples)\\n\")\n",
    "\n",
    "for idx in range(min(20, len(commonvoice_dataset[\"test\"]))):  # Ensure we don't exceed dataset size\n",
    "    sample = commonvoice_dataset[\"test\"][idx]\n",
    "\n",
    "    # Transcribe using Whisper\n",
    "    predicted_transcription = transcribe_audio(sample[\"audio\"][\"array\"], sample[\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "    print(f\"üéß **Sample {idx+1}:**\")\n",
    "    print(f\"üìù **Actual   :** {sample['sentence']}\")\n",
    "    print(f\"ü§ñ **Predicted:** {predicted_transcription}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b90b4-f58b-463c-af0e-a46125e02cf6",
   "metadata": {},
   "source": [
    "## Extract log-mel features and tokenize the transcriptions(for both train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91a685aa-175d-45a1-a51e-bb38c0e5d340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62c611102804c3b9980972b9b23653f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5289456273df402794d3a6765fa11970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature extraction and tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "# Load the Whisper feature extractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "def extract_features_and_encode(batch):\n",
    "    # Extract log-Mel spectrogram features\n",
    "    batch[\"input_features\"] = feature_extractor(batch[\"audio\"][\"array\"], sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # Encode target transcriptions\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the dataset\n",
    "commonvoice_dataset = commonvoice_dataset.map(extract_features_and_encode, num_proc=1)\n",
    "\n",
    "print(\"‚úÖ Feature extraction and tokenization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc3f62-101c-411a-a94d-ddef0bf34664",
   "metadata": {},
   "source": [
    "{\n",
    "    'audio': {\n",
    "        'array': numpy_array, \n",
    "        'sampling_rate': 16000\n",
    "    },\n",
    "    'sentence': 'This is the transcription of the audio.',\n",
    "    'input_features': tensor_of_log_mel_spectrogram,\n",
    "    'labels': [tokenized_ids]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c6f02-ce9e-494c-88ae-8dae94a6b512",
   "metadata": {},
   "source": [
    "Current dataset looks like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa30576f-3d5e-4fe8-8379-0abe891282e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print dataset format after feature extraction\n",
    "print(commonvoice_dataset)\n",
    "\n",
    "# Print a sample entry from the dataset (first example from train set)\n",
    "print(commonvoice_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85264e98-b75c-438c-8318-29f2e6304792",
   "metadata": {},
   "source": [
    "## Change it to\n",
    "Dataset({\n",
    "    features: ['input_features', 'labels'],\n",
    "    num_rows: 6760\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eabe58fe-449e-4ad4-bce1-ca7d45ee6612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 7563\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 3337\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "commonvoice_dataset = commonvoice_dataset.remove_columns([\"audio\", \"sentence\"])\n",
    "#commonvoice_dataset = commonvoice_dataset.remove_columns([\"transcription\"])\n",
    "\n",
    "# Print dataset structure\n",
    "print(commonvoice_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8199fce3-4171-4039-be9b-1ee7891a08d6",
   "metadata": {},
   "source": [
    "### input_features ‚Üí Tensor of shape (batch_size, 80, time_steps)\n",
    "\n",
    "80 is the number of Mel frequency bins (Whisper feature size)\n",
    "\n",
    "time_steps varies based on the longest audio in the batch (others are padded)\n",
    "\n",
    "### labels ‚Üí Tensor of shape (batch_size, label_length)\n",
    "\n",
    "The tokenized transcription sequences\n",
    "\n",
    "Padded to the longest sequence in the batch\n",
    "\n",
    "Padding tokens replaced with -100 (ignored during loss computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e941d988-b94b-452c-b7be-41df5c2dba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e355d88-d0ac-4f5b-9c1a-006d13662095",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056ef6-1a68-4c3f-b52b-28abe2a50ff2",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74e6afe8-797e-44a7-9873-09165aaf1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa1e63-dcc6-4886-8065-020c0f394976",
   "metadata": {},
   "source": [
    "## Post-processing on the model\n",
    "\n",
    "To reduce our models memory footprint, we load the model in 8bit, this means we quantize the model to use 1/4th precision (when comapared to float32) with minimal loss to performance. Finally, we need to apply some post-processing steps on the 8-bit model to enable training. We do so by first freezing all the model layers, and then cast the layer-norm and the output layer in float32 for training and model stability. Since the Whisper model uses Convolutional layers in the Encoder, checkpointing disables grad computation to avoid this we specifically need to make the inputs trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d08eeab0-77f1-4678-86c9-4133629170f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\").to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f1aba233-55fd-4269-a7e5-6a747f633569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (2.4.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (4.46.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (4.66.5)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (0.26.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers->peft) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.0.1 peft-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "06167b96-5069-4b03-881b-be65b93c1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.4-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.4.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.4-py3-none-win_amd64.whl (75.4 MB)\n",
      "   ---------------------------------------- 0.0/75.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.0/75.4 MB 6.3 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 3.4/75.4 MB 9.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 6.0/75.4 MB 10.0 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 8.4/75.4 MB 10.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 10.5/75.4 MB 10.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 11.8/75.4 MB 9.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 13.4/75.4 MB 9.0 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 14.7/75.4 MB 8.8 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 16.3/75.4 MB 8.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 18.1/75.4 MB 8.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 19.4/75.4 MB 8.2 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 21.0/75.4 MB 8.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 22.8/75.4 MB 8.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 24.9/75.4 MB 8.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 26.7/75.4 MB 8.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 28.8/75.4 MB 8.3 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 30.9/75.4 MB 8.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 33.0/75.4 MB 8.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 35.4/75.4 MB 8.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 37.7/75.4 MB 8.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 40.1/75.4 MB 8.8 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 42.5/75.4 MB 8.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 44.8/75.4 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 46.9/75.4 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 48.8/75.4 MB 9.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 50.6/75.4 MB 9.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 52.7/75.4 MB 9.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 54.3/75.4 MB 8.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 55.8/75.4 MB 8.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 57.4/75.4 MB 8.8 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 59.2/75.4 MB 8.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 60.8/75.4 MB 8.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 62.1/75.4 MB 8.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 63.4/75.4 MB 8.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 65.0/75.4 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 66.3/75.4 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 67.9/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 69.5/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 71.3/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 72.9/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  74.7/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  75.2/75.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 75.4/75.4 MB 8.2 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "773db515-254c-4399-b659-66c8e3c6e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\workstations\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (2.4.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (4.46.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (1.0.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (0.26.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers->peft) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7e788f4d-3640-4f89-b30c-9b876a0eb189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (2.4.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (0.26.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate>=0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"accelerate>=0.26.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f3f9c571-72c9-47a9-8c36-5b345a7c0cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc798051-eeb2-4348-969b-5111b599d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Instead of 8-bit\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Ensure compatibility\n",
    "    bnb_4bit_use_double_quant=True  # Optional: Helps reduce memory usage\n",
    ")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"openai/whisper-large\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5c5e264-f3a5-4084-9046-b7321827f280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x25519f7d520>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_inputs_require_grad(module, input, output):\n",
    "    output.requires_grad_(True)\n",
    "\n",
    "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b42f6-1fd7-49e9-b6e2-232947b7ffaf",
   "metadata": {},
   "source": [
    "## Apply Low-rank adapters (LoRA) to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5eccdc89-82c6-4217-97b1-8ff4131d2b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\workstations\\anaconda3\\lib\\site-packages (0.45.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\workstations\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: peft in c:\\users\\workstations\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\workstations\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.4.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade bitsandbytes transformers peft accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "616c4aaf-44be-405f-949b-1f82386ec62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,728,640 || all params: 1,559,033,600 || trainable%: 1.0089\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52a85b-a180-47d6-b823-ce327dfcd90d",
   "metadata": {},
   "source": [
    "We are ONLY using 1% of the total trainable parameters, thereby performing Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec1e5e-8091-475f-8ca3-329343f96c93",
   "metadata": {},
   "source": [
    "## Define the Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d482a29f-3a3c-4d54-b73e-906f6781da1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: h5py 3.11.0\n",
      "Uninstalling h5py-3.11.0:\n",
      "  Successfully uninstalled h5py-3.11.0\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.11.0-cp38-cp38-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\workstations\\anaconda3\\lib\\site-packages (from h5py) (1.24.4)\n",
      "Downloading h5py-3.11.0-cp38-cp38-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.8/3.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 11.6 MB/s eta 0:00:00\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-3.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.16.1 requires flatbuffers>=23.5.26, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires keras>=3.0.0, but you have keras 2.9.0 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires tensorboard<2.17,>=2.16, but you have tensorboard 2.9.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall h5py -y\n",
    "!pip install --no-cache-dir h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9a2d86b-47e9-4f7e-a35d-3da22265f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=r\"C:\\Users\\WORKSTATIONS\\Desktop\\BijoyashreeDas\\WHISPER\",  \n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  \n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=100,\n",
    "    #max_steps=100,  # only for testing purposes, remove this in final run\n",
    "    remove_unused_columns=False,  \n",
    "    label_names=[\"labels\"],  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4d4b7-6513-4bb9-bf6c-4c748c0e09e5",
   "metadata": {},
   "source": [
    "## Train the model->save the adapter weights and trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b78dbe9f-0f1e-4611-90a8-dfd9ba55f64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WORKSTATIONS\\AppData\\Local\\Temp\\ipykernel_16948\\4289590012.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import Seq2SeqTrainer, TrainerCallback, TrainerState, TrainerControl, TrainingArguments\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "# Define save path\n",
    "save_path = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER\"\n",
    "adapter_path = os.path.join(save_path, \"lora_adapter\")\n",
    "\n",
    "# Ensure save directories exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "\n",
    "# Callback to save only LoRA adapter weights\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=commonvoice_dataset[\"train\"],\n",
    "    eval_dataset=commonvoice_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "\n",
    "# Disable caching for training\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6e8e86f2-7023-4163-abfb-c44742cd7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 15:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.316768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.4601191711425781, metrics={'train_runtime': 910.2888, 'train_samples_per_second': 0.879, 'train_steps_per_second': 0.11, 'total_flos': 1.71665620992e+18, 'train_loss': 0.4601191711425781, 'epoch': 0.10570824524312897})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b9d53308-ccb4-44a9-9766-08e1fc69ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapter saved separately at: C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER\\lora_adapter\n"
     ]
    }
   ],
   "source": [
    "# Save the full fine-tuned model (Whisper + LoRA)\n",
    "#model.save_pretrained(save_path)\n",
    "#processor.save_pretrained(save_path)\n",
    "from peft import PeftModel\n",
    "\n",
    "# Save LoRA adapter separately\n",
    "model.save_pretrained(adapter_path)\n",
    "\n",
    "\n",
    "#print(f\"‚úÖ Model and weights saved at: {save_path}\")\n",
    "print(f\"‚úÖ LoRA adapter saved separately at: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e6976-3632-4f2e-af15-95411bf5b7d3",
   "metadata": {},
   "source": [
    "## Print steps-train loss-test loss (Cross-Entropy Loss (CE Loss))\n",
    "\n",
    "If you have 1,000 training samples and use batch_size=8, then you'll have:\n",
    "\n",
    "1000/8=125 steps¬†per¬†epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d93f6c8d-db58-42ba-bdf6-3eeca999a9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step\tTraining Loss\tValidation Loss\n",
      "100\t0.460100\tN/A\n",
      "100\tN/A\t0.316768\n",
      "100\tN/A\tN/A\n"
     ]
    }
   ],
   "source": [
    "print(\"Step\\tTraining Loss\\tValidation Loss\")\n",
    "for log in trainer.state.log_history:\n",
    "    step = log.get(\"step\", \"N/A\")\n",
    "    train_loss = log.get(\"loss\", None)  # Training loss\n",
    "    val_loss = log.get(\"eval_loss\", None)  # Validation loss\n",
    "\n",
    "    if step != \"N/A\":  # Only print if it's a valid step\n",
    "        train_loss_str = f\"{train_loss:.6f}\" if train_loss is not None else \"N/A\"\n",
    "        val_loss_str = f\"{val_loss:.6f}\" if val_loss is not None else \"N/A\"\n",
    "        print(f\"{step}\\t{train_loss_str}\\t{val_loss_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ec267-d6bf-47a4-a600-b795002e2fc5",
   "metadata": {},
   "source": [
    "## Evaluation and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778ebc2-f346-4ed7-a498-82128b394ce1",
   "metadata": {},
   "source": [
    " Loads the PEFT/LoRA configuration\n",
    "‚úÖ Loads the base Whisper model in 8-bit mode for efficiency\n",
    "‚úÖ Merges the fine-tuned LoRA weights with the base model\n",
    "‚úÖ Enables caching for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1a5fb969-ed0e-41ff-bcc6-ca3341999840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and LoRA adapter loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# Define paths\n",
    "base_model_path = \"openai/whisper-large\"  # Change if needed\n",
    "adapter_path = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/lora_adapter\"\n",
    "\n",
    "# Load base Whisper model\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(base_model_path).to(\"cuda\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Enable cache for inference\n",
    "model.config.use_cache = True\n",
    "\n",
    "print(\"‚úÖ Model and LoRA adapter loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee75f2-1468-486a-bbea-a854785066a3",
   "metadata": {},
   "source": [
    "## Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f45b2539-b562-44a8-90f2-d74754ec59cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/final_model\n"
     ]
    }
   ],
   "source": [
    "save_path = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/final_model\"\n",
    "\n",
    "# Save the full model with LoRA adapter\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model saved successfully at {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd827ae-cf1b-4077-8fb0-eae16aa245fc",
   "metadata": {},
   "source": [
    "## Compute WER on Train and Test Sets on saved model after finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4595bba2-7199-4283-9c03-23ec141cfb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Reload the processor from the base model and save it\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")  # Change to your base model\n",
    "processor.save_pretrained(\"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "645b3549-c450-47b1-9389-503d0876250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate  # ‚úÖ Use `evaluate` instead of `datasets.load_metric`\n",
    "\n",
    "# Define paths\n",
    "model_path = \"C:/Users/WORKSTATIONS/Desktop/BijoyashreeDas/WHISPER/final_model\"\n",
    "\n",
    "# Load the model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Set to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load WER metric\n",
    "metric = evaluate.load(\"wer\")  # ‚úÖ Correct way to load the WER metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e16d868e-3d0a-453c-a3db-73c079aa7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wer(dataset, batch_size=8, max_new_tokens=255):\n",
    "    \"\"\"Generates transcriptions and computes WER for the given dataset.\"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    predictions, references = [], []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating WER...\"):\n",
    "        with torch.no_grad():\n",
    "            input_features = batch[\"input_features\"].to(\"cuda\")\n",
    "\n",
    "            # Generate transcription\n",
    "            generated_tokens = model.generate(input_features, max_new_tokens=max_new_tokens)\n",
    "\n",
    "            # Decode predictions and references\n",
    "            decoded_preds = processor.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "\n",
    "        # Free memory\n",
    "        del generated_tokens, batch\n",
    "        gc.collect()\n",
    "\n",
    "    # Compute WER\n",
    "    wer = 100 * metric.compute(predictions=predictions, references=references)\n",
    "    return wer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "be62c4db-67f7-4c0a-b10e-50eb32addc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WER...:   0%|                                                                       | 0/946 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Evaluating WER...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 946/946 [2:35:37<00:00,  9.87s/it]\n",
      "Evaluating WER...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 418/418 [59:14<00:00,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train WER: 43.76%\n",
      "Test WER: 47.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'common_voice' is your dataset\n",
    "train_wer = compute_wer(commonvoice_dataset[\"train\"])\n",
    "test_wer = compute_wer(commonvoice_dataset[\"test\"])\n",
    "\n",
    "print(f\"Train WER: {train_wer:.2f}%\")\n",
    "print(f\"Test WER: {test_wer:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b89673-4252-4f94-9e39-472dc39f96c0",
   "metadata": {},
   "source": [
    "If your dataset has 7,568 samples, then:\n",
    "\n",
    "7568/8 = 946 batches\n",
    "\n",
    "This means your dataset has 946 mini-batches, and each iteration processes one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b07e15-133b-4478-9fd4-a9b8a4813614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192add0d-f79d-4ce1-bfeb-273e417172e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80c503-f32b-4738-929e-410da582fd23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
